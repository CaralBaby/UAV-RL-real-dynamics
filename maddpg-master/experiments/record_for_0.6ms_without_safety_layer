Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 59275, episodes: 1000, mean episode reward: -99.03739049960434, num_cumulative_constraints: 5934, agent episode reward: [-99.03739049960434], time: 72.641
steps: 118614, episodes: 2000, mean episode reward: -97.75929812970149, num_cumulative_constraints: 12779, agent episode reward: [-97.75929812970149], time: 97.584
steps: 177046, episodes: 3000, mean episode reward: -76.7261274395715, num_cumulative_constraints: 19289, agent episode reward: [-76.7261274395715], time: 108.388
steps: 234674, episodes: 4000, mean episode reward: -65.65787825355729, num_cumulative_constraints: 23018, agent episode reward: [-65.65787825355729], time: 119.947
steps: 291599, episodes: 5000, mean episode reward: -54.65189808766584, num_cumulative_constraints: 24183, agent episode reward: [-54.65189808766584], time: 122.301
steps: 344354, episodes: 6000, mean episode reward: -47.433393797862884, num_cumulative_constraints: 24827, agent episode reward: [-47.433393797862884], time: 92.53
steps: 394596, episodes: 7000, mean episode reward: -45.96893630681061, num_cumulative_constraints: 25573, agent episode reward: [-45.96893630681061], time: 96.316
steps: 442410, episodes: 8000, mean episode reward: -44.54873009320652, num_cumulative_constraints: 26304, agent episode reward: [-44.54873009320652], time: 85.63
steps: 486803, episodes: 9000, mean episode reward: -42.57897398908081, num_cumulative_constraints: 26924, agent episode reward: [-42.57897398908081], time: 70.385
steps: 528779, episodes: 10000, mean episode reward: -41.17093323219754, num_cumulative_constraints: 27446, agent episode reward: [-41.17093323219754], time: 64.846
steps: 570520, episodes: 11000, mean episode reward: -40.78405068242921, num_cumulative_constraints: 27914, agent episode reward: [-40.78405068242921], time: 67.337
steps: 612079, episodes: 12000, mean episode reward: -40.62406839010756, num_cumulative_constraints: 28338, agent episode reward: [-40.62406839010756], time: 60.52
steps: 653503, episodes: 13000, mean episode reward: -40.78103014685649, num_cumulative_constraints: 28857, agent episode reward: [-40.78103014685649], time: 74.486
steps: 694874, episodes: 14000, mean episode reward: -40.9856093652969, num_cumulative_constraints: 29438, agent episode reward: [-40.9856093652969], time: 81.331



another time
C:\Users\xichenyang\AppData\Local\Continuum\anaconda3\python.exe C:/Users/xichenyang/Desktop/UAV-RL-real-dynamics/maddpg-master/experiments/train.py
2019-07-21 17:02:49.731749: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From C:\Users\xichenyang\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Using good policy maddpg and adv policy maddpg
Loading previous state...
WARNING:tensorflow:From C:\Users\xichenyang\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\training\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Starting iterations...
steps: 41801, episodes: 1000, mean episode reward: -45.986390301876554, num_cumulative_constraints: 2190, agent episode reward: [-45.986390301876554], time: 24.656
steps: 83433, episodes: 2000, mean episode reward: -45.330709075111514, num_cumulative_constraints: 4119, agent episode reward: [-45.330709075111514], time: 28.97
steps: 125102, episodes: 3000, mean episode reward: -42.44788906218007, num_cumulative_constraints: 5083, agent episode reward: [-42.44788906218007], time: 30.755
steps: 166880, episodes: 4000, mean episode reward: -41.5011704871472, num_cumulative_constraints: 5706, agent episode reward: [-41.5011704871472], time: 31.62
steps: 208804, episodes: 5000, mean episode reward: -41.37239857209604, num_cumulative_constraints: 6294, agent episode reward: [-41.37239857209604], time: 30.599
steps: 250637, episodes: 6000, mean episode reward: -41.198031823068185, num_cumulative_constraints: 6856, agent episode reward: [-41.198031823068185], time: 31.334
steps: 291980, episodes: 7000, mean episode reward: -40.86888858184284, num_cumulative_constraints: 7360, agent episode reward: [-40.86888858184284], time: 31.888
steps: 333063, episodes: 8000, mean episode reward: -40.83713868670516, num_cumulative_constraints: 7892, agent episode reward: [-40.83713868670516], time: 30.365
steps: 374176, episodes: 9000, mean episode reward: -40.983527783411866, num_cumulative_constraints: 8444, agent episode reward: [-40.983527783411866], time: 31.756
steps: 415239, episodes: 10000, mean episode reward: -41.10180423289885, num_cumulative_constraints: 9064, agent episode reward: [-41.10180423289885], time: 29.799
steps: 456280, episodes: 11000, mean episode reward: -41.29722553111888, num_cumulative_constraints: 9730, agent episode reward: [-41.29722553111888], time: 31.392
steps: 497403, episodes: 12000, mean episode reward: -40.92231767844039, num_cumulative_constraints: 10250, agent episode reward: [-40.92231767844039], time: 31.162
steps: 538538, episodes: 13000, mean episode reward: -40.76223124207965, num_cumulative_constraints: 10720, agent episode reward: [-40.76223124207965], time: 30.901
steps: 579553, episodes: 14000, mean episode reward: -40.755195021274126, num_cumulative_constraints: 11224, agent episode reward: [-40.755195021274126], time: 30.783
steps: 620816, episodes: 15000, mean episode reward: -40.88003683001372, num_cumulative_constraints: 11722, agent episode reward: [-40.88003683001372], time: 30.864
steps: 662048, episodes: 16000, mean episode reward: -41.22042473180912, num_cumulative_constraints: 12285, agent episode reward: [-41.22042473180912], time: 31.296

Process finished with exit code -1



1 landmark
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 59548, episodes: 1000, mean episode reward: -88.57534921200029, num_cumulative_constraints: 1279, agent episode reward: [-88.57534921200029], time: 47.678
steps: 118467, episodes: 2000, mean episode reward: -76.08166509327857, num_cumulative_constraints: 3091, agent episode reward: [-76.08166509327857], time: 71.614
steps: 172486, episodes: 3000, mean episode reward: -51.59999641915736, num_cumulative_constraints: 4587, agent episode reward: [-51.59999641915736], time: 54.693
steps: 221530, episodes: 4000, mean episode reward: -45.97752314333855, num_cumulative_constraints: 5563, agent episode reward: [-45.97752314333855], time: 67.104
steps: 271808, episodes: 5000, mean episode reward: -45.30753506555583, num_cumulative_constraints: 6101, agent episode reward: [-45.30753506555583], time: 71.167
steps: 321050, episodes: 6000, mean episode reward: -43.27169490367739, num_cumulative_constraints: 6154, agent episode reward: [-43.27169490367739], time: 58.908
steps: 367431, episodes: 7000, mean episode reward: -41.41993877062692, num_cumulative_constraints: 6175, agent episode reward: [-41.41993877062692], time: 65.515
steps: 409711, episodes: 8000, mean episode reward: -39.22670842177663, num_cumulative_constraints: 6215, agent episode reward: [-39.22670842177663], time: 51.344
steps: 450681, episodes: 9000, mean episode reward: -38.64047079193772, num_cumulative_constraints: 6264, agent episode reward: [-38.64047079193772], time: 45.207
steps: 491418, episodes: 10000, mean episode reward: -38.55162527299274, num_cumulative_constraints: 6298, agent episode reward: [-38.55162527299274], time: 51.524
steps: 531772, episodes: 11000, mean episode reward: -38.35417908714002, num_cumulative_constraints: 6335, agent episode reward: [-38.35417908714002], time: 55.854
steps: 572744, episodes: 12000, mean episode reward: -38.52889705271986, num_cumulative_constraints: 6364, agent episode reward: [-38.52889705271986], time: 49.395

steps: 39896, episodes: 1000, mean episode reward: -38.146757405998024, num_cumulative_constraints: 17, agent episode reward: [-38.146757405998024], time: 36.944
steps: 80786, episodes: 2000, mean episode reward: -38.66540580191134, num_cumulative_constraints: 58, agent episode reward: [-38.66540580191134], time: 45.387
steps: 121187, episodes: 3000, mean episode reward: -38.77065105772451, num_cumulative_constraints: 212, agent episode reward: [-38.77065105772451], time: 52.647
steps: 161630, episodes: 4000, mean episode reward: -39.0507286827249, num_cumulative_constraints: 511, agent episode reward: [-39.0507286827249], time: 50.002
steps: 202393, episodes: 5000, mean episode reward: -39.509499789316926, num_cumulative_constraints: 831, agent episode reward: [-39.509499789316926], time: 49.35
steps: 242902, episodes: 6000, mean episode reward: -38.67042439247768, num_cumulative_constraints: 921, agent episode reward: [-38.67042439247768], time: 53.002
steps: 282874, episodes: 7000, mean episode reward: -38.34067076834887, num_cumulative_constraints: 1032, agent episode reward: [-38.34067076834887], time: 53.282
steps: 322991, episodes: 8000, mean episode reward: -38.396064771214064, num_cumulative_constraints: 1112, agent episode reward: [-38.396064771214064], time: 57.567

with safety_layer

steps: 39965, episodes: 1000, mean episode reward: -38.26160638910133, num_cumulative_constraints: 47, agent episode reward: [-38.26160638910133], time: 57.019
steps: 80060, episodes: 2000, mean episode reward: -38.337656588566986, num_cumulative_constraints: 90, agent episode reward: [-38.337656588566986], time: 69.637
steps: 120162, episodes: 3000, mean episode reward: -38.33986890237994, num_cumulative_constraints: 170, agent episode reward: [-38.33986890237994], time: 66.044
steps: 160232, episodes: 4000, mean episode reward: -38.193383387300884, num_cumulative_constraints: 240, agent episode reward: [-38.193383387300884], time: 66.015
steps: 200531, episodes: 5000, mean episode reward: -38.30009753673443, num_cumulative_constraints: 335, agent episode reward: [-38.30009753673443], time: 83.672
steps: 240904, episodes: 6000, mean episode reward: -38.54849751897353, num_cumulative_constraints: 508, agent episode reward: [-38.54849751897353], time: 76.702


6 landmarks without safety_layer
steps: 59220, episodes: 1000, mean episode reward: -99.66015173085468, num_cumulative_constraints: 7586, agent episode reward: [-99.66015173085468], time: 75.915
steps: 118193, episodes: 2000, mean episode reward: -94.70314121263178, num_cumulative_constraints: 15069, agent episode reward: [-94.70314121263178], time: 89.712
steps: 176498, episodes: 3000, mean episode reward: -75.38650998853004, num_cumulative_constraints: 21067, agent episode reward: [-75.38650998853004], time: 94.619
steps: 233801, episodes: 4000, mean episode reward: -65.00951923533887, num_cumulative_constraints: 23921, agent episode reward: [-65.00951923533887], time: 81.614
steps: 288634, episodes: 5000, mean episode reward: -49.588709740704225, num_cumulative_constraints: 24703, agent episode reward: [-49.588709740704225], time: 85.349
steps: 339457, episodes: 6000, mean episode reward: -46.113108510446686, num_cumulative_constraints: 25300, agent episode reward: [-46.113108510446686], time: 78.122
steps: 386144, episodes: 7000, mean episode reward: -43.753245175537245, num_cumulative_constraints: 25913, agent episode reward: [-43.753245175537245], time: 73.096
steps: 430281, episodes: 8000, mean episode reward: -42.38602514211941, num_cumulative_constraints: 26532, agent episode reward: [-42.38602514211941], time: 77.92
steps: 473863, episodes: 9000, mean episode reward: -42.28115075469634, num_cumulative_constraints: 27169, agent episode reward: [-42.28115075469634], time: 79.027
steps: 517765, episodes: 10000, mean episode reward: -42.15659468359263, num_cumulative_constraints: 27747, agent episode reward: [-42.15659468359263], time: 69.367
steps: 561606, episodes: 11000, mean episode reward: -41.96234218281111, num_cumulative_constraints: 28259, agent episode reward: [-41.96234218281111], time: 66.348
steps: 605082, episodes: 12000, mean episode reward: -42.090903237123314, num_cumulative_constraints: 28862, agent episode reward: [-42.090903237123314], time: 67.426
steps: 648443, episodes: 13000, mean episode reward: -41.951374386435276, num_cumulative_constraints: 29426, agent episode reward: [-41.951374386435276], time: 83.33
steps: 691177, episodes: 14000, mean episode reward: -41.5640441983278, num_cumulative_constraints: 29997, agent episode reward: [-41.5640441983278], time: 67.98
steps: 733511, episodes: 15000, mean episode reward: -41.13744772942601, num_cumulative_constraints: 30479, agent episode reward: [-41.13744772942601], time: 62.771
steps: 776100, episodes: 16000, mean episode reward: -41.144898351950424, num_cumulative_constraints: 30876, agent episode reward: [-41.144898351950424], time: 67.357
steps: 817839, episodes: 17000, mean episode reward: -40.97404371550394, num_cumulative_constraints: 31380, agent episode reward: [-40.97404371550394], time: 61.67
steps: 859486, episodes: 18000, mean episode reward: -40.95000829292416, num_cumulative_constraints: 31847, agent episode reward: [-40.95000829292416], time: 71.354
steps: 900914, episodes: 19000, mean episode reward: -40.666280498911924, num_cumulative_constraints: 32317, agent episode reward: [-40.666280498911924], time: 81.7
steps: 942372, episodes: 20000, mean episode reward: -40.89425909327231, num_cumulative_constraints: 32847, agent episode reward: [-40.89425909327231], time: 60.601
steps: 983554, episodes: 21000, mean episode reward: -40.636650554510915, num_cumulative_constraints: 33348, agent episode reward: [-40.636650554510915], time: 65.555
steps: 1024685, episodes: 22000, mean episode reward: -40.707321090702784, num_cumulative_constraints: 33872, agent episode reward: [-40.707321090702784], time: 59.048
steps: 1065839, episodes: 23000, mean episode reward: -40.662222526143395, num_cumulative_constraints: 34358, agent episode reward: [-40.662222526143395], time: 65.471
steps: 1106816, episodes: 24000, mean episode reward: -40.60963757204622, num_cumulative_constraints: 34867, agent episode reward: [-40.60963757204622], time: 70.605
steps: 1147799, episodes: 25000, mean episode reward: -40.41430924886609, num_cumulative_constraints: 35302, agent episode reward: [-40.41430924886609], time: 68.736
steps: 1188835, episodes: 26000, mean episode reward: -40.49854478999861, num_cumulative_constraints: 35716, agent episode reward: [-40.49854478999861], time: 73.659
steps: 1229723, episodes: 27000, mean episode reward: -40.429151235724625, num_cumulative_constraints: 36142, agent episode reward: [-40.429151235724625], time: 73.141
steps: 1270591, episodes: 28000, mean episode reward: -40.43052337346581, num_cumulative_constraints: 36588, agent episode reward: [-40.43052337346581], time: 64.461
steps: 1311479, episodes: 29000, mean episode reward: -40.33672351855592, num_cumulative_constraints: 36984, agent episode reward: [-40.33672351855592], time: 56.468
steps: 1352459, episodes: 30000, mean episode reward: -40.576558563888184, num_cumulative_constraints: 37445, agent episode reward: [-40.576558563888184], time: 63.638
steps: 1393339, episodes: 31000, mean episode reward: -40.3334408934471, num_cumulative_constraints: 37887, agent episode reward: [-40.3334408934471], time: 65.244
steps: 1434316, episodes: 32000, mean episode reward: -40.57931431990694, num_cumulative_constraints: 38349, agent episode reward: [-40.57931431990694], time: 77.471
steps: 1475331, episodes: 33000, mean episode reward: -40.243798969903175, num_cumulative_constraints: 38718, agent episode reward: [-40.243798969903175], time: 71.861
steps: 1516291, episodes: 34000, mean episode reward: -40.71337673927813, num_cumulative_constraints: 39234, agent episode reward: [-40.71337673927813], time: 83.971
steps: 1557193, episodes: 35000, mean episode reward: -40.36365042468671, num_cumulative_constraints: 39678, agent episode reward: [-40.36365042468671], time: 74.848
steps: 1598268, episodes: 36000, mean episode reward: -40.500049918208305, num_cumulative_constraints: 40106, agent episode reward: [-40.500049918208305], time: 78.668
steps: 1639343, episodes: 37000, mean episode reward: -40.135247182550444, num_cumulative_constraints: 40445, agent episode reward: [-40.135247182550444], time: 72.803
steps: 1680452, episodes: 38000, mean episode reward: -40.33294877474105, num_cumulative_constraints: 40825, agent episode reward: [-40.33294877474105], time: 80.947
steps: 1721500, episodes: 39000, mean episode reward: -40.2827029254425, num_cumulative_constraints: 41192, agent episode reward: [-40.2827029254425], time: 87.101
steps: 1762494, episodes: 40000, mean episode reward: -40.41684358970876, num_cumulative_constraints: 41619, agent episode reward: [-40.41684358970876], time: 74.45
steps: 1803459, episodes: 41000, mean episode reward: -40.048255146329026, num_cumulative_constraints: 41920, agent episode reward: [-40.048255146329026], time: 73.487
steps: 1844330, episodes: 42000, mean episode reward: -40.06868549605066, num_cumulative_constraints: 42282, agent episode reward: [-40.06868549605066], time: 79.626
steps: 1885853, episodes: 43000, mean episode reward: -40.369506799669274, num_cumulative_constraints: 42594, agent episode reward: [-40.369506799669274], time: 81.175
steps: 1927837, episodes: 44000, mean episode reward: -40.27033108861924, num_cumulative_constraints: 42840, agent episode reward: [-40.27033108861924], time: 76.606
steps: 1968688, episodes: 45000, mean episode reward: -39.78642298669807, num_cumulative_constraints: 43120, agent episode reward: [-39.78642298669807], time: 77.494
steps: 2009697, episodes: 46000, mean episode reward: -39.937997461257225, num_cumulative_constraints: 43423, agent episode reward: [-39.937997461257225], time: 78.205
steps: 2050487, episodes: 47000, mean episode reward: -39.86526917432494, num_cumulative_constraints: 43720, agent episode reward: [-39.86526917432494], time: 76.838
steps: 2091325, episodes: 48000, mean episode reward: -39.81772170868104, num_cumulative_constraints: 44031, agent episode reward: [-39.81772170868104], time: 73.87
steps: 2132120, episodes: 49000, mean episode reward: -39.6696127455722, num_cumulative_constraints: 44300, agent episode reward: [-39.6696127455722], time: 70.227
steps: 2172702, episodes: 50000, mean episode reward: -39.41940418882118, num_cumulative_constraints: 44544, agent episode reward: [-39.41940418882118], time: 70.025
steps: 2213479, episodes: 51000, mean episode reward: -39.67563199863864, num_cumulative_constraints: 44801, agent episode reward: [-39.67563199863864], time: 71.67
steps: 2253991, episodes: 52000, mean episode reward: -39.50175151568297, num_cumulative_constraints: 45049, agent episode reward: [-39.50175151568297], time: 70.311
steps: 2294576, episodes: 53000, mean episode reward: -39.33456930413689, num_cumulative_constraints: 45270, agent episode reward: [-39.33456930413689], time: 69.89
steps: 2335199, episodes: 54000, mean episode reward: -39.66406108205098, num_cumulative_constraints: 45549, agent episode reward: [-39.66406108205098], time: 76.375
steps: 2375679, episodes: 55000, mean episode reward: -39.437239606649385, num_cumulative_constraints: 45836, agent episode reward: [-39.437239606649385], time: 79.634
steps: 2416145, episodes: 56000, mean episode reward: -39.22135745425063, num_cumulative_constraints: 46032, agent episode reward: [-39.22135745425063], time: 79.623
steps: 2456735, episodes: 57000, mean episode reward: -39.30087411434887, num_cumulative_constraints: 46198, agent episode reward: [-39.30087411434887], time: 74.878
steps: 2497304, episodes: 58000, mean episode reward: -39.37729192054605, num_cumulative_constraints: 46415, agent episode reward: [-39.37729192054605], time: 70.848
steps: 2537869, episodes: 59000, mean episode reward: -39.4347365614155, num_cumulative_constraints: 46638, agent episode reward: [-39.4347365614155], time: 64.537
steps: 2578393, episodes: 60000, mean episode reward: -39.094732253610275, num_cumulative_constraints: 46781, agent episode reward: [-39.094732253610275], time: 58.226
steps: 2618889, episodes: 61000, mean episode reward: -39.375337396639345, num_cumulative_constraints: 47042, agent episode reward: [-39.375337396639345], time: 59.556
steps: 2659532, episodes: 62000, mean episode reward: -39.778044231813716, num_cumulative_constraints: 47382, agent episode reward: [-39.778044231813716], time: 64.93
steps: 2700153, episodes: 63000, mean episode reward: -39.42402696800148, num_cumulative_constraints: 47597, agent episode reward: [-39.42402696800148], time: 55.36
steps: 2740698, episodes: 64000, mean episode reward: -39.23436915661146, num_cumulative_constraints: 47765, agent episode reward: [-39.23436915661146], time: 58.542
steps: 2781158, episodes: 65000, mean episode reward: -39.10928023231053, num_cumulative_constraints: 47935, agent episode reward: [-39.10928023231053], time: 57.374
steps: 2821850, episodes: 66000, mean episode reward: -39.47022171953322, num_cumulative_constraints: 48139, agent episode reward: [-39.47022171953322], time: 68.603
steps: 2862293, episodes: 67000, mean episode reward: -39.13809197892863, num_cumulative_constraints: 48302, agent episode reward: [-39.13809197892863], time: 63.9
steps: 2902824, episodes: 68000, mean episode reward: -39.22926496579655, num_cumulative_constraints: 48483, agent episode reward: [-39.22926496579655], time: 58.259
steps: 2943394, episodes: 69000, mean episode reward: -39.43624382330335, num_cumulative_constraints: 48707, agent episode reward: [-39.43624382330335], time: 66.811
steps: 2983756, episodes: 70000, mean episode reward: -39.022881883934474, num_cumulative_constraints: 48878, agent episode reward: [-39.022881883934474], time: 59.209
steps: 3024201, episodes: 71000, mean episode reward: -39.07935757251733, num_cumulative_constraints: 49029, agent episode reward: [-39.07935757251733], time: 71.638
steps: 3064693, episodes: 72000, mean episode reward: -39.28527919763531, num_cumulative_constraints: 49246, agent episode reward: [-39.28527919763531], time: 58.574
steps: 3105539, episodes: 73000, mean episode reward: -39.33272210193561, num_cumulative_constraints: 49428, agent episode reward: [-39.33272210193561], time: 57.142
steps: 3146152, episodes: 74000, mean episode reward: -39.37754445563944, num_cumulative_constraints: 49620, agent episode reward: [-39.37754445563944], time: 51.851
steps: 3186892, episodes: 75000, mean episode reward: -39.39753039563033, num_cumulative_constraints: 49852, agent episode reward: [-39.39753039563033], time: 61.209
steps: 3227612, episodes: 76000, mean episode reward: -39.524406200604005, num_cumulative_constraints: 50089, agent episode reward: [-39.524406200604005], time: 67.112
steps: 3268198, episodes: 77000, mean episode reward: -39.38461839103192, num_cumulative_constraints: 50306, agent episode reward: [-39.38461839103192], time: 60.106
steps: 3308862, episodes: 78000, mean episode reward: -39.3149329525372, num_cumulative_constraints: 50479, agent episode reward: [-39.3149329525372], time: 70.186
steps: 3349528, episodes: 79000, mean episode reward: -39.12294373148529, num_cumulative_constraints: 50628, agent episode reward: [-39.12294373148529], time: 62.467
steps: 3390449, episodes: 80000, mean episode reward: -39.62529183449889, num_cumulative_constraints: 50835, agent episode reward: [-39.62529183449889], time: 73.755
steps: 3431048, episodes: 81000, mean episode reward: -39.16699336161417, num_cumulative_constraints: 51003, agent episode reward: [-39.16699336161417], time: 67.363
steps: 3471610, episodes: 82000, mean episode reward: -39.248503699196604, num_cumulative_constraints: 51196, agent episode reward: [-39.248503699196604], time: 67.877
steps: 3512109, episodes: 83000, mean episode reward: -39.28034793504175, num_cumulative_constraints: 51389, agent episode reward: [-39.28034793504175], time: 72.57
steps: 3552769, episodes: 84000, mean episode reward: -39.533539324884885, num_cumulative_constraints: 51621, agent episode reward: [-39.533539324884885], time: 61.281
steps: 3593350, episodes: 85000, mean episode reward: -39.48229749753691, num_cumulative_constraints: 51862, agent episode reward: [-39.48229749753691], time: 68.431
steps: 3634109, episodes: 86000, mean episode reward: -39.43848379663004, num_cumulative_constraints: 52074, agent episode reward: [-39.43848379663004], time: 65.903
steps: 3674972, episodes: 87000, mean episode reward: -39.29763415067298, num_cumulative_constraints: 52233, agent episode reward: [-39.29763415067298], time: 65.66
steps: 3715694, episodes: 88000, mean episode reward: -39.44582544938818, num_cumulative_constraints: 52437, agent episode reward: [-39.44582544938818], time: 52.449
steps: 3756257, episodes: 89000, mean episode reward: -39.002458815998644, num_cumulative_constraints: 52571, agent episode reward: [-39.002458815998644], time: 71.566
steps: 3796852, episodes: 90000, mean episode reward: -39.41583678243957, num_cumulative_constraints: 52797, agent episode reward: [-39.41583678243957], time: 71.713
steps: 3837405, episodes: 91000, mean episode reward: -39.258036960511156, num_cumulative_constraints: 52981, agent episode reward: [-39.258036960511156], time: 63.557
steps: 3877889, episodes: 92000, mean episode reward: -39.37591663519165, num_cumulative_constraints: 53193, agent episode reward: [-39.37591663519165], time: 59.022
steps: 3918343, episodes: 93000, mean episode reward: -39.30589644281924, num_cumulative_constraints: 53424, agent episode reward: [-39.30589644281924], time: 54.135
steps: 3958688, episodes: 94000, mean episode reward: -39.09627662874783, num_cumulative_constraints: 53610, agent episode reward: [-39.09627662874783], time: 67.548
steps: 3999193, episodes: 95000, mean episode reward: -39.28592136210157, num_cumulative_constraints: 53778, agent episode reward: [-39.28592136210157], time: 61.311
steps: 4039688, episodes: 96000, mean episode reward: -39.189645468346626, num_cumulative_constraints: 53963, agent episode reward: [-39.189645468346626], time: 66.578
steps: 4080213, episodes: 97000, mean episode reward: -39.15136896798339, num_cumulative_constraints: 54132, agent episode reward: [-39.15136896798339], time: 66.141
steps: 4120824, episodes: 98000, mean episode reward: -39.15211147354518, num_cumulative_constraints: 54267, agent episode reward: [-39.15211147354518], time: 66.841
steps: 4161268, episodes: 99000, mean episode reward: -39.121866056788534, num_cumulative_constraints: 54454, agent episode reward: [-39.121866056788534], time: 61.609
steps: 4201883, episodes: 100000, mean episode reward: -39.36625765287765, num_cumulative_constraints: 54639, agent episode reward: [-39.36625765287765], time: 67.817
steps: 4242512, episodes: 101000, mean episode reward: -39.35468933163785, num_cumulative_constraints: 54861, agent episode reward: [-39.35468933163785], time: 68.408
steps: 4283360, episodes: 102000, mean episode reward: -39.39984126659922, num_cumulative_constraints: 55081, agent episode reward: [-39.39984126659922], time: 69.412
steps: 4323988, episodes: 103000, mean episode reward: -39.26173590111568, num_cumulative_constraints: 55240, agent episode reward: [-39.26173590111568], time: 68.516
steps: 4364662, episodes: 104000, mean episode reward: -39.32940676723484, num_cumulative_constraints: 55440, agent episode reward: [-39.32940676723484], time: 64.909
steps: 4405262, episodes: 105000, mean episode reward: -39.251411793098725, num_cumulative_constraints: 55632, agent episode reward: [-39.251411793098725], time: 69.727
steps: 4445681, episodes: 106000, mean episode reward: -38.99754376253183, num_cumulative_constraints: 55784, agent episode reward: [-38.99754376253183], time: 71.331
steps: 4486131, episodes: 107000, mean episode reward: -39.02901466775507, num_cumulative_constraints: 55914, agent episode reward: [-39.02901466775507], time: 61.19
steps: 4526631, episodes: 108000, mean episode reward: -39.262196878326485, num_cumulative_constraints: 56110, agent episode reward: [-39.262196878326485], time: 72.796
steps: 4567141, episodes: 109000, mean episode reward: -39.03552057269757, num_cumulative_constraints: 56243, agent episode reward: [-39.03552057269757], time: 65.229
steps: 4607629, episodes: 110000, mean episode reward: -39.02343299582867, num_cumulative_constraints: 56368, agent episode reward: [-39.02343299582867], time: 72.514
steps: 4648095, episodes: 111000, mean episode reward: -39.37166395648796, num_cumulative_constraints: 56607, agent episode reward: [-39.37166395648796], time: 70.952
steps: 4688595, episodes: 112000, mean episode reward: -39.2813110602544, num_cumulative_constraints: 56795, agent episode reward: [-39.2813110602544], time: 61.525
steps: 4728995, episodes: 113000, mean episode reward: -38.947616011175036, num_cumulative_constraints: 56932, agent episode reward: [-38.947616011175036], time: 65.808
steps: 4769537, episodes: 114000, mean episode reward: -39.11319627782985, num_cumulative_constraints: 57100, agent episode reward: [-39.11319627782985], time: 62.862
steps: 4809929, episodes: 115000, mean episode reward: -39.11511491838835, num_cumulative_constraints: 57284, agent episode reward: [-39.11511491838835], time: 62.466
steps: 4850424, episodes: 116000, mean episode reward: -39.277023199512506, num_cumulative_constraints: 57512, agent episode reward: [-39.277023199512506], time: 70.498
steps: 4890849, episodes: 117000, mean episode reward: -39.33270986852026, num_cumulative_constraints: 57770, agent episode reward: [-39.33270986852026], time: 52.94
steps: 4931255, episodes: 118000, mean episode reward: -39.079108247847515, num_cumulative_constraints: 57958, agent episode reward: [-39.079108247847515], time: 69.735
steps: 4971643, episodes: 119000, mean episode reward: -39.23855727858844, num_cumulative_constraints: 58189, agent episode reward: [-39.23855727858844], time: 60.048
steps: 5012169, episodes: 120000, mean episode reward: -39.37673556681124, num_cumulative_constraints: 58452, agent episode reward: [-39.37673556681124], time: 51.512
steps: 5052771, episodes: 121000, mean episode reward: -39.46199374732022, num_cumulative_constraints: 58674, agent episode reward: [-39.46199374732022], time: 61.85
steps: 5093219, episodes: 122000, mean episode reward: -39.14800500572298, num_cumulative_constraints: 58878, agent episode reward: [-39.14800500572298], time: 65.797
steps: 5133853, episodes: 123000, mean episode reward: -39.39723012425618, num_cumulative_constraints: 59076, agent episode reward: [-39.39723012425618], time: 56.56
steps: 5174312, episodes: 124000, mean episode reward: -39.16851342878922, num_cumulative_constraints: 59272, agent episode reward: [-39.16851342878922], time: 62.78
steps: 5214945, episodes: 125000, mean episode reward: -39.405381259701535, num_cumulative_constraints: 59466, agent episode reward: [-39.405381259701535], time: 70.988
steps: 5255360, episodes: 126000, mean episode reward: -39.17872228049896, num_cumulative_constraints: 59688, agent episode reward: [-39.17872228049896], time: 58.386
steps: 5295907, episodes: 127000, mean episode reward: -39.28666429099341, num_cumulative_constraints: 59853, agent episode reward: [-39.28666429099341], time: 57.264
steps: 5336485, episodes: 128000, mean episode reward: -39.327045641368336, num_cumulative_constraints: 60059, agent episode reward: [-39.327045641368336], time: 62.725
steps: 5376791, episodes: 129000, mean episode reward: -38.92240985067996, num_cumulative_constraints: 60210, agent episode reward: [-38.92240985067996], time: 73.089
steps: 5417203, episodes: 130000, mean episode reward: -39.24980759147553, num_cumulative_constraints: 60429, agent episode reward: [-39.24980759147553], time: 69.817
steps: 5457573, episodes: 131000, mean episode reward: -39.15086018428734, num_cumulative_constraints: 60645, agent episode reward: [-39.15086018428734], time: 60.111
steps: 5497992, episodes: 132000, mean episode reward: -39.29368236897365, num_cumulative_constraints: 60845, agent episode reward: [-39.29368236897365], time: 61.374
steps: 5538379, episodes: 133000, mean episode reward: -38.965589317991444, num_cumulative_constraints: 61021, agent episode reward: [-38.965589317991444], time: 60.294
steps: 5578889, episodes: 134000, mean episode reward: -39.43988277833442, num_cumulative_constraints: 61285, agent episode reward: [-39.43988277833442], time: 57.062
steps: 5619387, episodes: 135000, mean episode reward: -39.256685144558524, num_cumulative_constraints: 61489, agent episode reward: [-39.256685144558524], time: 67.347
steps: 5659845, episodes: 136000, mean episode reward: -39.21141562383667, num_cumulative_constraints: 61681, agent episode reward: [-39.21141562383667], time: 70.298
steps: 5700337, episodes: 137000, mean episode reward: -39.10475354842008, num_cumulative_constraints: 61846, agent episode reward: [-39.10475354842008], time: 69.774
steps: 5740951, episodes: 138000, mean episode reward: -39.3509932283063, num_cumulative_constraints: 62063, agent episode reward: [-39.3509932283063], time: 69.574
steps: 5781551, episodes: 139000, mean episode reward: -39.19233081177651, num_cumulative_constraints: 62254, agent episode reward: [-39.19233081177651], time: 70.707
steps: 5822199, episodes: 140000, mean episode reward: -39.29183525963286, num_cumulative_constraints: 62415, agent episode reward: [-39.29183525963286], time: 76.235
steps: 5862838, episodes: 141000, mean episode reward: -39.14790729463646, num_cumulative_constraints: 62583, agent episode reward: [-39.14790729463646], time: 70.983
steps: 5903621, episodes: 142000, mean episode reward: -39.385817103166715, num_cumulative_constraints: 62752, agent episode reward: [-39.385817103166715], time: 69.711
steps: 5944143, episodes: 143000, mean episode reward: -39.242283891431775, num_cumulative_constraints: 62973, agent episode reward: [-39.242283891431775], time: 76.145
steps: 5984717, episodes: 144000, mean episode reward: -39.17114109894837, num_cumulative_constraints: 63130, agent episode reward: [-39.17114109894837], time: 72.939
steps: 6025103, episodes: 145000, mean episode reward: -38.98610339168296, num_cumulative_constraints: 63295, agent episode reward: [-38.98610339168296], time: 75.009
steps: 6065521, episodes: 146000, mean episode reward: -39.044961370362174, num_cumulative_constraints: 63440, agent episode reward: [-39.044961370362174], time: 63.998
steps: 6105998, episodes: 147000, mean episode reward: -39.20358311322561, num_cumulative_constraints: 63647, agent episode reward: [-39.20358311322561], time: 55.38
steps: 6146298, episodes: 148000, mean episode reward: -39.0465552420571, num_cumulative_constraints: 63858, agent episode reward: [-39.0465552420571], time: 60.155
steps: 6186583, episodes: 149000, mean episode reward: -39.2713142020725, num_cumulative_constraints: 64131, agent episode reward: [-39.2713142020725], time: 61.072
steps: 6226985, episodes: 150000, mean episode reward: -39.239084169968955, num_cumulative_constraints: 64351, agent episode reward: [-39.239084169968955], time: 62.341
steps: 6267374, episodes: 151000, mean episode reward: -39.21709711774545, num_cumulative_constraints: 64562, agent episode reward: [-39.21709711774545], time: 59.657
steps: 6307729, episodes: 152000, mean episode reward: -39.058855003728894, num_cumulative_constraints: 64762, agent episode reward: [-39.058855003728894], time: 59.758
steps: 6348245, episodes: 153000, mean episode reward: -39.21786702832472, num_cumulative_constraints: 64975, agent episode reward: [-39.21786702832472], time: 60.108
steps: 6388746, episodes: 154000, mean episode reward: -39.2298264966852, num_cumulative_constraints: 65205, agent episode reward: [-39.2298264966852], time: 63.543
steps: 6429153, episodes: 155000, mean episode reward: -39.26735027562967, num_cumulative_constraints: 65450, agent episode reward: [-39.26735027562967], time: 69.499
steps: 6469618, episodes: 156000, mean episode reward: -39.24454633198772, num_cumulative_constraints: 65670, agent episode reward: [-39.24454633198772], time: 70.682
steps: 6510338, episodes: 157000, mean episode reward: -39.32752843509118, num_cumulative_constraints: 65880, agent episode reward: [-39.32752843509118], time: 61.402
steps: 6550917, episodes: 158000, mean episode reward: -39.40246053633559, num_cumulative_constraints: 66143, agent episode reward: [-39.40246053633559], time: 68.299
steps: 6591746, episodes: 159000, mean episode reward: -39.2963022762905, num_cumulative_constraints: 66339, agent episode reward: [-39.2963022762905], time: 62.139
steps: 6632226, episodes: 160000, mean episode reward: -39.2321101064402, num_cumulative_constraints: 66546, agent episode reward: [-39.2321101064402], time: 69.107
steps: 6672799, episodes: 161000, mean episode reward: -39.05324079111658, num_cumulative_constraints: 66694, agent episode reward: [-39.05324079111658], time: 54.255
steps: 6713455, episodes: 162000, mean episode reward: -39.45705927855013, num_cumulative_constraints: 66925, agent episode reward: [-39.45705927855013], time: 65.659
steps: 6754099, episodes: 163000, mean episode reward: -39.0163388733625, num_cumulative_constraints: 67068, agent episode reward: [-39.0163388733625], time: 70.404
steps: 6794712, episodes: 164000, mean episode reward: -39.00019604865096, num_cumulative_constraints: 67202, agent episode reward: [-39.00019604865096], time: 79.769
steps: 6835257, episodes: 165000, mean episode reward: -39.2636188284808, num_cumulative_constraints: 67454, agent episode reward: [-39.2636188284808], time: 71.191
steps: 6875925, episodes: 166000, mean episode reward: -39.35845240216054, num_cumulative_constraints: 67668, agent episode reward: [-39.35845240216054], time: 63.047
steps: 6916640, episodes: 167000, mean episode reward: -39.31053935769802, num_cumulative_constraints: 67884, agent episode reward: [-39.31053935769802], time: 72.239
steps: 6957239, episodes: 168000, mean episode reward: -39.26167105940962, num_cumulative_constraints: 68106, agent episode reward: [-39.26167105940962], time: 65.291
steps: 6997847, episodes: 169000, mean episode reward: -39.2750812695743, num_cumulative_constraints: 68335, agent episode reward: [-39.2750812695743], time: 58.56
steps: 7038367, episodes: 170000, mean episode reward: -39.14619368251227, num_cumulative_constraints: 68519, agent episode reward: [-39.14619368251227], time: 62.506
steps: 7078837, episodes: 171000, mean episode reward: -38.83445804677643, num_cumulative_constraints: 68636, agent episode reward: [-38.83445804677643], time: 64.632
steps: 7119251, episodes: 172000, mean episode reward: -38.95843173866567, num_cumulative_constraints: 68803, agent episode reward: [-38.95843173866567], time: 68.565
steps: 7159909, episodes: 173000, mean episode reward: -39.18707243765107, num_cumulative_constraints: 68939, agent episode reward: [-39.18707243765107], time: 67.112
steps: 7200580, episodes: 174000, mean episode reward: -38.971668845067754, num_cumulative_constraints: 69103, agent episode reward: [-38.971668845067754], time: 59.15
steps: 7241447, episodes: 175000, mean episode reward: -39.33722005221664, num_cumulative_constraints: 69304, agent episode reward: [-39.33722005221664], time: 59.841
steps: 7282079, episodes: 176000, mean episode reward: -38.93901980565901, num_cumulative_constraints: 69479, agent episode reward: [-38.93901980565901], time: 57.915
steps: 7322686, episodes: 177000, mean episode reward: -39.00225739738522, num_cumulative_constraints: 69620, agent episode reward: [-39.00225739738522], time: 63.779
steps: 7363384, episodes: 178000, mean episode reward: -39.16551776785133, num_cumulative_constraints: 69823, agent episode reward: [-39.16551776785133], time: 58.339
steps: 7404189, episodes: 179000, mean episode reward: -39.447518010688, num_cumulative_constraints: 70031, agent episode reward: [-39.447518010688], time: 67.404
steps: 7444764, episodes: 180000, mean episode reward: -39.04328676250326, num_cumulative_constraints: 70171, agent episode reward: [-39.04328676250326], time: 61.016
steps: 7485537, episodes: 181000, mean episode reward: -39.103498896794726, num_cumulative_constraints: 70329, agent episode reward: [-39.103498896794726], time: 56.976
steps: 7526662, episodes: 182000, mean episode reward: -39.32160289747386, num_cumulative_constraints: 70470, agent episode reward: [-39.32160289747386], time: 61.685
steps: 7567295, episodes: 183000, mean episode reward: -38.99133666427563, num_cumulative_constraints: 70633, agent episode reward: [-38.99133666427563], time: 62.644
steps: 7607906, episodes: 184000, mean episode reward: -39.20367760064086, num_cumulative_constraints: 70840, agent episode reward: [-39.20367760064086], time: 64.681
steps: 7648443, episodes: 185000, mean episode reward: -39.12229399538163, num_cumulative_constraints: 71022, agent episode reward: [-39.12229399538163], time: 57.329
steps: 7689156, episodes: 186000, mean episode reward: -39.13151584578131, num_cumulative_constraints: 71187, agent episode reward: [-39.13151584578131], time: 59.683
steps: 7729749, episodes: 187000, mean episode reward: -39.326679577804924, num_cumulative_constraints: 71409, agent episode reward: [-39.326679577804924], time: 60.173
steps: 7770216, episodes: 188000, mean episode reward: -39.09283883990216, num_cumulative_constraints: 71585, agent episode reward: [-39.09283883990216], time: 67.53
steps: 7810660, episodes: 189000, mean episode reward: -39.057943702445215, num_cumulative_constraints: 71769, agent episode reward: [-39.057943702445215], time: 68.997
steps: 7851084, episodes: 190000, mean episode reward: -39.09660884946512, num_cumulative_constraints: 71955, agent episode reward: [-39.09660884946512], time: 69.316
steps: 7891590, episodes: 191000, mean episode reward: -39.20901251562715, num_cumulative_constraints: 72175, agent episode reward: [-39.20901251562715], time: 66.902
steps: 7932115, episodes: 192000, mean episode reward: -39.23055310266011, num_cumulative_constraints: 72377, agent episode reward: [-39.23055310266011], time: 72.698
steps: 7972521, episodes: 193000, mean episode reward: -39.076281836868965, num_cumulative_constraints: 72599, agent episode reward: [-39.076281836868965], time: 59.768
steps: 8013032, episodes: 194000, mean episode reward: -39.21152083972957, num_cumulative_constraints: 72797, agent episode reward: [-39.21152083972957], time: 69.788
steps: 8053555, episodes: 195000, mean episode reward: -39.109697997446595, num_cumulative_constraints: 72983, agent episode reward: [-39.109697997446595], time: 70.449
steps: 8094109, episodes: 196000, mean episode reward: -39.16838511257976, num_cumulative_constraints: 73183, agent episode reward: [-39.16838511257976], time: 59.51
steps: 8134656, episodes: 197000, mean episode reward: -39.074701923943394, num_cumulative_constraints: 73359, agent episode reward: [-39.074701923943394], time: 70.757
steps: 8175227, episodes: 198000, mean episode reward: -39.21000246889623, num_cumulative_constraints: 73571, agent episode reward: [-39.21000246889623], time: 61.77
steps: 8215872, episodes: 199000, mean episode reward: -39.20978089310651, num_cumulative_constraints: 73731, agent episode reward: [-39.20978089310651], time: 58.692
steps: 8256459, episodes: 200000, mean episode reward: -39.13668162138794, num_cumulative_constraints: 73886, agent episode reward: [-39.13668162138794], time: 65.859
steps: 8296989, episodes: 201000, mean episode reward: -39.239524270652566, num_cumulative_constraints: 74084, agent episode reward: [-39.239524270652566], time: 67.7
steps: 8337557, episodes: 202000, mean episode reward: -39.32905990197217, num_cumulative_constraints: 74301, agent episode reward: [-39.32905990197217], time: 60.301
steps: 8378069, episodes: 203000, mean episode reward: -39.31605550132861, num_cumulative_constraints: 74543, agent episode reward: [-39.31605550132861], time: 63.256
steps: 8418576, episodes: 204000, mean episode reward: -39.38030631453723, num_cumulative_constraints: 74799, agent episode reward: [-39.38030631453723], time: 66.599
steps: 8459108, episodes: 205000, mean episode reward: -39.165108866964964, num_cumulative_constraints: 74992, agent episode reward: [-39.165108866964964], time: 61.039
steps: 8499591, episodes: 206000, mean episode reward: -39.1498115719459, num_cumulative_constraints: 75228, agent episode reward: [-39.1498115719459], time: 62.531
steps: 8540189, episodes: 207000, mean episode reward: -39.00165007648448, num_cumulative_constraints: 75352, agent episode reward: [-39.00165007648448], time: 62.827
steps: 8580822, episodes: 208000, mean episode reward: -39.24922333890505, num_cumulative_constraints: 75527, agent episode reward: [-39.24922333890505], time: 61.986
steps: 8621414, episodes: 209000, mean episode reward: -39.12534521149418, num_cumulative_constraints: 75682, agent episode reward: [-39.12534521149418], time: 61.542
steps: 8662218, episodes: 210000, mean episode reward: -39.215493206322044, num_cumulative_constraints: 75829, agent episode reward: [-39.215493206322044], time: 65.371
steps: 8703022, episodes: 211000, mean episode reward: -39.19714505034937, num_cumulative_constraints: 75958, agent episode reward: [-39.19714505034937], time: 68.672
steps: 8743663, episodes: 212000, mean episode reward: -39.20337941929488, num_cumulative_constraints: 76124, agent episode reward: [-39.20337941929488], time: 67.448
steps: 8784295, episodes: 213000, mean episode reward: -39.20418661171904, num_cumulative_constraints: 76291, agent episode reward: [-39.20418661171904], time: 62.602
steps: 8824990, episodes: 214000, mean episode reward: -39.2104506118116, num_cumulative_constraints: 76436, agent episode reward: [-39.2104506118116], time: 66.976
steps: 8865798, episodes: 215000, mean episode reward: -39.134795514703015, num_cumulative_constraints: 76572, agent episode reward: [-39.134795514703015], time: 65.685
steps: 8906556, episodes: 216000, mean episode reward: -39.38871936066257, num_cumulative_constraints: 76770, agent episode reward: [-39.38871936066257], time: 64.554
steps: 8947459, episodes: 217000, mean episode reward: -39.367280680985395, num_cumulative_constraints: 76914, agent episode reward: [-39.367280680985395], time: 74.576
steps: 8988259, episodes: 218000, mean episode reward: -39.240894167650445, num_cumulative_constraints: 77062, agent episode reward: [-39.240894167650445], time: 71.83
steps: 9028943, episodes: 219000, mean episode reward: -39.25892809648659, num_cumulative_constraints: 77233, agent episode reward: [-39.25892809648659], time: 62.112
steps: 9069745, episodes: 220000, mean episode reward: -39.220701026969685, num_cumulative_constraints: 77386, agent episode reward: [-39.220701026969685], time: 60.846
steps: 9110298, episodes: 221000, mean episode reward: -39.159998028588326, num_cumulative_constraints: 77582, agent episode reward: [-39.159998028588326], time: 66.107
steps: 9151044, episodes: 222000, mean episode reward: -39.069159037575794, num_cumulative_constraints: 77731, agent episode reward: [-39.069159037575794], time: 58.245
steps: 9191505, episodes: 223000, mean episode reward: -38.997103588531864, num_cumulative_constraints: 77899, agent episode reward: [-38.997103588531864], time: 67.149
steps: 9232072, episodes: 224000, mean episode reward: -39.1853565719599, num_cumulative_constraints: 78095, agent episode reward: [-39.1853565719599], time: 59.479
steps: 9272926, episodes: 225000, mean episode reward: -39.43596408609512, num_cumulative_constraints: 78294, agent episode reward: [-39.43596408609512], time: 67.476
steps: 9313494, episodes: 226000, mean episode reward: -39.2679553005796, num_cumulative_constraints: 78519, agent episode reward: [-39.2679553005796], time: 59.487
steps: 9354147, episodes: 227000, mean episode reward: -39.208176120984646, num_cumulative_constraints: 78720, agent episode reward: [-39.208176120984646], time: 56.131
steps: 9394908, episodes: 228000, mean episode reward: -39.23901232824295, num_cumulative_constraints: 78918, agent episode reward: [-39.23901232824295], time: 60.111
steps: 9435646, episodes: 229000, mean episode reward: -39.401485337976425, num_cumulative_constraints: 79166, agent episode reward: [-39.401485337976425], time: 66.22
steps: 9476423, episodes: 230000, mean episode reward: -39.50367030386161, num_cumulative_constraints: 79394, agent episode reward: [-39.50367030386161], time: 77.803
steps: 9517096, episodes: 231000, mean episode reward: -39.39730614402979, num_cumulative_constraints: 79609, agent episode reward: [-39.39730614402979], time: 71.564
steps: 9557698, episodes: 232000, mean episode reward: -39.36504898261602, num_cumulative_constraints: 79870, agent episode reward: [-39.36504898261602], time: 65.895
steps: 9598375, episodes: 233000, mean episode reward: -39.172607282196445, num_cumulative_constraints: 80076, agent episode reward: [-39.172607282196445], time: 57.003
steps: 9639302, episodes: 234000, mean episode reward: -39.70139360524296, num_cumulative_constraints: 80342, agent episode reward: [-39.70139360524296], time: 55.147
steps: 9680135, episodes: 235000, mean episode reward: -39.486017629184175, num_cumulative_constraints: 80553, agent episode reward: [-39.486017629184175], time: 61.28
steps: 9720900, episodes: 236000, mean episode reward: -39.62308962009745, num_cumulative_constraints: 80837, agent episode reward: [-39.62308962009745], time: 63.559
steps: 9761763, episodes: 237000, mean episode reward: -39.534929803359, num_cumulative_constraints: 81047, agent episode reward: [-39.534929803359], time: 65.469
steps: 9802593, episodes: 238000, mean episode reward: -39.33280401787924, num_cumulative_constraints: 81237, agent episode reward: [-39.33280401787924], time: 67.747
steps: 9843389, episodes: 239000, mean episode reward: -39.42399969663214, num_cumulative_constraints: 81446, agent episode reward: [-39.42399969663214], time: 69.281
steps: 9884183, episodes: 240000, mean episode reward: -39.38237036207275, num_cumulative_constraints: 81635, agent episode reward: [-39.38237036207275], time: 51.684
steps: 9924970, episodes: 241000, mean episode reward: -39.51277099689799, num_cumulative_constraints: 81847, agent episode reward: [-39.51277099689799], time: 64.091
steps: 9965703, episodes: 242000, mean episode reward: -39.34271063678467, num_cumulative_constraints: 82045, agent episode reward: [-39.34271063678467], time: 51.59
steps: 10006469, episodes: 243000, mean episode reward: -39.2625753952168, num_cumulative_constraints: 82185, agent episode reward: [-39.2625753952168], time: 59.51
steps: 10047334, episodes: 244000, mean episode reward: -39.46582677946147, num_cumulative_constraints: 82369, agent episode reward: [-39.46582677946147], time: 59.775
steps: 10088128, episodes: 245000, mean episode reward: -39.38437368543085, num_cumulative_constraints: 82556, agent episode reward: [-39.38437368543085], time: 63.086
steps: 10128679, episodes: 246000, mean episode reward: -39.32615371007091, num_cumulative_constraints: 82777, agent episode reward: [-39.32615371007091], time: 61.394
steps: 10169151, episodes: 247000, mean episode reward: -39.12759144456884, num_cumulative_constraints: 82947, agent episode reward: [-39.12759144456884], time: 53.108
steps: 10209686, episodes: 248000, mean episode reward: -39.308945942129505, num_cumulative_constraints: 83159, agent episode reward: [-39.308945942129505], time: 51.522
steps: 10250261, episodes: 249000, mean episode reward: -39.21569420132249, num_cumulative_constraints: 83314, agent episode reward: [-39.21569420132249], time: 56.298
steps: 10290770, episodes: 250000, mean episode reward: -39.00426173588386, num_cumulative_constraints: 83436, agent episode reward: [-39.00426173588386], time: 63.652
steps: 10331418, episodes: 251000, mean episode reward: -39.35027781761529, num_cumulative_constraints: 83618, agent episode reward: [-39.35027781761529], time: 64.003
steps: 10372166, episodes: 252000, mean episode reward: -39.491567292789185, num_cumulative_constraints: 83842, agent episode reward: [-39.491567292789185], time: 67.72
steps: 10412802, episodes: 253000, mean episode reward: -39.22470412741993, num_cumulative_constraints: 84031, agent episode reward: [-39.22470412741993], time: 67.85
steps: 10453427, episodes: 254000, mean episode reward: -39.03602747985092, num_cumulative_constraints: 84167, agent episode reward: [-39.03602747985092], time: 70.324
steps: 10494043, episodes: 255000, mean episode reward: -39.07434456427491, num_cumulative_constraints: 84289, agent episode reward: [-39.07434456427491], time: 68.56
steps: 10534768, episodes: 256000, mean episode reward: -39.29749150683909, num_cumulative_constraints: 84460, agent episode reward: [-39.29749150683909], time: 61.15
steps: 10575618, episodes: 257000, mean episode reward: -39.320231733197, num_cumulative_constraints: 84637, agent episode reward: [-39.320231733197], time: 58.925
steps: 10616680, episodes: 258000, mean episode reward: -39.46099905222931, num_cumulative_constraints: 84805, agent episode reward: [-39.46099905222931], time: 64.512
steps: 10657608, episodes: 259000, mean episode reward: -39.36773175358199, num_cumulative_constraints: 84962, agent episode reward: [-39.36773175358199], time: 68.349
steps: 10698597, episodes: 260000, mean episode reward: -39.35407936431937, num_cumulative_constraints: 85157, agent episode reward: [-39.35407936431937], time: 65.989
steps: 10739848, episodes: 261000, mean episode reward: -39.571510551077054, num_cumulative_constraints: 85365, agent episode reward: [-39.571510551077054], time: 64.048
steps: 10781834, episodes: 262000, mean episode reward: -39.72291099400607, num_cumulative_constraints: 85507, agent episode reward: [-39.72291099400607], time: 67.849
steps: 10823920, episodes: 263000, mean episode reward: -39.842062680787755, num_cumulative_constraints: 85694, agent episode reward: [-39.842062680787755], time: 62.686
steps: 10865154, episodes: 264000, mean episode reward: -39.73081872791562, num_cumulative_constraints: 85958, agent episode reward: [-39.73081872791562], time: 69.475
steps: 10906333, episodes: 265000, mean episode reward: -39.44019113808055, num_cumulative_constraints: 86163, agent episode reward: [-39.44019113808055], time: 62.417
steps: 10947459, episodes: 266000, mean episode reward: -39.359121355244326, num_cumulative_constraints: 86302, agent episode reward: [-39.359121355244326], time: 59.167
steps: 10988552, episodes: 267000, mean episode reward: -39.56675099458677, num_cumulative_constraints: 86532, agent episode reward: [-39.56675099458677], time: 54.64
steps: 11029640, episodes: 268000, mean episode reward: -39.60977783933931, num_cumulative_constraints: 86730, agent episode reward: [-39.60977783933931], time: 58.916
steps: 11070775, episodes: 269000, mean episode reward: -39.38876979041481, num_cumulative_constraints: 86899, agent episode reward: [-39.38876979041481], time: 56.861
steps: 11111793, episodes: 270000, mean episode reward: -39.450563751017896, num_cumulative_constraints: 87084, agent episode reward: [-39.450563751017896], time: 53.284
steps: 11152998, episodes: 271000, mean episode reward: -39.454850831624135, num_cumulative_constraints: 87250, agent episode reward: [-39.454850831624135], time: 58.473
steps: 11194056, episodes: 272000, mean episode reward: -39.33063980699993, num_cumulative_constraints: 87411, agent episode reward: [-39.33063980699993], time: 60.45
steps: 11235225, episodes: 273000, mean episode reward: -39.5003442950834, num_cumulative_constraints: 87615, agent episode reward: [-39.5003442950834], time: 71.832
steps: 11276245, episodes: 274000, mean episode reward: -39.317729903180926, num_cumulative_constraints: 87763, agent episode reward: [-39.317729903180926], time: 64.367
steps: 11317703, episodes: 275000, mean episode reward: -39.68081938061485, num_cumulative_constraints: 87955, agent episode reward: [-39.68081938061485], time: 64.821
steps: 11359043, episodes: 276000, mean episode reward: -39.594011285277475, num_cumulative_constraints: 88139, agent episode reward: [-39.594011285277475], time: 58.348
steps: 11400451, episodes: 277000, mean episode reward: -39.642155828234294, num_cumulative_constraints: 88324, agent episode reward: [-39.642155828234294], time: 72.302
steps: 11441939, episodes: 278000, mean episode reward: -39.73631931233694, num_cumulative_constraints: 88507, agent episode reward: [-39.73631931233694], time: 67.343
steps: 11483489, episodes: 279000, mean episode reward: -39.63390181754797, num_cumulative_constraints: 88667, agent episode reward: [-39.63390181754797], time: 58.426
steps: 11525535, episodes: 280000, mean episode reward: -39.94056110955412, num_cumulative_constraints: 88844, agent episode reward: [-39.94056110955412], time: 65.89
steps: 11567446, episodes: 281000, mean episode reward: -39.76243131593709, num_cumulative_constraints: 89020, agent episode reward: [-39.76243131593709], time: 63.248
steps: 11609629, episodes: 282000, mean episode reward: -40.113272176424275, num_cumulative_constraints: 89238, agent episode reward: [-40.113272176424275], time: 67.702
steps: 11651567, episodes: 283000, mean episode reward: -39.42669710100406, num_cumulative_constraints: 89351, agent episode reward: [-39.42669710100406], time: 58.322
steps: 11693465, episodes: 284000, mean episode reward: -40.00402282424381, num_cumulative_constraints: 89600, agent episode reward: [-40.00402282424381], time: 70.82
steps: 11735375, episodes: 285000, mean episode reward: -39.837477337198905, num_cumulative_constraints: 89800, agent episode reward: [-39.837477337198905], time: 67.294
steps: 11777245, episodes: 286000, mean episode reward: -39.92202454011712, num_cumulative_constraints: 90018, agent episode reward: [-39.92202454011712], time: 62.718
steps: 11819042, episodes: 287000, mean episode reward: -39.64317956382455, num_cumulative_constraints: 90199, agent episode reward: [-39.64317956382455], time: 67.369
steps: 11860919, episodes: 288000, mean episode reward: -39.88844193455841, num_cumulative_constraints: 90430, agent episode reward: [-39.88844193455841], time: 54.533
steps: 11902558, episodes: 289000, mean episode reward: -39.55155440353358, num_cumulative_constraints: 90630, agent episode reward: [-39.55155440353358], time: 59.585
steps: 11944202, episodes: 290000, mean episode reward: -40.10089252879181, num_cumulative_constraints: 90904, agent episode reward: [-40.10089252879181], time: 64.85
steps: 11985542, episodes: 291000, mean episode reward: -39.52826530964234, num_cumulative_constraints: 91073, agent episode reward: [-39.52826530964234], time: 56.413
steps: 12026700, episodes: 292000, mean episode reward: -39.554538264634964, num_cumulative_constraints: 91287, agent episode reward: [-39.554538264634964], time: 55.915
steps: 12067591, episodes: 293000, mean episode reward: -39.41298236750338, num_cumulative_constraints: 91488, agent episode reward: [-39.41298236750338], time: 50.899
steps: 12108716, episodes: 294000, mean episode reward: -39.55196293952305, num_cumulative_constraints: 91694, agent episode reward: [-39.55196293952305], time: 60.558
steps: 12149916, episodes: 295000, mean episode reward: -39.43151985684999, num_cumulative_constraints: 91866, agent episode reward: [-39.43151985684999], time: 62.655
steps: 12190770, episodes: 296000, mean episode reward: -39.299535105200064, num_cumulative_constraints: 92039, agent episode reward: [-39.299535105200064], time: 63.53
steps: 12231895, episodes: 297000, mean episode reward: -39.61661623110974, num_cumulative_constraints: 92253, agent episode reward: [-39.61661623110974], time: 59.693
steps: 12272784, episodes: 298000, mean episode reward: -39.379937334074526, num_cumulative_constraints: 92461, agent episode reward: [-39.379937334074526], time: 65.782
steps: 12313786, episodes: 299000, mean episode reward: -39.598722864430854, num_cumulative_constraints: 92658, agent episode reward: [-39.598722864430854], time: 61.478
steps: 12354582, episodes: 300000, mean episode reward: -39.44941495508122, num_cumulative_constraints: 92876, agent episode reward: [-39.44941495508122], time: 61.336
steps: 12395341, episodes: 301000, mean episode reward: -39.44491393129491, num_cumulative_constraints: 93070, agent episode reward: [-39.44491393129491], time: 59.788
steps: 12435938, episodes: 302000, mean episode reward: -39.24248430602035, num_cumulative_constraints: 93243, agent episode reward: [-39.24248430602035], time: 65.755
steps: 12476383, episodes: 303000, mean episode reward: -39.16935228122446, num_cumulative_constraints: 93428, agent episode reward: [-39.16935228122446], time: 57.716
steps: 12516981, episodes: 304000, mean episode reward: -39.20024422834314, num_cumulative_constraints: 93579, agent episode reward: [-39.20024422834314], time: 60.758
steps: 12557604, episodes: 305000, mean episode reward: -39.427137318375756, num_cumulative_constraints: 93800, agent episode reward: [-39.427137318375756], time: 55.026
steps: 12598238, episodes: 306000, mean episode reward: -39.281988354397164, num_cumulative_constraints: 93982, agent episode reward: [-39.281988354397164], time: 60.313
steps: 12638781, episodes: 307000, mean episode reward: -39.0450270020731, num_cumulative_constraints: 94106, agent episode reward: [-39.0450270020731], time: 58.854
steps: 12679360, episodes: 308000, mean episode reward: -39.17136665273891, num_cumulative_constraints: 94303, agent episode reward: [-39.17136665273891], time: 59.839
steps: 12719815, episodes: 309000, mean episode reward: -39.101363487638984, num_cumulative_constraints: 94523, agent episode reward: [-39.101363487638984], time: 54.433
steps: 12760624, episodes: 310000, mean episode reward: -39.371631022433014, num_cumulative_constraints: 94684, agent episode reward: [-39.371631022433014], time: 55.704
steps: 12801270, episodes: 311000, mean episode reward: -39.10934915273175, num_cumulative_constraints: 94836, agent episode reward: [-39.10934915273175], time: 58.349
steps: 12842143, episodes: 312000, mean episode reward: -39.36273687833458, num_cumulative_constraints: 95009, agent episode reward: [-39.36273687833458], time: 64.769
steps: 12882713, episodes: 313000, mean episode reward: -39.103383615608514, num_cumulative_constraints: 95155, agent episode reward: [-39.103383615608514], time: 55.066
steps: 12923291, episodes: 314000, mean episode reward: -39.204932951418684, num_cumulative_constraints: 95332, agent episode reward: [-39.204932951418684], time: 57.011
steps: 12963764, episodes: 315000, mean episode reward: -39.12470383941059, num_cumulative_constraints: 95506, agent episode reward: [-39.12470383941059], time: 56.946
steps: 13004324, episodes: 316000, mean episode reward: -39.22274941105797, num_cumulative_constraints: 95691, agent episode reward: [-39.22274941105797], time: 62.756
steps: 13044897, episodes: 317000, mean episode reward: -39.134528839047505, num_cumulative_constraints: 95852, agent episode reward: [-39.134528839047505], time: 61.771
steps: 13085216, episodes: 318000, mean episode reward: -38.90961988805017, num_cumulative_constraints: 96025, agent episode reward: [-38.90961988805017], time: 67.131
steps: 13125650, episodes: 319000, mean episode reward: -39.2837985142339, num_cumulative_constraints: 96272, agent episode reward: [-39.2837985142339], time: 68.167
steps: 13166025, episodes: 320000, mean episode reward: -38.9730617247635, num_cumulative_constraints: 96450, agent episode reward: [-38.9730617247635], time: 69.056
steps: 13206515, episodes: 321000, mean episode reward: -38.90385238117791, num_cumulative_constraints: 96564, agent episode reward: [-38.90385238117791], time: 53.17
steps: 13247095, episodes: 322000, mean episode reward: -39.33861799873342, num_cumulative_constraints: 96761, agent episode reward: [-39.33861799873342], time: 59.996
steps: 13287578, episodes: 323000, mean episode reward: -39.103551545457115, num_cumulative_constraints: 96932, agent episode reward: [-39.103551545457115], time: 55.545
steps: 13328040, episodes: 324000, mean episode reward: -39.217577738645524, num_cumulative_constraints: 97154, agent episode reward: [-39.217577738645524], time: 57.171
steps: 13368579, episodes: 325000, mean episode reward: -39.23485266190231, num_cumulative_constraints: 97348, agent episode reward: [-39.23485266190231], time: 61.822
steps: 13408987, episodes: 326000, mean episode reward: -39.2358435044272, num_cumulative_constraints: 97585, agent episode reward: [-39.2358435044272], time: 56.262
steps: 13449520, episodes: 327000, mean episode reward: -39.240390773608816, num_cumulative_constraints: 97796, agent episode reward: [-39.240390773608816], time: 60.134
steps: 13490251, episodes: 328000, mean episode reward: -39.52099609774436, num_cumulative_constraints: 98066, agent episode reward: [-39.52099609774436], time: 66.798
steps: 13531022, episodes: 329000, mean episode reward: -39.35502078448378, num_cumulative_constraints: 98268, agent episode reward: [-39.35502078448378], time: 62.659
steps: 13571728, episodes: 330000, mean episode reward: -39.18757320902047, num_cumulative_constraints: 98414, agent episode reward: [-39.18757320902047], time: 58.572
steps: 13612286, episodes: 331000, mean episode reward: -39.07523454872231, num_cumulative_constraints: 98577, agent episode reward: [-39.07523454872231], time: 62.407
steps: 13652944, episodes: 332000, mean episode reward: -39.18320440919878, num_cumulative_constraints: 98762, agent episode reward: [-39.18320440919878], time: 55.571
steps: 13693489, episodes: 333000, mean episode reward: -39.07696854334161, num_cumulative_constraints: 98934, agent episode reward: [-39.07696854334161], time: 69.363
steps: 13734010, episodes: 334000, mean episode reward: -38.94924942820651, num_cumulative_constraints: 99064, agent episode reward: [-38.94924942820651], time: 63.577
steps: 13774630, episodes: 335000, mean episode reward: -38.91533724459085, num_cumulative_constraints: 99194, agent episode reward: [-38.91533724459085], time: 54.946
steps: 13815325, episodes: 336000, mean episode reward: -39.26958638761606, num_cumulative_constraints: 99390, agent episode reward: [-39.26958638761606], time: 57.744
steps: 13856011, episodes: 337000, mean episode reward: -39.3368225458776, num_cumulative_constraints: 99598, agent episode reward: [-39.3368225458776], time: 63.026
steps: 13896782, episodes: 338000, mean episode reward: -39.173756900895896, num_cumulative_constraints: 99750, agent episode reward: [-39.173756900895896], time: 58.7
steps: 13937324, episodes: 339000, mean episode reward: -38.93601063396063, num_cumulative_constraints: 99873, agent episode reward: [-38.93601063396063], time: 53.89
steps: 13978090, episodes: 340000, mean episode reward: -39.29349711259146, num_cumulative_constraints: 100050, agent episode reward: [-39.29349711259146], time: 64.288
steps: 14018700, episodes: 341000, mean episode reward: -39.071772867413415, num_cumulative_constraints: 100231, agent episode reward: [-39.071772867413415], time: 59.585
steps: 14059655, episodes: 342000, mean episode reward: -39.48765189374607, num_cumulative_constraints: 100419, agent episode reward: [-39.48765189374607], time: 68.589
steps: 14100783, episodes: 343000, mean episode reward: -39.54859488605207, num_cumulative_constraints: 100600, agent episode reward: [-39.54859488605207], time: 68.322
steps: 14142294, episodes: 344000, mean episode reward: -39.53287654064863, num_cumulative_constraints: 100791, agent episode reward: [-39.53287654064863], time: 60.98
steps: 14184268, episodes: 345000, mean episode reward: -40.089287797881305, num_cumulative_constraints: 100994, agent episode reward: [-40.089287797881305], time: 60.39
steps: 14226573, episodes: 346000, mean episode reward: -40.234902401790855, num_cumulative_constraints: 101204, agent episode reward: [-40.234902401790855], time: 58.422
steps: 14269796, episodes: 347000, mean episode reward: -40.56587791831156, num_cumulative_constraints: 101390, agent episode reward: [-40.56587791831156], time: 56.05
steps: 14314267, episodes: 348000, mean episode reward: -41.15416445414247, num_cumulative_constraints: 101581, agent episode reward: [-41.15416445414247], time: 66.604
steps: 14358447, episodes: 349000, mean episode reward: -41.29156494964071, num_cumulative_constraints: 101883, agent episode reward: [-41.29156494964071], time: 63.492
steps: 14401778, episodes: 350000, mean episode reward: -40.62576060552831, num_cumulative_constraints: 102133, agent episode reward: [-40.62576060552831], time: 66.64
steps: 14443331, episodes: 351000, mean episode reward: -39.738750530677386, num_cumulative_constraints: 102383, agent episode reward: [-39.738750530677386], time: 63.005
steps: 14484903, episodes: 352000, mean episode reward: -40.08391202410838, num_cumulative_constraints: 102708, agent episode reward: [-40.08391202410838], time: 64.545
steps: 14526057, episodes: 353000, mean episode reward: -39.54833858896066, num_cumulative_constraints: 102906, agent episode reward: [-39.54833858896066], time: 59.48
steps: 14566913, episodes: 354000, mean episode reward: -39.41083180093084, num_cumulative_constraints: 103104, agent episode reward: [-39.41083180093084], time: 59.495
steps: 14607697, episodes: 355000, mean episode reward: -39.24167483004722, num_cumulative_constraints: 103294, agent episode reward: [-39.24167483004722], time: 64.751
steps: 14648789, episodes: 356000, mean episode reward: -39.33666929398978, num_cumulative_constraints: 103420, agent episode reward: [-39.33666929398978], time: 62.891
steps: 14689394, episodes: 357000, mean episode reward: -39.15441476194216, num_cumulative_constraints: 103588, agent episode reward: [-39.15441476194216], time: 65.322
steps: 14730361, episodes: 358000, mean episode reward: -39.308365846183705, num_cumulative_constraints: 103785, agent episode reward: [-39.308365846183705], time: 67.59
steps: 14771175, episodes: 359000, mean episode reward: -39.43908256536573, num_cumulative_constraints: 103999, agent episode reward: [-39.43908256536573], time: 62.39
steps: 14812422, episodes: 360000, mean episode reward: -39.55748681350237, num_cumulative_constraints: 104194, agent episode reward: [-39.55748681350237], time: 71.173
steps: 14853272, episodes: 361000, mean episode reward: -39.40423291089542, num_cumulative_constraints: 104381, agent episode reward: [-39.40423291089542], time: 65.57
steps: 14893889, episodes: 362000, mean episode reward: -39.13096055346007, num_cumulative_constraints: 104539, agent episode reward: [-39.13096055346007], time: 65.681
steps: 14934620, episodes: 363000, mean episode reward: -39.41797500288235, num_cumulative_constraints: 104765, agent episode reward: [-39.41797500288235], time: 57.319
steps: 14975388, episodes: 364000, mean episode reward: -39.30337251987666, num_cumulative_constraints: 104927, agent episode reward: [-39.30337251987666], time: 66.331
steps: 15015878, episodes: 365000, mean episode reward: -38.83919545327933, num_cumulative_constraints: 105055, agent episode reward: [-38.83919545327933], time: 71.431
steps: 15056490, episodes: 366000, mean episode reward: -39.0573768312339, num_cumulative_constraints: 105214, agent episode reward: [-39.0573768312339], time: 59.666
steps: 15097214, episodes: 367000, mean episode reward: -39.270143416436845, num_cumulative_constraints: 105406, agent episode reward: [-39.270143416436845], time: 61.261
steps: 15138154, episodes: 368000, mean episode reward: -39.40851046106716, num_cumulative_constraints: 105570, agent episode reward: [-39.40851046106716], time: 51.86
steps: 15179000, episodes: 369000, mean episode reward: -39.51665417355644, num_cumulative_constraints: 105806, agent episode reward: [-39.51665417355644], time: 66.745
steps: 15219719, episodes: 370000, mean episode reward: -39.17249152450334, num_cumulative_constraints: 105944, agent episode reward: [-39.17249152450334], time: 64.102
steps: 15260545, episodes: 371000, mean episode reward: -39.08391214946653, num_cumulative_constraints: 106037, agent episode reward: [-39.08391214946653], time: 75.527
steps: 15301401, episodes: 372000, mean episode reward: -39.41987745601097, num_cumulative_constraints: 106236, agent episode reward: [-39.41987745601097], time: 79.547
steps: 15342015, episodes: 373000, mean episode reward: -39.07976157281144, num_cumulative_constraints: 106384, agent episode reward: [-39.07976157281144], time: 74.812
steps: 15382772, episodes: 374000, mean episode reward: -39.216760604610656, num_cumulative_constraints: 106560, agent episode reward: [-39.216760604610656], time: 71.775
steps: 15423472, episodes: 375000, mean episode reward: -39.14644633041996, num_cumulative_constraints: 106718, agent episode reward: [-39.14644633041996], time: 75.32
steps: 15464277, episodes: 376000, mean episode reward: -39.32376377955573, num_cumulative_constraints: 106887, agent episode reward: [-39.32376377955573], time: 60.281
steps: 15504989, episodes: 377000, mean episode reward: -39.25910026122608, num_cumulative_constraints: 107068, agent episode reward: [-39.25910026122608], time: 68.063
steps: 15545720, episodes: 378000, mean episode reward: -39.50489913499519, num_cumulative_constraints: 107319, agent episode reward: [-39.50489913499519], time: 58.103
steps: 15586726, episodes: 379000, mean episode reward: -39.67500985491104, num_cumulative_constraints: 107584, agent episode reward: [-39.67500985491104], time: 55.026
steps: 15627821, episodes: 380000, mean episode reward: -39.69673490098462, num_cumulative_constraints: 107843, agent episode reward: [-39.69673490098462], time: 53.64
steps: 15669017, episodes: 381000, mean episode reward: -39.8683921850224, num_cumulative_constraints: 108153, agent episode reward: [-39.8683921850224], time: 58.658
steps: 15710150, episodes: 382000, mean episode reward: -39.60609998376838, num_cumulative_constraints: 108351, agent episode reward: [-39.60609998376838], time: 73.226
steps: 15751460, episodes: 383000, mean episode reward: -39.749682520055735, num_cumulative_constraints: 108569, agent episode reward: [-39.749682520055735], time: 77.82
steps: 15792567, episodes: 384000, mean episode reward: -39.71287747911601, num_cumulative_constraints: 108822, agent episode reward: [-39.71287747911601], time: 68.553
steps: 15833817, episodes: 385000, mean episode reward: -39.46949145314353, num_cumulative_constraints: 108980, agent episode reward: [-39.46949145314353], time: 67.32
steps: 15874955, episodes: 386000, mean episode reward: -39.185931653115695, num_cumulative_constraints: 109122, agent episode reward: [-39.185931653115695], time: 70.578
steps: 15916238, episodes: 387000, mean episode reward: -39.40711051908259, num_cumulative_constraints: 109224, agent episode reward: [-39.40711051908259], time: 63.689
steps: 15957563, episodes: 388000, mean episode reward: -39.64780275811092, num_cumulative_constraints: 109391, agent episode reward: [-39.64780275811092], time: 66.41
steps: 15998862, episodes: 389000, mean episode reward: -39.547096509019994, num_cumulative_constraints: 109535, agent episode reward: [-39.547096509019994], time: 62.41
steps: 16040151, episodes: 390000, mean episode reward: -39.49699134522224, num_cumulative_constraints: 109675, agent episode reward: [-39.49699134522224], time: 62.643
steps: 16081669, episodes: 391000, mean episode reward: -39.59043599596807, num_cumulative_constraints: 109826, agent episode reward: [-39.59043599596807], time: 56.593
steps: 16122922, episodes: 392000, mean episode reward: -39.45052056795326, num_cumulative_constraints: 109987, agent episode reward: [-39.45052056795326], time: 66.442
steps: 16163986, episodes: 393000, mean episode reward: -39.27441268739454, num_cumulative_constraints: 110089, agent episode reward: [-39.27441268739454], time: 60.046
steps: 16204980, episodes: 394000, mean episode reward: -39.2063922389887, num_cumulative_constraints: 110191, agent episode reward: [-39.2063922389887], time: 53.163
steps: 16245967, episodes: 395000, mean episode reward: -39.24674790709368, num_cumulative_constraints: 110315, agent episode reward: [-39.24674790709368], time: 55.886
steps: 16286692, episodes: 396000, mean episode reward: -39.05406538530359, num_cumulative_constraints: 110436, agent episode reward: [-39.05406538530359], time: 65.904
steps: 16327407, episodes: 397000, mean episode reward: -39.35238311624483, num_cumulative_constraints: 110623, agent episode reward: [-39.35238311624483], time: 61.609
steps: 16368081, episodes: 398000, mean episode reward: -39.00554255326465, num_cumulative_constraints: 110748, agent episode reward: [-39.00554255326465], time: 63.86
steps: 16408989, episodes: 399000, mean episode reward: -39.57159645160662, num_cumulative_constraints: 110945, agent episode reward: [-39.57159645160662], time: 63.305
steps: 16449572, episodes: 400000, mean episode reward: -39.07948408864172, num_cumulative_constraints: 111090, agent episode reward: [-39.07948408864172], time: 49.291
steps: 16490034, episodes: 401000, mean episode reward: -38.87908732701572, num_cumulative_constraints: 111228, agent episode reward: [-38.87908732701572], time: 56.234
steps: 16530601, episodes: 402000, mean episode reward: -39.18706528560525, num_cumulative_constraints: 111370, agent episode reward: [-39.18706528560525], time: 56.664
steps: 16571123, episodes: 403000, mean episode reward: -39.02094265454265, num_cumulative_constraints: 111527, agent episode reward: [-39.02094265454265], time: 63.108
steps: 16611667, episodes: 404000, mean episode reward: -39.0612025801596, num_cumulative_constraints: 111664, agent episode reward: [-39.0612025801596], time: 58.512
steps: 16652137, episodes: 405000, mean episode reward: -39.05475177908538, num_cumulative_constraints: 111820, agent episode reward: [-39.05475177908538], time: 64.55
steps: 16692521, episodes: 406000, mean episode reward: -39.11505857252566, num_cumulative_constraints: 112012, agent episode reward: [-39.11505857252566], time: 62.359
steps: 16732907, episodes: 407000, mean episode reward: -38.969522493897095, num_cumulative_constraints: 112151, agent episode reward: [-38.969522493897095], time: 57.475
steps: 16773442, episodes: 408000, mean episode reward: -39.2195367356931, num_cumulative_constraints: 112324, agent episode reward: [-39.2195367356931], time: 66.348
steps: 16813891, episodes: 409000, mean episode reward: -39.11355710788835, num_cumulative_constraints: 112494, agent episode reward: [-39.11355710788835], time: 68.585
steps: 16854214, episodes: 410000, mean episode reward: -38.719678892475294, num_cumulative_constraints: 112604, agent episode reward: [-38.719678892475294], time: 54.92
steps: 16894693, episodes: 411000, mean episode reward: -39.161245420116266, num_cumulative_constraints: 112775, agent episode reward: [-39.161245420116266], time: 54.469
steps: 16935137, episodes: 412000, mean episode reward: -39.39726994151797, num_cumulative_constraints: 113048, agent episode reward: [-39.39726994151797], time: 62.855
steps: 16975576, episodes: 413000, mean episode reward: -39.16025572048457, num_cumulative_constraints: 113217, agent episode reward: [-39.16025572048457], time: 59.825
steps: 17016131, episodes: 414000, mean episode reward: -39.30171217957728, num_cumulative_constraints: 113443, agent episode reward: [-39.30171217957728], time: 67.447
steps: 17056638, episodes: 415000, mean episode reward: -39.026391843655176, num_cumulative_constraints: 113586, agent episode reward: [-39.026391843655176], time: 59.116
steps: 17097192, episodes: 416000, mean episode reward: -39.07615625898355, num_cumulative_constraints: 113740, agent episode reward: [-39.07615625898355], time: 49.634
steps: 17137690, episodes: 417000, mean episode reward: -39.08094620357803, num_cumulative_constraints: 113898, agent episode reward: [-39.08094620357803], time: 54.148
steps: 17178227, episodes: 418000, mean episode reward: -39.190174430224914, num_cumulative_constraints: 114097, agent episode reward: [-39.190174430224914], time: 66.028
steps: 17218826, episodes: 419000, mean episode reward: -39.09155008871107, num_cumulative_constraints: 114281, agent episode reward: [-39.09155008871107], time: 62.13
steps: 17259484, episodes: 420000, mean episode reward: -39.34310232417728, num_cumulative_constraints: 114478, agent episode reward: [-39.34310232417728], time: 54.846
steps: 17299950, episodes: 421000, mean episode reward: -39.01508798348953, num_cumulative_constraints: 114656, agent episode reward: [-39.01508798348953], time: 60.822
steps: 17340624, episodes: 422000, mean episode reward: -39.27023361822511, num_cumulative_constraints: 114836, agent episode reward: [-39.27023361822511], time: 61.727
steps: 17381121, episodes: 423000, mean episode reward: -39.34656095383169, num_cumulative_constraints: 115096, agent episode reward: [-39.34656095383169], time: 65.511
steps: 17421551, episodes: 424000, mean episode reward: -39.131988455043334, num_cumulative_constraints: 115284, agent episode reward: [-39.131988455043334], time: 53.032
steps: 17461994, episodes: 425000, mean episode reward: -39.02882471446823, num_cumulative_constraints: 115464, agent episode reward: [-39.02882471446823], time: 60.879
steps: 17502319, episodes: 426000, mean episode reward: -38.95317913390648, num_cumulative_constraints: 115652, agent episode reward: [-38.95317913390648], time: 61.902
steps: 17542744, episodes: 427000, mean episode reward: -39.12469983411358, num_cumulative_constraints: 115818, agent episode reward: [-39.12469983411358], time: 61.576
steps: 17583241, episodes: 428000, mean episode reward: -39.4594595608935, num_cumulative_constraints: 116095, agent episode reward: [-39.4594595608935], time: 58.011
steps: 17623684, episodes: 429000, mean episode reward: -39.18479775357162, num_cumulative_constraints: 116305, agent episode reward: [-39.18479775357162], time: 55.335
steps: 17664258, episodes: 430000, mean episode reward: -39.14377787804524, num_cumulative_constraints: 116466, agent episode reward: [-39.14377787804524], time: 55.247
steps: 17704715, episodes: 431000, mean episode reward: -39.13285936666493, num_cumulative_constraints: 116640, agent episode reward: [-39.13285936666493], time: 56.065
steps: 17745263, episodes: 432000, mean episode reward: -39.44475401334977, num_cumulative_constraints: 116871, agent episode reward: [-39.44475401334977], time: 52.531
steps: 17785757, episodes: 433000, mean episode reward: -39.21680283707495, num_cumulative_constraints: 117057, agent episode reward: [-39.21680283707495], time: 51.348
steps: 17826111, episodes: 434000, mean episode reward: -39.11985522228601, num_cumulative_constraints: 117262, agent episode reward: [-39.11985522228601], time: 53.643
steps: 17866578, episodes: 435000, mean episode reward: -39.25145062601162, num_cumulative_constraints: 117478, agent episode reward: [-39.25145062601162], time: 61.534
steps: 17907010, episodes: 436000, mean episode reward: -39.112443314554234, num_cumulative_constraints: 117645, agent episode reward: [-39.112443314554234], time: 64.284
steps: 17947585, episodes: 437000, mean episode reward: -39.2126334674704, num_cumulative_constraints: 117801, agent episode reward: [-39.2126334674704], time: 59.623
steps: 17987901, episodes: 438000, mean episode reward: -38.79345587600373, num_cumulative_constraints: 117930, agent episode reward: [-38.79345587600373], time: 60.264
steps: 18028370, episodes: 439000, mean episode reward: -39.08655350303982, num_cumulative_constraints: 118077, agent episode reward: [-39.08655350303982], time: 60.506
steps: 18068835, episodes: 440000, mean episode reward: -38.94358582047677, num_cumulative_constraints: 118186, agent episode reward: [-38.94358582047677], time: 49.116
steps: 18109213, episodes: 441000, mean episode reward: -38.98541397976514, num_cumulative_constraints: 118333, agent episode reward: [-38.98541397976514], time: 52.193
steps: 18149860, episodes: 442000, mean episode reward: -39.42881955622472, num_cumulative_constraints: 118541, agent episode reward: [-39.42881955622472], time: 56.418
steps: 18190373, episodes: 443000, mean episode reward: -39.22865856043519, num_cumulative_constraints: 118752, agent episode reward: [-39.22865856043519], time: 57.659
steps: 18231041, episodes: 444000, mean episode reward: -39.1034873397264, num_cumulative_constraints: 118901, agent episode reward: [-39.1034873397264], time: 58.993
steps: 18271710, episodes: 445000, mean episode reward: -39.10722914375401, num_cumulative_constraints: 119053, agent episode reward: [-39.10722914375401], time: 56.838
steps: 18312537, episodes: 446000, mean episode reward: -39.50233189024389, num_cumulative_constraints: 119264, agent episode reward: [-39.50233189024389], time: 55.544
steps: 18353283, episodes: 447000, mean episode reward: -39.33740415395781, num_cumulative_constraints: 119468, agent episode reward: [-39.33740415395781], time: 58.997
steps: 18394041, episodes: 448000, mean episode reward: -39.35722179807836, num_cumulative_constraints: 119626, agent episode reward: [-39.35722179807836], time: 54.803
steps: 18434649, episodes: 449000, mean episode reward: -39.43512923312803, num_cumulative_constraints: 119853, agent episode reward: [-39.43512923312803], time: 53.075
steps: 18475236, episodes: 450000, mean episode reward: -39.29312866354107, num_cumulative_constraints: 120023, agent episode reward: [-39.29312866354107], time: 60.574
steps: 18515716, episodes: 451000, mean episode reward: -39.2706047971631, num_cumulative_constraints: 120219, agent episode reward: [-39.2706047971631], time: 57.333
steps: 18556247, episodes: 452000, mean episode reward: -39.30681195376908, num_cumulative_constraints: 120406, agent episode reward: [-39.30681195376908], time: 52.318
steps: 18596628, episodes: 453000, mean episode reward: -39.1302768233405, num_cumulative_constraints: 120600, agent episode reward: [-39.1302768233405], time: 55.52
steps: 18637102, episodes: 454000, mean episode reward: -39.3652464058578, num_cumulative_constraints: 120818, agent episode reward: [-39.3652464058578], time: 55.565
steps: 18677511, episodes: 455000, mean episode reward: -39.11734614662912, num_cumulative_constraints: 120963, agent episode reward: [-39.11734614662912], time: 56.55
steps: 18717984, episodes: 456000, mean episode reward: -39.169567863331395, num_cumulative_constraints: 121116, agent episode reward: [-39.169567863331395], time: 59.574
steps: 18758395, episodes: 457000, mean episode reward: -39.1056900084575, num_cumulative_constraints: 121294, agent episode reward: [-39.1056900084575], time: 54.321
steps: 18798897, episodes: 458000, mean episode reward: -39.185174799743784, num_cumulative_constraints: 121483, agent episode reward: [-39.185174799743784], time: 58.215
steps: 18839455, episodes: 459000, mean episode reward: -39.18822370439132, num_cumulative_constraints: 121663, agent episode reward: [-39.18822370439132], time: 63.096
steps: 18880070, episodes: 460000, mean episode reward: -39.45540742788333, num_cumulative_constraints: 121905, agent episode reward: [-39.45540742788333], time: 63.002
steps: 18920514, episodes: 461000, mean episode reward: -39.14671603646659, num_cumulative_constraints: 122090, agent episode reward: [-39.14671603646659], time: 55.95
steps: 18960973, episodes: 462000, mean episode reward: -39.12824090696331, num_cumulative_constraints: 122269, agent episode reward: [-39.12824090696331], time: 50.006
steps: 19001424, episodes: 463000, mean episode reward: -39.03318636895996, num_cumulative_constraints: 122428, agent episode reward: [-39.03318636895996], time: 64.055
steps: 19042075, episodes: 464000, mean episode reward: -39.239046851824064, num_cumulative_constraints: 122595, agent episode reward: [-39.239046851824064], time: 60.291
steps: 19082513, episodes: 465000, mean episode reward: -39.12841887543417, num_cumulative_constraints: 122800, agent episode reward: [-39.12841887543417], time: 61.005
steps: 19123285, episodes: 466000, mean episode reward: -39.256730361048405, num_cumulative_constraints: 122960, agent episode reward: [-39.256730361048405], time: 54.261
steps: 19163761, episodes: 467000, mean episode reward: -38.972168941249514, num_cumulative_constraints: 123107, agent episode reward: [-38.972168941249514], time: 59.088
steps: 19204928, episodes: 468000, mean episode reward: -39.17533330332347, num_cumulative_constraints: 123240, agent episode reward: [-39.17533330332347], time: 59.705
steps: 19246905, episodes: 469000, mean episode reward: -39.76366095695751, num_cumulative_constraints: 123408, agent episode reward: [-39.76366095695751], time: 54.926
steps: 19288657, episodes: 470000, mean episode reward: -39.64074394910732, num_cumulative_constraints: 123578, agent episode reward: [-39.64074394910732], time: 58.276
steps: 19330403, episodes: 471000, mean episode reward: -39.81044216299948, num_cumulative_constraints: 123777, agent episode reward: [-39.81044216299948], time: 56.254
steps: 19372908, episodes: 472000, mean episode reward: -40.18347037856092, num_cumulative_constraints: 123964, agent episode reward: [-40.18347037856092], time: 54.849
steps: 19414944, episodes: 473000, mean episode reward: -39.89038493342545, num_cumulative_constraints: 124133, agent episode reward: [-39.89038493342545], time: 58.149
steps: 19456519, episodes: 474000, mean episode reward: -39.73364558855162, num_cumulative_constraints: 124326, agent episode reward: [-39.73364558855162], time: 66.066
steps: 19497512, episodes: 475000, mean episode reward: -39.46309404807589, num_cumulative_constraints: 124542, agent episode reward: [-39.46309404807589], time: 57.215
steps: 19538463, episodes: 476000, mean episode reward: -39.365617699878406, num_cumulative_constraints: 124718, agent episode reward: [-39.365617699878406], time: 54.401
steps: 19579394, episodes: 477000, mean episode reward: -39.47776315781509, num_cumulative_constraints: 124930, agent episode reward: [-39.47776315781509], time: 57.673
steps: 19620309, episodes: 478000, mean episode reward: -39.348352577277296, num_cumulative_constraints: 125113, agent episode reward: [-39.348352577277296], time: 52.921
steps: 19661137, episodes: 479000, mean episode reward: -39.52336127428611, num_cumulative_constraints: 125348, agent episode reward: [-39.52336127428611], time: 66.918
steps: 19702007, episodes: 480000, mean episode reward: -39.51527648861333, num_cumulative_constraints: 125561, agent episode reward: [-39.51527648861333], time: 60.49
steps: 19742708, episodes: 481000, mean episode reward: -39.40151547813538, num_cumulative_constraints: 125803, agent episode reward: [-39.40151547813538], time: 55.588
steps: 19783630, episodes: 482000, mean episode reward: -39.62165776436629, num_cumulative_constraints: 126018, agent episode reward: [-39.62165776436629], time: 51.322
steps: 19824301, episodes: 483000, mean episode reward: -39.236646848820094, num_cumulative_constraints: 126199, agent episode reward: [-39.236646848820094], time: 53.537
steps: 19864945, episodes: 484000, mean episode reward: -39.21079828097151, num_cumulative_constraints: 126357, agent episode reward: [-39.21079828097151], time: 56.389
steps: 19905441, episodes: 485000, mean episode reward: -39.0909176609023, num_cumulative_constraints: 126549, agent episode reward: [-39.0909176609023], time: 62.439
steps: 19945954, episodes: 486000, mean episode reward: -39.199725485080876, num_cumulative_constraints: 126761, agent episode reward: [-39.199725485080876], time: 52.935
steps: 19986464, episodes: 487000, mean episode reward: -39.125995435925326, num_cumulative_constraints: 126928, agent episode reward: [-39.125995435925326], time: 57.086
steps: 20026953, episodes: 488000, mean episode reward: -38.963702369943846, num_cumulative_constraints: 127077, agent episode reward: [-38.963702369943846], time: 63.136
steps: 20067702, episodes: 489000, mean episode reward: -39.496927825647646, num_cumulative_constraints: 127314, agent episode reward: [-39.496927825647646], time: 67.568
steps: 20108516, episodes: 490000, mean episode reward: -39.37533162488544, num_cumulative_constraints: 127512, agent episode reward: [-39.37533162488544], time: 63.997
steps: 20149193, episodes: 491000, mean episode reward: -39.1361952264485, num_cumulative_constraints: 127652, agent episode reward: [-39.1361952264485], time: 59.909
steps: 20189767, episodes: 492000, mean episode reward: -39.19769439006038, num_cumulative_constraints: 127840, agent episode reward: [-39.19769439006038], time: 50.709
steps: 20230553, episodes: 493000, mean episode reward: -39.38245405630393, num_cumulative_constraints: 128015, agent episode reward: [-39.38245405630393], time: 55.609
steps: 20271556, episodes: 494000, mean episode reward: -39.691340143275696, num_cumulative_constraints: 128266, agent episode reward: [-39.691340143275696], time: 62.357
steps: 20312499, episodes: 495000, mean episode reward: -39.3766003099107, num_cumulative_constraints: 128422, agent episode reward: [-39.3766003099107], time: 55.283
steps: 20353526, episodes: 496000, mean episode reward: -39.554510076170644, num_cumulative_constraints: 128607, agent episode reward: [-39.554510076170644], time: 60.956
steps: 20394663, episodes: 497000, mean episode reward: -39.597659631187206, num_cumulative_constraints: 128782, agent episode reward: [-39.597659631187206], time: 64.436
steps: 20435664, episodes: 498000, mean episode reward: -39.52599244644641, num_cumulative_constraints: 128985, agent episode reward: [-39.52599244644641], time: 54.846
steps: 20476725, episodes: 499000, mean episode reward: -39.562763944076515, num_cumulative_constraints: 129166, agent episode reward: [-39.562763944076515], time: 60.091
steps: 20517690, episodes: 500000, mean episode reward: -39.567691782157915, num_cumulative_constraints: 129387, agent episode reward: [-39.567691782157915], time: 68.865
steps: 20558755, episodes: 501000, mean episode reward: -39.827507763343576, num_cumulative_constraints: 129653, agent episode reward: [-39.827507763343576], time: 64.844
steps: 20599437, episodes: 502000, mean episode reward: -39.16808519248279, num_cumulative_constraints: 129840, agent episode reward: [-39.16808519248279], time: 63.489
steps: 20640158, episodes: 503000, mean episode reward: -39.32999076269058, num_cumulative_constraints: 130030, agent episode reward: [-39.32999076269058], time: 59.013
steps: 20680999, episodes: 504000, mean episode reward: -39.6533205151033, num_cumulative_constraints: 130276, agent episode reward: [-39.6533205151033], time: 59.634
steps: 20721596, episodes: 505000, mean episode reward: -39.22393593773161, num_cumulative_constraints: 130457, agent episode reward: [-39.22393593773161], time: 61.446
steps: 20762252, episodes: 506000, mean episode reward: -39.31402461993066, num_cumulative_constraints: 130634, agent episode reward: [-39.31402461993066], time: 55.719
steps: 20803120, episodes: 507000, mean episode reward: -39.56754701592747, num_cumulative_constraints: 130837, agent episode reward: [-39.56754701592747], time: 55.417
steps: 20843813, episodes: 508000, mean episode reward: -39.28058066105729, num_cumulative_constraints: 131021, agent episode reward: [-39.28058066105729], time: 67.475
steps: 20884574, episodes: 509000, mean episode reward: -39.42592966442108, num_cumulative_constraints: 131230, agent episode reward: [-39.42592966442108], time: 66.446
steps: 20925645, episodes: 510000, mean episode reward: -39.61556912217527, num_cumulative_constraints: 131447, agent episode reward: [-39.61556912217527], time: 58.461
steps: 20966536, episodes: 511000, mean episode reward: -39.31908497684373, num_cumulative_constraints: 131593, agent episode reward: [-39.31908497684373], time: 58.846
steps: 21007866, episodes: 512000, mean episode reward: -39.78613746659948, num_cumulative_constraints: 131792, agent episode reward: [-39.78613746659948], time: 64.704
steps: 21048768, episodes: 513000, mean episode reward: -39.52739972296379, num_cumulative_constraints: 132002, agent episode reward: [-39.52739972296379], time: 61.906
steps: 21089493, episodes: 514000, mean episode reward: -39.42105964396287, num_cumulative_constraints: 132204, agent episode reward: [-39.42105964396287], time: 69.966
steps: 21130229, episodes: 515000, mean episode reward: -39.349882223120005, num_cumulative_constraints: 132395, agent episode reward: [-39.349882223120005], time: 64.975
steps: 21170870, episodes: 516000, mean episode reward: -39.434593684621625, num_cumulative_constraints: 132628, agent episode reward: [-39.434593684621625], time: 62.36
steps: 21211782, episodes: 517000, mean episode reward: -39.43740900044336, num_cumulative_constraints: 132783, agent episode reward: [-39.43740900044336], time: 59.572
steps: 21252386, episodes: 518000, mean episode reward: -39.31524782522732, num_cumulative_constraints: 132996, agent episode reward: [-39.31524782522732], time: 55.305
steps: 21293098, episodes: 519000, mean episode reward: -39.36541990340571, num_cumulative_constraints: 133190, agent episode reward: [-39.36541990340571], time: 59.22
steps: 21333844, episodes: 520000, mean episode reward: -39.34950251113005, num_cumulative_constraints: 133391, agent episode reward: [-39.34950251113005], time: 53.426
steps: 21374476, episodes: 521000, mean episode reward: -39.25683894287454, num_cumulative_constraints: 133574, agent episode reward: [-39.25683894287454], time: 51.009
steps: 21415244, episodes: 522000, mean episode reward: -39.23934143231176, num_cumulative_constraints: 133732, agent episode reward: [-39.23934143231176], time: 58.928
steps: 21456109, episodes: 523000, mean episode reward: -39.39971627464907, num_cumulative_constraints: 133864, agent episode reward: [-39.39971627464907], time: 69.584
steps: 21496953, episodes: 524000, mean episode reward: -39.43624689510406, num_cumulative_constraints: 134067, agent episode reward: [-39.43624689510406], time: 67.691
steps: 21537699, episodes: 525000, mean episode reward: -39.49832194801338, num_cumulative_constraints: 134313, agent episode reward: [-39.49832194801338], time: 63.191
steps: 21578387, episodes: 526000, mean episode reward: -39.414137554854904, num_cumulative_constraints: 134532, agent episode reward: [-39.414137554854904], time: 63.235
steps: 21618974, episodes: 527000, mean episode reward: -39.379944882687276, num_cumulative_constraints: 134807, agent episode reward: [-39.379944882687276], time: 61.402
steps: 21659588, episodes: 528000, mean episode reward: -39.536062734966606, num_cumulative_constraints: 135099, agent episode reward: [-39.536062734966606], time: 53.014
steps: 21700300, episodes: 529000, mean episode reward: -40.20433868982655, num_cumulative_constraints: 135590, agent episode reward: [-40.20433868982655], time: 56.14
steps: 21740972, episodes: 530000, mean episode reward: -40.18865092056747, num_cumulative_constraints: 136083, agent episode reward: [-40.18865092056747], time: 59.272
steps: 21781556, episodes: 531000, mean episode reward: -40.46598300438644, num_cumulative_constraints: 136714, agent episode reward: [-40.46598300438644], time: 58.32
steps: 21822188, episodes: 532000, mean episode reward: -40.757405131008255, num_cumulative_constraints: 137417, agent episode reward: [-40.757405131008255], time: 59.231
steps: 21862917, episodes: 533000, mean episode reward: -40.72986231783891, num_cumulative_constraints: 138082, agent episode reward: [-40.72986231783891], time: 56.667
steps: 21903486, episodes: 534000, mean episode reward: -41.174012584411464, num_cumulative_constraints: 138912, agent episode reward: [-41.174012584411464], time: 59.0
steps: 21944075, episodes: 535000, mean episode reward: -41.12698631683139, num_cumulative_constraints: 139729, agent episode reward: [-41.12698631683139], time: 56.261
steps: 21984692, episodes: 536000, mean episode reward: -40.958554348537845, num_cumulative_constraints: 140503, agent episode reward: [-40.958554348537845], time: 61.026
steps: 22025267, episodes: 537000, mean episode reward: -40.54765832673281, num_cumulative_constraints: 141156, agent episode reward: [-40.54765832673281], time: 60.188
steps: 22065881, episodes: 538000, mean episode reward: -39.96385967295596, num_cumulative_constraints: 141592, agent episode reward: [-39.96385967295596], time: 61.269
steps: 22106526, episodes: 539000, mean episode reward: -39.495480115661216, num_cumulative_constraints: 141875, agent episode reward: [-39.495480115661216], time: 57.544
steps: 22147171, episodes: 540000, mean episode reward: -39.39085072183191, num_cumulative_constraints: 142117, agent episode reward: [-39.39085072183191], time: 55.886
steps: 22187786, episodes: 541000, mean episode reward: -39.36730589273568, num_cumulative_constraints: 142365, agent episode reward: [-39.36730589273568], time: 53.845
steps: 22228362, episodes: 542000, mean episode reward: -39.416145009805625, num_cumulative_constraints: 142630, agent episode reward: [-39.416145009805625], time: 54.209
steps: 22269076, episodes: 543000, mean episode reward: -39.42021048576942, num_cumulative_constraints: 142855, agent episode reward: [-39.42021048576942], time: 64.137
steps: 22309891, episodes: 544000, mean episode reward: -39.60786321190762, num_cumulative_constraints: 143089, agent episode reward: [-39.60786321190762], time: 60.848
steps: 22350610, episodes: 545000, mean episode reward: -39.353215796001344, num_cumulative_constraints: 143305, agent episode reward: [-39.353215796001344], time: 57.767
steps: 22391436, episodes: 546000, mean episode reward: -39.39711852872942, num_cumulative_constraints: 143485, agent episode reward: [-39.39711852872942], time: 57.091
steps: 22432272, episodes: 547000, mean episode reward: -39.30764667098296, num_cumulative_constraints: 143625, agent episode reward: [-39.30764667098296], time: 57.678
steps: 22473059, episodes: 548000, mean episode reward: -39.35594026741304, num_cumulative_constraints: 143831, agent episode reward: [-39.35594026741304], time: 61.275
steps: 22513796, episodes: 549000, mean episode reward: -39.309928108727156, num_cumulative_constraints: 144007, agent episode reward: [-39.309928108727156], time: 60.602
steps: 22554489, episodes: 550000, mean episode reward: -39.28176856887707, num_cumulative_constraints: 144188, agent episode reward: [-39.28176856887707], time: 59.611
steps: 22595232, episodes: 551000, mean episode reward: -39.24843248565267, num_cumulative_constraints: 144352, agent episode reward: [-39.24843248565267], time: 56.037
steps: 22636042, episodes: 552000, mean episode reward: -39.22264899186733, num_cumulative_constraints: 144493, agent episode reward: [-39.22264899186733], time: 67.155
steps: 22676761, episodes: 553000, mean episode reward: -39.262271541800146, num_cumulative_constraints: 144672, agent episode reward: [-39.262271541800146], time: 61.352
steps: 22717562, episodes: 554000, mean episode reward: -39.29629460439176, num_cumulative_constraints: 144835, agent episode reward: [-39.29629460439176], time: 51.41
steps: 22758413, episodes: 555000, mean episode reward: -39.64779883980362, num_cumulative_constraints: 145067, agent episode reward: [-39.64779883980362], time: 52.146
steps: 22799415, episodes: 556000, mean episode reward: -39.66603862429357, num_cumulative_constraints: 145279, agent episode reward: [-39.66603862429357], time: 53.583
steps: 22840216, episodes: 557000, mean episode reward: -39.46612340644489, num_cumulative_constraints: 145480, agent episode reward: [-39.46612340644489], time: 53.574
steps: 22880984, episodes: 558000, mean episode reward: -39.278066765064835, num_cumulative_constraints: 145638, agent episode reward: [-39.278066765064835], time: 55.024
steps: 22921926, episodes: 559000, mean episode reward: -39.686130995828485, num_cumulative_constraints: 145841, agent episode reward: [-39.686130995828485], time: 48.539
steps: 22962689, episodes: 560000, mean episode reward: -39.513058190831764, num_cumulative_constraints: 146080, agent episode reward: [-39.513058190831764], time: 55.93
steps: 23003290, episodes: 561000, mean episode reward: -39.330067539582075, num_cumulative_constraints: 146306, agent episode reward: [-39.330067539582075], time: 52.309
steps: 23044052, episodes: 562000, mean episode reward: -39.39533641856613, num_cumulative_constraints: 146500, agent episode reward: [-39.39533641856613], time: 50.547
steps: 23084962, episodes: 563000, mean episode reward: -39.659276887234974, num_cumulative_constraints: 146717, agent episode reward: [-39.659276887234974], time: 55.316
steps: 23125853, episodes: 564000, mean episode reward: -39.53730233792498, num_cumulative_constraints: 146916, agent episode reward: [-39.53730233792498], time: 66.572
steps: 23166724, episodes: 565000, mean episode reward: -39.59751887608278, num_cumulative_constraints: 147146, agent episode reward: [-39.59751887608278], time: 72.518
steps: 23207711, episodes: 566000, mean episode reward: -39.60037693021047, num_cumulative_constraints: 147355, agent episode reward: [-39.60037693021047], time: 69.377
steps: 23248540, episodes: 567000, mean episode reward: -39.448047453814944, num_cumulative_constraints: 147556, agent episode reward: [-39.448047453814944], time: 69.558
steps: 23289633, episodes: 568000, mean episode reward: -39.86712858725918, num_cumulative_constraints: 147817, agent episode reward: [-39.86712858725918], time: 63.337
steps: 23330443, episodes: 569000, mean episode reward: -39.502383211301975, num_cumulative_constraints: 148024, agent episode reward: [-39.502383211301975], time: 65.98
steps: 23371264, episodes: 570000, mean episode reward: -39.72310963245308, num_cumulative_constraints: 148311, agent episode reward: [-39.72310963245308], time: 60.15
steps: 23412163, episodes: 571000, mean episode reward: -39.40079250395425, num_cumulative_constraints: 148485, agent episode reward: [-39.40079250395425], time: 66.535
steps: 23453193, episodes: 572000, mean episode reward: -39.76773205738141, num_cumulative_constraints: 148771, agent episode reward: [-39.76773205738141], time: 60.972
steps: 23494210, episodes: 573000, mean episode reward: -39.607459931060234, num_cumulative_constraints: 148966, agent episode reward: [-39.607459931060234], time: 62.892
steps: 23535280, episodes: 574000, mean episode reward: -39.583556418426866, num_cumulative_constraints: 149170, agent episode reward: [-39.583556418426866], time: 63.213
steps: 23576329, episodes: 575000, mean episode reward: -39.632710878605856, num_cumulative_constraints: 149379, agent episode reward: [-39.632710878605856], time: 56.79
steps: 23617567, episodes: 576000, mean episode reward: -39.82514036374345, num_cumulative_constraints: 149582, agent episode reward: [-39.82514036374345], time: 58.484
steps: 23658510, episodes: 577000, mean episode reward: -39.52399461001679, num_cumulative_constraints: 149758, agent episode reward: [-39.52399461001679], time: 58.089
steps: 23699408, episodes: 578000, mean episode reward: -39.430332080328796, num_cumulative_constraints: 149974, agent episode reward: [-39.430332080328796], time: 63.101
steps: 23740799, episodes: 579000, mean episode reward: -39.847121562917906, num_cumulative_constraints: 150173, agent episode reward: [-39.847121562917906], time: 59.589
steps: 23782263, episodes: 580000, mean episode reward: -39.75069200092047, num_cumulative_constraints: 150365, agent episode reward: [-39.75069200092047], time: 55.991
steps: 23823404, episodes: 581000, mean episode reward: -39.58236785912375, num_cumulative_constraints: 150526, agent episode reward: [-39.58236785912375], time: 58.367
steps: 23864743, episodes: 582000, mean episode reward: -39.89058255795579, num_cumulative_constraints: 150713, agent episode reward: [-39.89058255795579], time: 52.998
steps: 23905892, episodes: 583000, mean episode reward: -39.54030741354075, num_cumulative_constraints: 150891, agent episode reward: [-39.54030741354075], time: 55.027
steps: 23947032, episodes: 584000, mean episode reward: -39.6502771226817, num_cumulative_constraints: 151067, agent episode reward: [-39.6502771226817], time: 52.173
steps: 23988023, episodes: 585000, mean episode reward: -39.57208907024571, num_cumulative_constraints: 151269, agent episode reward: [-39.57208907024571], time: 60.229
steps: 24029109, episodes: 586000, mean episode reward: -39.41298394301067, num_cumulative_constraints: 151410, agent episode reward: [-39.41298394301067], time: 59.537
steps: 24070172, episodes: 587000, mean episode reward: -39.536839234708616, num_cumulative_constraints: 151574, agent episode reward: [-39.536839234708616], time: 54.863
steps: 24111139, episodes: 588000, mean episode reward: -39.38773409570429, num_cumulative_constraints: 151713, agent episode reward: [-39.38773409570429], time: 61.429
steps: 24152232, episodes: 589000, mean episode reward: -39.6111573124568, num_cumulative_constraints: 151909, agent episode reward: [-39.6111573124568], time: 66.912
steps: 24193290, episodes: 590000, mean episode reward: -39.511662484965676, num_cumulative_constraints: 152068, agent episode reward: [-39.511662484965676], time: 65.194
steps: 24234383, episodes: 591000, mean episode reward: -39.66829865392769, num_cumulative_constraints: 152264, agent episode reward: [-39.66829865392769], time: 70.426
steps: 24275314, episodes: 592000, mean episode reward: -39.35070873539198, num_cumulative_constraints: 152419, agent episode reward: [-39.35070873539198], time: 62.417
steps: 24316441, episodes: 593000, mean episode reward: -39.440248009915436, num_cumulative_constraints: 152595, agent episode reward: [-39.440248009915436], time: 60.493
steps: 24357437, episodes: 594000, mean episode reward: -39.63623261718856, num_cumulative_constraints: 152818, agent episode reward: [-39.63623261718856], time: 52.092
steps: 24398396, episodes: 595000, mean episode reward: -39.48150324755543, num_cumulative_constraints: 152991, agent episode reward: [-39.48150324755543], time: 58.658
steps: 24439236, episodes: 596000, mean episode reward: -39.36845593384044, num_cumulative_constraints: 153171, agent episode reward: [-39.36845593384044], time: 55.421
steps: 24480174, episodes: 597000, mean episode reward: -39.480619485766326, num_cumulative_constraints: 153337, agent episode reward: [-39.480619485766326], time: 53.102
steps: 24521179, episodes: 598000, mean episode reward: -39.59152352510615, num_cumulative_constraints: 153526, agent episode reward: [-39.59152352510615], time: 58.805
steps: 24562439, episodes: 599000, mean episode reward: -39.72535300781076, num_cumulative_constraints: 153715, agent episode reward: [-39.72535300781076], time: 62.291
steps: 24603558, episodes: 600000, mean episode reward: -39.686879711714894, num_cumulative_constraints: 153933, agent episode reward: [-39.686879711714894], time: 57.915


safe_layer
'
                if xx[0] > 1.0:
                    xx[0] = 1.0
                if xx[0] < -1.0:
                    xx[0] = -1.0

                if xx == action_omega:
                    return action

                action[3] = + xx[0]/2
                action[4] = - xx[0]/2
' with 0.05 gap
steps: 41366, episodes: 1000, mean episode reward: -39.52227936323721, num_cumulative_constraints: 58, agent episode reward: [-39.52227936323721], time: 84.642
steps: 84247, episodes: 2000, mean episode reward: -44.50774908243718, num_cumulative_constraints: 1303, agent episode reward: [-44.50774908243718], time: 106.115
steps: 127067, episodes: 3000, mean episode reward: -51.74671902678731, num_cumulative_constraints: 5004, agent episode reward: [-51.74671902678731], time: 124.302


with safety_layer
                if xx[0] > 2.0:
                    xx[0] = 2.0
                if xx[0] < -2.0:
                    xx[0] = -2.0

                if xx == action_omega:
                    return action

                delta_action = xx[0]/2 - action_omega
                action[3] = action[3] + delta_action/2
                action[4] = action[4] - delta_action/2


Starting iterations...
steps: 41048, episodes: 1000, mean episode reward: -39.24725583844495, num_cumulative_constraints: 101, agent episode reward: [-39.24725583844495], time: 95.307
steps: 82221, episodes: 2000, mean episode reward: -39.60002562139826, num_cumulative_constraints: 237, agent episode reward: [-39.60002562139826], time: 101.398
steps: 123333, episodes: 3000, mean episode reward: -39.44373632325706, num_cumulative_constraints: 312, agent episode reward: [-39.44373632325706], time: 86.969
steps: 164574, episodes: 4000, mean episode reward: -39.6265026597871, num_cumulative_constraints: 416, agent episode reward: [-39.6265026597871], time: 86.731
steps: 206376, episodes: 5000, mean episode reward: -40.01651775458276, num_cumulative_constraints: 549, agent episode reward: [-40.01651775458276], time: 92.175
steps: 247820, episodes: 6000, mean episode reward: -39.55692505502718, num_cumulative_constraints: 611, agent episode reward: [-39.55692505502718], time: 111.318
steps: 289170, episodes: 7000, mean episode reward: -39.69158465575525, num_cumulative_constraints: 740, agent episode reward: [-39.69158465575525], time: 96.252
steps: 330505, episodes: 8000, mean episode reward: -39.750610617362014, num_cumulative_constraints: 895, agent episode reward: [-39.750610617362014], time: 96.947
steps: 371880, episodes: 9000, mean episode reward: -39.82566833676209, num_cumulative_constraints: 996, agent episode reward: [-39.82566833676209], time: 94.751
steps: 413264, episodes: 10000, mean episode reward: -39.792204693354186, num_cumulative_constraints: 1076, agent episode reward: [-39.792204693354186], time: 92.913
steps: 454679, episodes: 11000, mean episode reward: -39.76971270325485, num_cumulative_constraints: 1156, agent episode reward: [-39.76971270325485], time: 90.067
steps: 496188, episodes: 12000, mean episode reward: -39.63611309320409, num_cumulative_constraints: 1232, agent episode reward: [-39.63611309320409], time: 92.733
steps: 538235, episodes: 13000, mean episode reward: -40.09202255283883, num_cumulative_constraints: 1369, agent episode reward: [-40.09202255283883], time: 98.681
steps: 580131, episodes: 14000, mean episode reward: -40.097989732614366, num_cumulative_constraints: 1515, agent episode reward: [-40.097989732614366], time: 108.143
steps: 621709, episodes: 15000, mean episode reward: -39.775297602597796, num_cumulative_constraints: 1598, agent episode reward: [-39.775297602597796], time: 93.084
steps: 663152, episodes: 16000, mean episode reward: -39.7119357526903, num_cumulative_constraints: 1684, agent episode reward: [-39.7119357526903], time: 100.839
steps: 704533, episodes: 17000, mean episode reward: -39.724952614725275, num_cumulative_constraints: 1762, agent episode reward: [-39.724952614725275], time: 97.405
steps: 745777, episodes: 18000, mean episode reward: -39.6159286243277, num_cumulative_constraints: 1855, agent episode reward: [-39.6159286243277], time: 91.509
steps: 787116, episodes: 19000, mean episode reward: -39.6581834032855, num_cumulative_constraints: 1935, agent episode reward: [-39.6581834032855], time: 97.771
steps: 828361, episodes: 20000, mean episode reward: -39.73553615713047, num_cumulative_constraints: 2041, agent episode reward: [-39.73553615713047], time: 99.786
steps: 869521, episodes: 21000, mean episode reward: -39.564826795626196, num_cumulative_constraints: 2135, agent episode reward: [-39.564826795626196], time: 97.655
steps: 910711, episodes: 22000, mean episode reward: -39.61076063002154, num_cumulative_constraints: 2211, agent episode reward: [-39.61076063002154], time: 93.515
steps: 951899, episodes: 23000, mean episode reward: -39.602207796292994, num_cumulative_constraints: 2309, agent episode reward: [-39.602207796292994], time: 94.422
steps: 993306, episodes: 24000, mean episode reward: -39.81562804096812, num_cumulative_constraints: 2429, agent episode reward: [-39.81562804096812], time: 91.799
steps: 1034600, episodes: 25000, mean episode reward: -39.77964913738961, num_cumulative_constraints: 2555, agent episode reward: [-39.77964913738961], time: 99.136
steps: 1076110, episodes: 26000, mean episode reward: -39.90031982049202, num_cumulative_constraints: 2661, agent episode reward: [-39.90031982049202], time: 91.536
steps: 1117689, episodes: 27000, mean episode reward: -40.02080629817808, num_cumulative_constraints: 2783, agent episode reward: [-40.02080629817808], time: 94.998
steps: 1159276, episodes: 28000, mean episode reward: -39.91583525418181, num_cumulative_constraints: 2906, agent episode reward: [-39.91583525418181], time: 99.92
steps: 1200813, episodes: 29000, mean episode reward: -40.134175643096285, num_cumulative_constraints: 3040, agent episode reward: [-40.134175643096285], time: 94.052
steps: 1242395, episodes: 30000, mean episode reward: -39.954537489308784, num_cumulative_constraints: 3156, agent episode reward: [-39.954537489308784], time: 89.669
steps: 1283943, episodes: 31000, mean episode reward: -39.941412223009, num_cumulative_constraints: 3254, agent episode reward: [-39.941412223009], time: 106.799
steps: 1325656, episodes: 32000, mean episode reward: -40.29708983838385, num_cumulative_constraints: 3363, agent episode reward: [-40.29708983838385], time: 122.504
steps: 1367287, episodes: 33000, mean episode reward: -40.10057326343624, num_cumulative_constraints: 3453, agent episode reward: [-40.10057326343624], time: 93.608
steps: 1408768, episodes: 34000, mean episode reward: -39.921048355800934, num_cumulative_constraints: 3575, agent episode reward: [-39.921048355800934], time: 91.231
steps: 1450465, episodes: 35000, mean episode reward: -40.344491388260614, num_cumulative_constraints: 3761, agent episode reward: [-40.344491388260614], time: 100.632
steps: 1492210, episodes: 36000, mean episode reward: -40.27992160574793, num_cumulative_constraints: 3922, agent episode reward: [-40.27992160574793], time: 96.934
steps: 1533801, episodes: 37000, mean episode reward: -40.04013479517891, num_cumulative_constraints: 4014, agent episode reward: [-40.04013479517891], time: 106.468
steps: 1575564, episodes: 38000, mean episode reward: -40.127027598808624, num_cumulative_constraints: 4161, agent episode reward: [-40.127027598808624], time: 96.729
steps: 1617232, episodes: 39000, mean episode reward: -39.88024005001363, num_cumulative_constraints: 4276, agent episode reward: [-39.88024005001363], time: 97.502
steps: 1658931, episodes: 40000, mean episode reward: -39.96984975099015, num_cumulative_constraints: 4390, agent episode reward: [-39.96984975099015], time: 94.833
steps: 1700347, episodes: 41000, mean episode reward: -39.8827539318138, num_cumulative_constraints: 4517, agent episode reward: [-39.8827539318138], time: 97.897
steps: 1741884, episodes: 42000, mean episode reward: -39.99634071322931, num_cumulative_constraints: 4654, agent episode reward: [-39.99634071322931], time: 146.0
steps: 1783127, episodes: 43000, mean episode reward: -39.53942170622186, num_cumulative_constraints: 4732, agent episode reward: [-39.53942170622186], time: 102.835
steps: 1824619, episodes: 44000, mean episode reward: -39.74922909734882, num_cumulative_constraints: 4831, agent episode reward: [-39.74922909734882], time: 93.721
steps: 1866283, episodes: 45000, mean episode reward: -39.71062301367183, num_cumulative_constraints: 4907, agent episode reward: [-39.71062301367183], time: 98.792
steps: 1907996, episodes: 46000, mean episode reward: -39.82797110847231, num_cumulative_constraints: 5041, agent episode reward: [-39.82797110847231], time: 91.133
steps: 1949591, episodes: 47000, mean episode reward: -39.91259205885068, num_cumulative_constraints: 5168, agent episode reward: [-39.91259205885068], time: 94.032
steps: 1990775, episodes: 48000, mean episode reward: -39.70440802456741, num_cumulative_constraints: 5286, agent episode reward: [-39.70440802456741], time: 105.085
steps: 2031774, episodes: 49000, mean episode reward: -39.54932902490966, num_cumulative_constraints: 5409, agent episode reward: [-39.54932902490966], time: 90.995
steps: 2072870, episodes: 50000, mean episode reward: -39.54152750865677, num_cumulative_constraints: 5479, agent episode reward: [-39.54152750865677], time: 180.086
steps: 2113861, episodes: 51000, mean episode reward: -39.55254369195495, num_cumulative_constraints: 5605, agent episode reward: [-39.55254369195495], time: 83.662
steps: 2155029, episodes: 52000, mean episode reward: -39.652253131277085, num_cumulative_constraints: 5698, agent episode reward: [-39.652253131277085], time: 104.503
steps: 2196389, episodes: 53000, mean episode reward: -39.79821606818613, num_cumulative_constraints: 5799, agent episode reward: [-39.79821606818613], time: 86.819
steps: 2237663, episodes: 54000, mean episode reward: -39.8389041517458, num_cumulative_constraints: 5909, agent episode reward: [-39.8389041517458], time: 90.591
steps: 2278964, episodes: 55000, mean episode reward: -39.875639305182716, num_cumulative_constraints: 6007, agent episode reward: [-39.875639305182716], time: 87.699
steps: 2320021, episodes: 56000, mean episode reward: -39.567463358078875, num_cumulative_constraints: 6110, agent episode reward: [-39.567463358078875], time: 90.203
steps: 2361131, episodes: 57000, mean episode reward: -39.69156879351403, num_cumulative_constraints: 6220, agent episode reward: [-39.69156879351403], time: 156.61
steps: 2402366, episodes: 58000, mean episode reward: -39.73694043310807, num_cumulative_constraints: 6295, agent episode reward: [-39.73694043310807], time: 88.58
steps: 2443787, episodes: 59000, mean episode reward: -39.968034064340564, num_cumulative_constraints: 6431, agent episode reward: [-39.968034064340564], time: 89.301
steps: 2484967, episodes: 60000, mean episode reward: -39.705658080755114, num_cumulative_constraints: 6542, agent episode reward: [-39.705658080755114], time: 84.033
steps: 2526540, episodes: 61000, mean episode reward: -39.86784622696483, num_cumulative_constraints: 6639, agent episode reward: [-39.86784622696483], time: 97.705
steps: 2568099, episodes: 62000, mean episode reward: -40.01925767230024, num_cumulative_constraints: 6778, agent episode reward: [-40.01925767230024], time: 90.726
steps: 2609865, episodes: 63000, mean episode reward: -39.96920172536406, num_cumulative_constraints: 6915, agent episode reward: [-39.96920172536406], time: 95.084
steps: 2651731, episodes: 64000, mean episode reward: -40.13522183280877, num_cumulative_constraints: 7012, agent episode reward: [-40.13522183280877], time: 90.396
steps: 2693018, episodes: 65000, mean episode reward: -39.80967158772648, num_cumulative_constraints: 7125, agent episode reward: [-39.80967158772648], time: 97.325
steps: 2734394, episodes: 66000, mean episode reward: -39.870471561710424, num_cumulative_constraints: 7236, agent episode reward: [-39.870471561710424], time: 92.464
steps: 2775713, episodes: 67000, mean episode reward: -39.76001382408981, num_cumulative_constraints: 7349, agent episode reward: [-39.76001382408981], time: 97.501
steps: 2816931, episodes: 68000, mean episode reward: -39.78660795715709, num_cumulative_constraints: 7471, agent episode reward: [-39.78660795715709], time: 109.766
steps: 2858059, episodes: 69000, mean episode reward: -39.75095848215811, num_cumulative_constraints: 7579, agent episode reward: [-39.75095848215811], time: 112.543
steps: 2899126, episodes: 70000, mean episode reward: -39.64889986066255, num_cumulative_constraints: 7722, agent episode reward: [-39.64889986066255], time: 145.958
steps: 2940114, episodes: 71000, mean episode reward: -39.56140950654729, num_cumulative_constraints: 7831, agent episode reward: [-39.56140950654729], time: 98.657
steps: 2981309, episodes: 72000, mean episode reward: -39.903693881583486, num_cumulative_constraints: 7982, agent episode reward: [-39.903693881583486], time: 100.552
steps: 3022324, episodes: 73000, mean episode reward: -39.49800892420298, num_cumulative_constraints: 8090, agent episode reward: [-39.49800892420298], time: 92.679
steps: 3063346, episodes: 74000, mean episode reward: -39.48375331022768, num_cumulative_constraints: 8206, agent episode reward: [-39.48375331022768], time: 91.584


with safety_layer
tarting iterations...
steps: 41780, episodes: 1000, mean episode reward: -39.976962305463616, num_cumulative_constraints: 66, agent episode reward: [-39.976962305463616], time: 86.981
steps: 83734, episodes: 2000, mean episode reward: -40.16044512997768, num_cumulative_constraints: 146, agent episode reward: [-40.16044512997768], time: 82.311
steps: 125586, episodes: 3000, mean episode reward: -40.28530162884895, num_cumulative_constraints: 180, agent episode reward: [-40.28530162884895], time: 88.169
steps: 167347, episodes: 4000, mean episode reward: -40.08463860226241, num_cumulative_constraints: 226, agent episode reward: [-40.08463860226241], time: 84.405
steps: 209250, episodes: 5000, mean episode reward: -40.37794587466993, num_cumulative_constraints: 328, agent episode reward: [-40.37794587466993], time: 87.77
steps: 251203, episodes: 6000, mean episode reward: -40.26223521739344, num_cumulative_constraints: 405, agent episode reward: [-40.26223521739344], time: 90.391
steps: 293523, episodes: 7000, mean episode reward: -40.659033457658666, num_cumulative_constraints: 489, agent episode reward: [-40.659033457658666], time: 91.37
steps: 335818, episodes: 8000, mean episode reward: -40.62525643898477, num_cumulative_constraints: 573, agent episode reward: [-40.62525643898477], time: 93.365
steps: 378288, episodes: 9000, mean episode reward: -40.78103296622437, num_cumulative_constraints: 670, agent episode reward: [-40.78103296622437], time: 87.815
steps: 420451, episodes: 10000, mean episode reward: -40.42491721294062, num_cumulative_constraints: 788, agent episode reward: [-40.42491721294062], time: 89.872
steps: 462689, episodes: 11000, mean episode reward: -40.68023907357019, num_cumulative_constraints: 921, agent episode reward: [-40.68023907357019], time: 91.486
steps: 505113, episodes: 12000, mean episode reward: -40.51825533924415, num_cumulative_constraints: 987, agent episode reward: [-40.51825533924415], time: 91.094
steps: 547031, episodes: 13000, mean episode reward: -40.1687005124632, num_cumulative_constraints: 1074, agent episode reward: [-40.1687005124632], time: 92.124
steps: 589046, episodes: 14000, mean episode reward: -40.34322627066334, num_cumulative_constraints: 1181, agent episode reward: [-40.34322627066334], time: 89.83
steps: 631277, episodes: 15000, mean episode reward: -40.36269402343141, num_cumulative_constraints: 1277, agent episode reward: [-40.36269402343141], time: 87.066
steps: 673498, episodes: 16000, mean episode reward: -40.56463540005566, num_cumulative_constraints: 1361, agent episode reward: [-40.56463540005566], time: 86.75
steps: 715402, episodes: 17000, mean episode reward: -40.3295454117572, num_cumulative_constraints: 1477, agent episode reward: [-40.3295454117572], time: 119.676
steps: 757069, episodes: 18000, mean episode reward: -39.91819967573385, num_cumulative_constraints: 1559, agent episode reward: [-39.91819967573385], time: 92.799
steps: 798608, episodes: 19000, mean episode reward: -39.84269685319085, num_cumulative_constraints: 1629, agent episode reward: [-39.84269685319085], time: 86.008
steps: 840175, episodes: 20000, mean episode reward: -40.02312595602616, num_cumulative_constraints: 1716, agent episode reward: [-40.02312595602616], time: 83.1
steps: 881546, episodes: 21000, mean episode reward: -39.77934085058004, num_cumulative_constraints: 1817, agent episode reward: [-39.77934085058004], time: 79.672
steps: 922900, episodes: 22000, mean episode reward: -39.79488831325203, num_cumulative_constraints: 1867, agent episode reward: [-39.79488831325203], time: 80.425
steps: 964395, episodes: 23000, mean episode reward: -39.973692742399, num_cumulative_constraints: 1937, agent episode reward: [-39.973692742399], time: 83.559
steps: 1005739, episodes: 24000, mean episode reward: -39.75190547103886, num_cumulative_constraints: 2015, agent episode reward: [-39.75190547103886], time: 85.581
steps: 1047164, episodes: 25000, mean episode reward: -39.71053023854076, num_cumulative_constraints: 2069, agent episode reward: [-39.71053023854076], time: 85.289
steps: 1088638, episodes: 26000, mean episode reward: -39.87183534819286, num_cumulative_constraints: 2128, agent episode reward: [-39.87183534819286], time: 87.017
steps: 1130008, episodes: 27000, mean episode reward: -39.61486556686998, num_cumulative_constraints: 2179, agent episode reward: [-39.61486556686998], time: 85.603
steps: 1172001, episodes: 28000, mean episode reward: -39.90201443036539, num_cumulative_constraints: 2231, agent episode reward: [-39.90201443036539], time: 86.419
steps: 1214121, episodes: 29000, mean episode reward: -40.11299051095523, num_cumulative_constraints: 2295, agent episode reward: [-40.11299051095523], time: 90.913
steps: 1255635, episodes: 30000, mean episode reward: -39.81211998737983, num_cumulative_constraints: 2365, agent episode reward: [-39.81211998737983], time: 89.495
steps: 1296824, episodes: 31000, mean episode reward: -39.44555677988533, num_cumulative_constraints: 2405, agent episode reward: [-39.44555677988533], time: 86.038
steps: 1338101, episodes: 32000, mean episode reward: -39.60292681173692, num_cumulative_constraints: 2455, agent episode reward: [-39.60292681173692], time: 90.926
steps: 1379332, episodes: 33000, mean episode reward: -39.62918872629405, num_cumulative_constraints: 2532, agent episode reward: [-39.62918872629405], time: 88.983
steps: 1420942, episodes: 34000, mean episode reward: -39.87192832890648, num_cumulative_constraints: 2575, agent episode reward: [-39.87192832890648], time: 92.562
steps: 1462582, episodes: 35000, mean episode reward: -39.875684544032254, num_cumulative_constraints: 2623, agent episode reward: [-39.875684544032254], time: 90.414
steps: 1504385, episodes: 36000, mean episode reward: -39.94796304088011, num_cumulative_constraints: 2675, agent episode reward: [-39.94796304088011], time: 94.502
steps: 1546403, episodes: 37000, mean episode reward: -40.15547183466819, num_cumulative_constraints: 2777, agent episode reward: [-40.15547183466819], time: 95.159
steps: 1588283, episodes: 38000, mean episode reward: -39.82767936911483, num_cumulative_constraints: 2825, agent episode reward: [-39.82767936911483], time: 97.433
steps: 1630192, episodes: 39000, mean episode reward: -39.8008174935813, num_cumulative_constraints: 2892, agent episode reward: [-39.8008174935813], time: 96.481
steps: 1672664, episodes: 40000, mean episode reward: -40.09100468869603, num_cumulative_constraints: 2929, agent episode reward: [-40.09100468869603], time: 94.185
steps: 1715655, episodes: 41000, mean episode reward: -40.43502295065233, num_cumulative_constraints: 2998, agent episode reward: [-40.43502295065233], time: 93.36
steps: 1758095, episodes: 42000, mean episode reward: -40.40562845493309, num_cumulative_constraints: 3091, agent episode reward: [-40.40562845493309], time: 91.965
steps: 1799989, episodes: 43000, mean episode reward: -40.146417580366716, num_cumulative_constraints: 3205, agent episode reward: [-40.146417580366716], time: 88.045
steps: 1841491, episodes: 44000, mean episode reward: -40.055990720597926, num_cumulative_constraints: 3315, agent episode reward: [-40.055990720597926], time: 85.845
steps: 1882929, episodes: 45000, mean episode reward: -39.90687556606374, num_cumulative_constraints: 3436, agent episode reward: [-39.90687556606374], time: 88.08
steps: 1924246, episodes: 46000, mean episode reward: -39.71082629763153, num_cumulative_constraints: 3501, agent episode reward: [-39.71082629763153], time: 84.369
steps: 1965727, episodes: 47000, mean episode reward: -39.972386527157184, num_cumulative_constraints: 3621, agent episode reward: [-39.972386527157184], time: 88.533
steps: 2007136, episodes: 48000, mean episode reward: -39.927524985096944, num_cumulative_constraints: 3694, agent episode reward: [-39.927524985096944], time: 87.822
steps: 2048628, episodes: 49000, mean episode reward: -40.122502777781946, num_cumulative_constraints: 3805, agent episode reward: [-40.122502777781946], time: 87.749
steps: 2090190, episodes: 50000, mean episode reward: -40.107442698184784, num_cumulative_constraints: 3929, agent episode reward: [-40.107442698184784], time: 89.478
steps: 2131578, episodes: 51000, mean episode reward: -39.75764379040078, num_cumulative_constraints: 4011, agent episode reward: [-39.75764379040078], time: 86.929
steps: 2173137, episodes: 52000, mean episode reward: -40.01100665269894, num_cumulative_constraints: 4109, agent episode reward: [-40.01100665269894], time: 89.733
steps: 2214915, episodes: 53000, mean episode reward: -40.23603787219582, num_cumulative_constraints: 4203, agent episode reward: [-40.23603787219582], time: 87.754
steps: 2256917, episodes: 54000, mean episode reward: -40.28373541101467, num_cumulative_constraints: 4318, agent episode reward: [-40.28373541101467], time: 87.065
steps: 2299239, episodes: 55000, mean episode reward: -40.479241358015685, num_cumulative_constraints: 4412, agent episode reward: [-40.479241358015685], time: 90.388
steps: 2341119, episodes: 56000, mean episode reward: -40.08709423085077, num_cumulative_constraints: 4521, agent episode reward: [-40.08709423085077], time: 89.131
steps: 2382668, episodes: 57000, mean episode reward: -39.958328310105436, num_cumulative_constraints: 4604, agent episode reward: [-39.958328310105436], time: 86.649
steps: 2424506, episodes: 58000, mean episode reward: -40.29593535784172, num_cumulative_constraints: 4709, agent episode reward: [-40.29593535784172], time: 93.157
steps: 2466014, episodes: 59000, mean episode reward: -39.99531231890259, num_cumulative_constraints: 4822, agent episode reward: [-39.99531231890259], time: 90.082
steps: 2507817, episodes: 60000, mean episode reward: -40.1227966707515, num_cumulative_constraints: 4908, agent episode reward: [-40.1227966707515], time: 89.49
steps: 2550201, episodes: 61000, mean episode reward: -40.61757150107436, num_cumulative_constraints: 5025, agent episode reward: [-40.61757150107436], time: 93.875
steps: 2591791, episodes: 62000, mean episode reward: -40.24590882660864, num_cumulative_constraints: 5146, agent episode reward: [-40.24590882660864], time: 93.974
steps: 2633037, episodes: 63000, mean episode reward: -39.73798889190998, num_cumulative_constraints: 5235, agent episode reward: [-39.73798889190998], time: 96.874
steps: 2674689, episodes: 64000, mean episode reward: -40.32908788256125, num_cumulative_constraints: 5416, agent episode reward: [-40.32908788256125], time: 99.503
steps: 2716266, episodes: 65000, mean episode reward: -39.99997737041126, num_cumulative_constraints: 5508, agent episode reward: [-39.99997737041126], time: 97.892
steps: 2757670, episodes: 66000, mean episode reward: -39.897599932874186, num_cumulative_constraints: 5617, agent episode reward: [-39.897599932874186], time: 94.53
steps: 2799358, episodes: 67000, mean episode reward: -40.27549916962224, num_cumulative_constraints: 5783, agent episode reward: [-40.27549916962224], time: 100.38
steps: 2840689, episodes: 68000, mean episode reward: -40.04031827272551, num_cumulative_constraints: 5976, agent episode reward: [-40.04031827272551], time: 97.454
steps: 2881978, episodes: 69000, mean episode reward: -39.84489054950552, num_cumulative_constraints: 6109, agent episode reward: [-39.84489054950552], time: 97.375
steps: 2923161, episodes: 70000, mean episode reward: -40.02165842721325, num_cumulative_constraints: 6294, agent episode reward: [-40.02165842721325], time: 98.433
steps: 2964270, episodes: 71000, mean episode reward: -39.81061396101788, num_cumulative_constraints: 6457, agent episode reward: [-39.81061396101788], time: 96.962
steps: 3005678, episodes: 72000, mean episode reward: -39.99106546776902, num_cumulative_constraints: 6596, agent episode reward: [-39.99106546776902], time: 96.699
steps: 3046682, episodes: 73000, mean episode reward: -39.42101234462786, num_cumulative_constraints: 6715, agent episode reward: [-39.42101234462786], time: 97.059
steps: 3088026, episodes: 74000, mean episode reward: -39.948962059992596, num_cumulative_constraints: 6851, agent episode reward: [-39.948962059992596], time: 94.572
steps: 3129223, episodes: 75000, mean episode reward: -39.88912689121246, num_cumulative_constraints: 7014, agent episode reward: [-39.88912689121246], time: 98.751
steps: 3170562, episodes: 76000, mean episode reward: -39.76198499261288, num_cumulative_constraints: 7144, agent episode reward: [-39.76198499261288], time: 93.758
steps: 3212354, episodes: 77000, mean episode reward: -40.34210427162942, num_cumulative_constraints: 7362, agent episode reward: [-40.34210427162942], time: 99.782
steps: 3254839, episodes: 78000, mean episode reward: -40.66189988422726, num_cumulative_constraints: 7591, agent episode reward: [-40.66189988422726], time: 97.704
steps: 3298625, episodes: 79000, mean episode reward: -41.07355616034971, num_cumulative_constraints: 7812, agent episode reward: [-41.07355616034971], time: 100.335
steps: 3344034, episodes: 80000, mean episode reward: -41.959217636407466, num_cumulative_constraints: 8087, agent episode reward: [-41.959217636407466], time: 106.41
steps: 3389029, episodes: 81000, mean episode reward: -41.53539557211683, num_cumulative_constraints: 8357, agent episode reward: [-41.53539557211683], time: 101.592
steps: 3431970, episodes: 82000, mean episode reward: -40.68246019368094, num_cumulative_constraints: 8554, agent episode reward: [-40.68246019368094], time: 96.116
steps: 3474622, episodes: 83000, mean episode reward: -40.25216459643811, num_cumulative_constraints: 8655, agent episode reward: [-40.25216459643811], time: 94.112
steps: 3520186, episodes: 84000, mean episode reward: -41.75955860118264, num_cumulative_constraints: 8807, agent episode reward: [-41.75955860118264], time: 98.983
steps: 3562593, episodes: 85000, mean episode reward: -40.4723827165947, num_cumulative_constraints: 8972, agent episode reward: [-40.4723827165947], time: 97.48
steps: 3603834, episodes: 86000, mean episode reward: -39.92999737834653, num_cumulative_constraints: 9147, agent episode reward: [-39.92999737834653], time: 93.07
steps: 3645120, episodes: 87000, mean episode reward: -39.79677113121468, num_cumulative_constraints: 9252, agent episode reward: [-39.79677113121468], time: 92.622
steps: 3686624, episodes: 88000, mean episode reward: -39.94815828170607, num_cumulative_constraints: 9413, agent episode reward: [-39.94815828170607], time: 99.175
steps: 3728093, episodes: 89000, mean episode reward: -40.04903727437297, num_cumulative_constraints: 9614, agent episode reward: [-40.04903727437297], time: 96.067
steps: 3770074, episodes: 90000, mean episode reward: -40.43978289266514, num_cumulative_constraints: 9819, agent episode reward: [-40.43978289266514], time: 100.738
steps: 3811956, episodes: 91000, mean episode reward: -40.41179817613312, num_cumulative_constraints: 10064, agent episode reward: [-40.41179817613312], time: 96.383
steps: 3853298, episodes: 92000, mean episode reward: -39.80035810787312, num_cumulative_constraints: 10218, agent episode reward: [-39.80035810787312], time: 99.322
steps: 3894987, episodes: 93000, mean episode reward: -40.192436310685004, num_cumulative_constraints: 10459, agent episode reward: [-40.192436310685004], time: 98.449
steps: 3936738, episodes: 94000, mean episode reward: -40.32910716752778, num_cumulative_constraints: 10719, agent episode reward: [-40.32910716752778], time: 100.422
steps: 3978437, episodes: 95000, mean episode reward: -40.179892003761566, num_cumulative_constraints: 10923, agent episode reward: [-40.179892003761566], time: 98.06
steps: 4019846, episodes: 96000, mean episode reward: -40.06831350879362, num_cumulative_constraints: 11120, agent episode reward: [-40.06831350879362], time: 96.061
steps: 4061377, episodes: 97000, mean episode reward: -40.20343932473038, num_cumulative_constraints: 11339, agent episode reward: [-40.20343932473038], time: 100.854
steps: 4102763, episodes: 98000, mean episode reward: -39.94336027379255, num_cumulative_constraints: 11493, agent episode reward: [-39.94336027379255], time: 97.201
steps: 4144327, episodes: 99000, mean episode reward: -40.364221847973326, num_cumulative_constraints: 11720, agent episode reward: [-40.364221847973326], time: 101.486
steps: 4185943, episodes: 100000, mean episode reward: -40.48105173432064, num_cumulative_constraints: 11998, agent episode reward: [-40.48105173432064], time: 101.958
steps: 4227587, episodes: 101000, mean episode reward: -40.08524697964349, num_cumulative_constraints: 12125, agent episode reward: [-40.08524697964349], time: 99.972
steps: 4269237, episodes: 102000, mean episode reward: -40.24488350608166, num_cumulative_constraints: 12333, agent episode reward: [-40.24488350608166], time: 100.561
steps: 4311080, episodes: 103000, mean episode reward: -40.361829064512214, num_cumulative_constraints: 12587, agent episode reward: [-40.361829064512214], time: 100.645
steps: 4353058, episodes: 104000, mean episode reward: -40.817570742635525, num_cumulative_constraints: 12901, agent episode reward: [-40.817570742635525], time: 101.132
steps: 4394582, episodes: 105000, mean episode reward: -40.74899575148351, num_cumulative_constraints: 13289, agent episode reward: [-40.74899575148351], time: 103.015
steps: 4435731, episodes: 106000, mean episode reward: -40.91439064137343, num_cumulative_constraints: 13808, agent episode reward: [-40.91439064137343], time: 102.059
steps: 4477030, episodes: 107000, mean episode reward: -41.16850743260697, num_cumulative_constraints: 14387, agent episode reward: [-41.16850743260697], time: 102.299
steps: 4518457, episodes: 108000, mean episode reward: -41.89306982357538, num_cumulative_constraints: 15195, agent episode reward: [-41.89306982357538], time: 101.739
steps: 4559812, episodes: 109000, mean episode reward: -41.50225663113594, num_cumulative_constraints: 15914, agent episode reward: [-41.50225663113594], time: 99.205
steps: 4601120, episodes: 110000, mean episode reward: -41.27447940896066, num_cumulative_constraints: 16540, agent episode reward: [-41.27447940896066], time: 97.323
steps: 4642551, episodes: 111000, mean episode reward: -41.275686904329596, num_cumulative_constraints: 17136, agent episode reward: [-41.275686904329596], time: 95.262
steps: 4684532, episodes: 112000, mean episode reward: -41.67809249711811, num_cumulative_constraints: 17788, agent episode reward: [-41.67809249711811], time: 97.159
steps: 4726406, episodes: 113000, mean episode reward: -42.002772772315204, num_cumulative_constraints: 18503, agent episode reward: [-42.002772772315204], time: 95.134
steps: 4768576, episodes: 114000, mean episode reward: -42.02551780248449, num_cumulative_constraints: 19204, agent episode reward: [-42.02551780248449], time: 96.508
steps: 4810805, episodes: 115000, mean episode reward: -41.67310415827173, num_cumulative_constraints: 19779, agent episode reward: [-41.67310415827173], time: 93.305
steps: 4853113, episodes: 116000, mean episode reward: -41.9306139973473, num_cumulative_constraints: 20378, agent episode reward: [-41.9306139973473], time: 94.278
steps: 4896026, episodes: 117000, mean episode reward: -42.202604013526724, num_cumulative_constraints: 20963, agent episode reward: [-42.202604013526724], time: 91.297
steps: 4939436, episodes: 118000, mean episode reward: -42.186075222791786, num_cumulative_constraints: 21505, agent episode reward: [-42.186075222791786], time: 91.658
steps: 4983232, episodes: 119000, mean episode reward: -42.843029233001246, num_cumulative_constraints: 22217, agent episode reward: [-42.843029233001246], time: 93.593
steps: 5026436, episodes: 120000, mean episode reward: -42.59266347666591, num_cumulative_constraints: 22914, agent episode reward: [-42.59266347666591], time: 93.048
steps: 5069182, episodes: 121000, mean episode reward: -43.00437928839166, num_cumulative_constraints: 23834, agent episode reward: [-43.00437928839166], time: 91.284
steps: 5111430, episodes: 122000, mean episode reward: -43.325202804109054, num_cumulative_constraints: 24946, agent episode reward: [-43.325202804109054], time: 95.994
steps: 5153725, episodes: 123000, mean episode reward: -44.694852836034514, num_cumulative_constraints: 26499, agent episode reward: [-44.694852836034514], time: 94.031
steps: 5196612, episodes: 124000, mean episode reward: -45.529309385548956, num_cumulative_constraints: 28236, agent episode reward: [-45.529309385548956], time: 99.429
steps: 5239584, episodes: 125000, mean episode reward: -46.74556583626696, num_cumulative_constraints: 30290, agent episode reward: [-46.74556583626696], time: 101.392
steps: 5282308, episodes: 126000, mean episode reward: -46.37538084824147, num_cumulative_constraints: 32285, agent episode reward: [-46.37538084824147], time: 102.983
steps: 5325165, episodes: 127000, mean episode reward: -46.33053476974855, num_cumulative_constraints: 34230, agent episode reward: [-46.33053476974855], time: 100.101
steps: 5367814, episodes: 128000, mean episode reward: -45.3432289331354, num_cumulative_constraints: 35945, agent episode reward: [-45.3432289331354], time: 100.789
steps: 5411056, episodes: 129000, mean episode reward: -47.42576804639019, num_cumulative_constraints: 38158, agent episode reward: [-47.42576804639019], time: 106.243
steps: 5453817, episodes: 130000, mean episode reward: -47.33306425489659, num_cumulative_constraints: 40434, agent episode reward: [-47.33306425489659], time: 106.623
steps: 5496796, episodes: 131000, mean episode reward: -46.24097397991001, num_cumulative_constraints: 42332, agent episode reward: [-46.24097397991001], time: 103.697
steps: 5539296, episodes: 132000, mean episode reward: -45.686814274528096, num_cumulative_constraints: 44150, agent episode reward: [-45.686814274528096], time: 96.752
steps: 5582300, episodes: 133000, mean episode reward: -46.36714710921305, num_cumulative_constraints: 46068, agent episode reward: [-46.36714710921305], time: 101.253
steps: 5625162, episodes: 134000, mean episode reward: -46.620086081480984, num_cumulative_constraints: 48128, agent episode reward: [-46.620086081480984], time: 98.615
steps: 5668072, episodes: 135000, mean episode reward: -46.833495033537886, num_cumulative_constraints: 50169, agent episode reward: [-46.833495033537886], time: 100.355
steps: 5711042, episodes: 136000, mean episode reward: -45.083422734776036, num_cumulative_constraints: 51651, agent episode reward: [-45.083422734776036], time: 91.58
steps: 5753533, episodes: 137000, mean episode reward: -44.750342594140065, num_cumulative_constraints: 53136, agent episode reward: [-44.750342594140065], time: 92.155
steps: 5795978, episodes: 138000, mean episode reward: -44.566839927558085, num_cumulative_constraints: 54517, agent episode reward: [-44.566839927558085], time: 90.075
steps: 5837979, episodes: 139000, mean episode reward: -44.38438212140328, num_cumulative_constraints: 55908, agent episode reward: [-44.38438212140328], time: 88.898
steps: 5880221, episodes: 140000, mean episode reward: -44.247908500583634, num_cumulative_constraints: 57263, agent episode reward: [-44.247908500583634], time: 89.084
steps: 5922484, episodes: 141000, mean episode reward: -45.1757314012701, num_cumulative_constraints: 58919, agent episode reward: [-45.1757314012701], time: 91.119
steps: 5965149, episodes: 142000, mean episode reward: -45.543461752694164, num_cumulative_constraints: 60616, agent episode reward: [-45.543461752694164], time: 91.022
steps: 6007757, episodes: 143000, mean episode reward: -44.78849821816358, num_cumulative_constraints: 62051, agent episode reward: [-44.78849821816358], time: 91.424
steps: 6050502, episodes: 144000, mean episode reward: -43.61351572563212, num_cumulative_constraints: 63165, agent episode reward: [-43.61351572563212], time: 83.73
steps: 6092691, episodes: 145000, mean episode reward: -42.806181443253664, num_cumulative_constraints: 64106, agent episode reward: [-42.806181443253664], time: 81.505
steps: 6134555, episodes: 146000, mean episode reward: -42.198736596027445, num_cumulative_constraints: 64858, agent episode reward: [-42.198736596027445], time: 77.413
steps: 6176838, episodes: 147000, mean episode reward: -42.11237796541567, num_cumulative_constraints: 65504, agent episode reward: [-42.11237796541567], time: 78.942
steps: 6218990, episodes: 148000, mean episode reward: -41.87941334264993, num_cumulative_constraints: 66099, agent episode reward: [-41.87941334264993], time: 75.823
steps: 6261882, episodes: 149000, mean episode reward: -42.80132282153923, num_cumulative_constraints: 66862, agent episode reward: [-42.80132282153923], time: 78.746
steps: 6304090, episodes: 150000, mean episode reward: -41.675955665913584, num_cumulative_constraints: 67414, agent episode reward: [-41.675955665913584], time: 76.573
steps: 6346778, episodes: 151000, mean episode reward: -41.951862913407126, num_cumulative_constraints: 67924, agent episode reward: [-41.951862913407126], time: 76.375
steps: 6389642, episodes: 152000, mean episode reward: -41.947221163362514, num_cumulative_constraints: 68417, agent episode reward: [-41.947221163362514], time: 79.711
steps: 6432340, episodes: 153000, mean episode reward: -41.07262715719761, num_cumulative_constraints: 68721, agent episode reward: [-41.07262715719761], time: 76.165
steps: 6474965, episodes: 154000, mean episode reward: -41.08226941965394, num_cumulative_constraints: 69035, agent episode reward: [-41.08226941965394], time: 73.403
steps: 6517159, episodes: 155000, mean episode reward: -40.971199916720316, num_cumulative_constraints: 69390, agent episode reward: [-40.971199916720316], time: 76.69
steps: 6558993, episodes: 156000, mean episode reward: -40.90928662086232, num_cumulative_constraints: 69731, agent episode reward: [-40.90928662086232], time: 74.236
steps: 6600569, episodes: 157000, mean episode reward: -40.72853244389431, num_cumulative_constraints: 70082, agent episode reward: [-40.72853244389431], time: 74.354
steps: 6642085, episodes: 158000, mean episode reward: -40.431612388162904, num_cumulative_constraints: 70338, agent episode reward: [-40.431612388162904], time: 72.479
steps: 6683688, episodes: 159000, mean episode reward: -40.921061190428894, num_cumulative_constraints: 70663, agent episode reward: [-40.921061190428894], time: 78.111
steps: 6725126, episodes: 160000, mean episode reward: -40.51357659959514, num_cumulative_constraints: 70906, agent episode reward: [-40.51357659959514], time: 76.919
steps: 6766513, episodes: 161000, mean episode reward: -40.259526889642295, num_cumulative_constraints: 71133, agent episode reward: [-40.259526889642295], time: 80.31
steps: 6807929, episodes: 162000, mean episode reward: -40.16152171218004, num_cumulative_constraints: 71311, agent episode reward: [-40.16152171218004], time: 77.826
steps: 6849237, episodes: 163000, mean episode reward: -40.19607276641342, num_cumulative_constraints: 71549, agent episode reward: [-40.19607276641342], time: 79.699
steps: 6890684, episodes: 164000, mean episode reward: -40.36013351525247, num_cumulative_constraints: 71775, agent episode reward: [-40.36013351525247], time: 83.191
steps: 6932558, episodes: 165000, mean episode reward: -40.59282987260943, num_cumulative_constraints: 72006, agent episode reward: [-40.59282987260943], time: 83.486
steps: 6974318, episodes: 166000, mean episode reward: -40.77029300833646, num_cumulative_constraints: 72250, agent episode reward: [-40.77029300833646], time: 82.021
steps: 7015899, episodes: 167000, mean episode reward: -40.664215244232835, num_cumulative_constraints: 72525, agent episode reward: [-40.664215244232835], time: 80.665
steps: 7057399, episodes: 168000, mean episode reward: -40.587384261975835, num_cumulative_constraints: 72794, agent episode reward: [-40.587384261975835], time: 81.331
steps: 7098961, episodes: 169000, mean episode reward: -40.65866628565524, num_cumulative_constraints: 73064, agent episode reward: [-40.65866628565524], time: 83.227
steps: 7140917, episodes: 170000, mean episode reward: -41.62976670968897, num_cumulative_constraints: 73475, agent episode reward: [-41.62976670968897], time: 83.712
steps: 7182563, episodes: 171000, mean episode reward: -40.955070475458065, num_cumulative_constraints: 73859, agent episode reward: [-40.955070475458065], time: 82.506
steps: 7224139, episodes: 172000, mean episode reward: -41.03344112857597, num_cumulative_constraints: 74200, agent episode reward: [-41.03344112857597], time: 78.479
steps: 7266065, episodes: 173000, mean episode reward: -41.374474568178265, num_cumulative_constraints: 74646, agent episode reward: [-41.374474568178265], time: 82.386
steps: 7307767, episodes: 174000, mean episode reward: -41.168214327152405, num_cumulative_constraints: 75049, agent episode reward: [-41.168214327152405], time: 81.192
steps: 7349334, episodes: 175000, mean episode reward: -41.00564040834726, num_cumulative_constraints: 75476, agent episode reward: [-41.00564040834726], time: 80.225
steps: 7391490, episodes: 176000, mean episode reward: -41.285550578210795, num_cumulative_constraints: 75895, agent episode reward: [-41.285550578210795], time: 81.071
steps: 7433607, episodes: 177000, mean episode reward: -41.33752767435394, num_cumulative_constraints: 76254, agent episode reward: [-41.33752767435394], time: 81.253
steps: 7475573, episodes: 178000, mean episode reward: -40.93328039882492, num_cumulative_constraints: 76549, agent episode reward: [-40.93328039882492], time: 81.702
steps: 7517533, episodes: 179000, mean episode reward: -41.03000915191042, num_cumulative_constraints: 76935, agent episode reward: [-41.03000915191042], time: 84.833
steps: 7559870, episodes: 180000, mean episode reward: -40.95174379941886, num_cumulative_constraints: 77265, agent episode reward: [-40.95174379941886], time: 86.016
steps: 7601910, episodes: 181000, mean episode reward: -40.73968265253179, num_cumulative_constraints: 77542, agent episode reward: [-40.73968265253179], time: 83.214
steps: 7644018, episodes: 182000, mean episode reward: -41.292154858287475, num_cumulative_constraints: 77948, agent episode reward: [-41.292154858287475], time: 85.884
steps: 7686092, episodes: 183000, mean episode reward: -41.95001964537306, num_cumulative_constraints: 78430, agent episode reward: [-41.95001964537306], time: 91.005
steps: 7728188, episodes: 184000, mean episode reward: -41.68538611153828, num_cumulative_constraints: 78886, agent episode reward: [-41.68538611153828], time: 86.918
steps: 7770472, episodes: 185000, mean episode reward: -42.324629961043435, num_cumulative_constraints: 79436, agent episode reward: [-42.324629961043435], time: 86.826
steps: 7813215, episodes: 186000, mean episode reward: -42.14708919656207, num_cumulative_constraints: 79958, agent episode reward: [-42.14708919656207], time: 91.038
steps: 7856238, episodes: 187000, mean episode reward: -42.296673196188436, num_cumulative_constraints: 80592, agent episode reward: [-42.296673196188436], time: 89.149
steps: 7899387, episodes: 188000, mean episode reward: -42.53916056928265, num_cumulative_constraints: 81244, agent episode reward: [-42.53916056928265], time: 87.124
steps: 7941904, episodes: 189000, mean episode reward: -42.62062215884751, num_cumulative_constraints: 81936, agent episode reward: [-42.62062215884751], time: 84.754
steps: 7983944, episodes: 190000, mean episode reward: -42.116426031849976, num_cumulative_constraints: 82596, agent episode reward: [-42.116426031849976], time: 88.516
steps: 8025972, episodes: 191000, mean episode reward: -41.83019955918261, num_cumulative_constraints: 83146, agent episode reward: [-41.83019955918261], time: 86.422
steps: 8067645, episodes: 192000, mean episode reward: -41.39579183812684, num_cumulative_constraints: 83626, agent episode reward: [-41.39579183812684], time: 85.339
steps: 8109175, episodes: 193000, mean episode reward: -40.94683995952352, num_cumulative_constraints: 84037, agent episode reward: [-40.94683995952352], time: 80.737
steps: 8150661, episodes: 194000, mean episode reward: -41.322021608378535, num_cumulative_constraints: 84575, agent episode reward: [-41.322021608378535], time: 84.047
steps: 8192234, episodes: 195000, mean episode reward: -41.51097596746655, num_cumulative_constraints: 85112, agent episode reward: [-41.51097596746655], time: 84.956
steps: 8233926, episodes: 196000, mean episode reward: -41.38786262602865, num_cumulative_constraints: 85616, agent episode reward: [-41.38786262602865], time: 83.969
steps: 8275836, episodes: 197000, mean episode reward: -41.398227440553434, num_cumulative_constraints: 86068, agent episode reward: [-41.398227440553434], time: 81.151
steps: 8317239, episodes: 198000, mean episode reward: -41.129135625740496, num_cumulative_constraints: 86556, agent episode reward: [-41.129135625740496], time: 82.351



1s 0.24x
without safety_layer
steps: 40657, episodes: 1000, mean episode reward: -39.19822689697923, num_cumulative_constraints: 204, agent episode reward: [-39.19822689697923], time: 59.645
steps: 81710, episodes: 2000, mean episode reward: -39.618573187731975, num_cumulative_constraints: 477, agent episode reward: [-39.618573187731975], time: 66.788
steps: 122335, episodes: 3000, mean episode reward: -38.91539249235821, num_cumulative_constraints: 623, agent episode reward: [-38.91539249235821], time: 82.37
steps: 162949, episodes: 4000, mean episode reward: -38.91300791205745, num_cumulative_constraints: 788, agent episode reward: [-38.91300791205745], time: 107.596
steps: 203705, episodes: 5000, mean episode reward: -38.93903020203057, num_cumulative_constraints: 925, agent episode reward: [-38.93903020203057], time: 65.053
steps: 244284, episodes: 6000, mean episode reward: -38.6608178345891, num_cumulative_constraints: 1007, agent episode reward: [-38.6608178345891], time: 64.732
steps: 284996, episodes: 7000, mean episode reward: -38.72049772456949, num_cumulative_constraints: 1099, agent episode reward: [-38.72049772456949], time: 66.364
steps: 325731, episodes: 8000, mean episode reward: -38.654015055155035, num_cumulative_constraints: 1164, agent episode reward: [-38.654015055155035], time: 70.125
steps: 366730, episodes: 9000, mean episode reward: -38.78485684276948, num_cumulative_constraints: 1232, agent episode reward: [-38.78485684276948], time: 68.598
steps: 407378, episodes: 10000, mean episode reward: -38.95042881754078, num_cumulative_constraints: 1371, agent episode reward: [-38.95042881754078], time: 67.847
steps: 448025, episodes: 11000, mean episode reward: -38.8337373875465, num_cumulative_constraints: 1497, agent episode reward: [-38.8337373875465], time: 67.396
steps: 488647, episodes: 12000, mean episode reward: -38.83267404948499, num_cumulative_constraints: 1634, agent episode reward: [-38.83267404948499], time: 63.197


without safety-layer once again 0.24rad/s
steps: 40422, episodes: 1000, mean episode reward: -38.70881830648586, num_cumulative_constraints: 182, agent episode reward: [-38.70881830648586], time: 58.868
steps: 81208, episodes: 2000, mean episode reward: -38.9330882540616, num_cumulative_constraints: 343, agent episode reward: [-38.9330882540616], time: 67.504
steps: 121927, episodes: 3000, mean episode reward: -38.642301991574875, num_cumulative_constraints: 431, agent episode reward: [-38.642301991574875], time: 73.973
steps: 162462, episodes: 4000, mean episode reward: -38.6821133260747, num_cumulative_constraints: 562, agent episode reward: [-38.6821133260747], time: 75.191
steps: 202953, episodes: 5000, mean episode reward: -38.6543595188974, num_cumulative_constraints: 676, agent episode reward: [-38.6543595188974], time: 74.958
steps: 243587, episodes: 6000, mean episode reward: -38.8377974940594, num_cumulative_constraints: 785, agent episode reward: [-38.8377974940594], time: 72.034
steps: 284201, episodes: 7000, mean episode reward: -38.701193348878874, num_cumulative_constraints: 884, agent episode reward: [-38.701193348878874], time: 71.441
steps: 324784, episodes: 8000, mean episode reward: -38.798807083000405, num_cumulative_constraints: 1041, agent episode reward: [-38.798807083000405], time: 73.991
steps: 365424, episodes: 9000, mean episode reward: -38.719648763786736, num_cumulative_constraints: 1144, agent episode reward: [-38.719648763786736], time: 74.723
steps: 406029, episodes: 10000, mean episode reward: -38.66777241773311, num_cumulative_constraints: 1253, agent episode reward: [-38.66777241773311], time: 74.056
steps: 446667, episodes: 11000, mean episode reward: -38.672555089834596, num_cumulative_constraints: 1376, agent episode reward: [-38.672555089834596], time: 67.93
steps: 487381, episodes: 12000, mean episode reward: -38.74677401217276, num_cumulative_constraints: 1494, agent episode reward: [-38.74677401217276], time: 62.104
steps: 528174, episodes: 13000, mean episode reward: -38.67719125831192, num_cumulative_constraints: 1607, agent episode reward: [-38.67719125831192], time: 61.716
steps: 568704, episodes: 14000, mean episode reward: -38.70539554668966, num_cumulative_constraints: 1784, agent episode reward: [-38.70539554668966], time: 62.642
steps: 609264, episodes: 15000, mean episode reward: -38.58158362822174, num_cumulative_constraints: 1883, agent episode reward: [-38.58158362822174], time: 72.778
steps: 649807, episodes: 16000, mean episode reward: -38.71553418575452, num_cumulative_constraints: 2015, agent episode reward: [-38.71553418575452], time: 75.004
steps: 690200, episodes: 17000, mean episode reward: -38.392977629710835, num_cumulative_constraints: 2104, agent episode reward: [-38.392977629710835], time: 75.715
steps: 730682, episodes: 18000, mean episode reward: -38.556191738072116, num_cumulative_constraints: 2191, agent episode reward: [-38.556191738072116], time: 76.611
steps: 771099, episodes: 19000, mean episode reward: -38.47110236170648, num_cumulative_constraints: 2256, agent episode reward: [-38.47110236170648], time: 68.74
steps: 811572, episodes: 20000, mean episode reward: -38.513931618147524, num_cumulative_constraints: 2347, agent episode reward: [-38.513931618147524], time: 73.623
steps: 852278, episodes: 21000, mean episode reward: -38.976282605275, num_cumulative_constraints: 2493, agent episode reward: [-38.976282605275], time: 67.094
steps: 892774, episodes: 22000, mean episode reward: -38.47487223944692, num_cumulative_constraints: 2566, agent episode reward: [-38.47487223944692], time: 59.594
steps: 933164, episodes: 23000, mean episode reward: -38.34296228097874, num_cumulative_constraints: 2616, agent episode reward: [-38.34296228097874], time: 68.58
steps: 973537, episodes: 24000, mean episode reward: -38.62382121847739, num_cumulative_constraints: 2728, agent episode reward: [-38.62382121847739], time: 56.693
steps: 1013779, episodes: 25000, mean episode reward: -38.319448010273554, num_cumulative_constraints: 2805, agent episode reward: [-38.319448010273554], time: 62.906
steps: 1054278, episodes: 26000, mean episode reward: -38.75694008792838, num_cumulative_constraints: 2940, agent episode reward: [-38.75694008792838], time: 81.949
steps: 1094749, episodes: 27000, mean episode reward: -38.779596401010494, num_cumulative_constraints: 3089, agent episode reward: [-38.779596401010494], time: 73.626
steps: 1135186, episodes: 28000, mean episode reward: -38.68422293904985, num_cumulative_constraints: 3221, agent episode reward: [-38.68422293904985], time: 59.349
steps: 1175548, episodes: 29000, mean episode reward: -38.71771489335141, num_cumulative_constraints: 3364, agent episode reward: [-38.71771489335141], time: 61.12
steps: 1215970, episodes: 30000, mean episode reward: -38.47632314151271, num_cumulative_constraints: 3444, agent episode reward: [-38.47632314151271], time: 75.06
steps: 1256393, episodes: 31000, mean episode reward: -38.51953737117165, num_cumulative_constraints: 3524, agent episode reward: [-38.51953737117165], time: 73.026
steps: 1296770, episodes: 32000, mean episode reward: -38.552762627042995, num_cumulative_constraints: 3603, agent episode reward: [-38.552762627042995], time: 75.42
steps: 1337025, episodes: 33000, mean episode reward: -38.51335785091245, num_cumulative_constraints: 3715, agent episode reward: [-38.51335785091245], time: 74.45
steps: 1377371, episodes: 34000, mean episode reward: -38.775965758160446, num_cumulative_constraints: 3862, agent episode reward: [-38.775965758160446], time: 75.728
steps: 1417576, episodes: 35000, mean episode reward: -38.392669181262654, num_cumulative_constraints: 3955, agent episode reward: [-38.392669181262654], time: 76.459
steps: 1457825, episodes: 36000, mean episode reward: -38.48716961649772, num_cumulative_constraints: 4058, agent episode reward: [-38.48716961649772], time: 74.979
steps: 1498136, episodes: 37000, mean episode reward: -38.63782553089544, num_cumulative_constraints: 4166, agent episode reward: [-38.63782553089544], time: 71.46
steps: 1538396, episodes: 38000, mean episode reward: -38.50758569943655, num_cumulative_constraints: 4262, agent episode reward: [-38.50758569943655], time: 71.717
steps: 1578639, episodes: 39000, mean episode reward: -38.64198490421682, num_cumulative_constraints: 4390, agent episode reward: [-38.64198490421682], time: 64.33
steps: 1618938, episodes: 40000, mean episode reward: -38.57724485347046, num_cumulative_constraints: 4506, agent episode reward: [-38.57724485347046], time: 62.879
steps: 1659256, episodes: 41000, mean episode reward: -38.52882147976403, num_cumulative_constraints: 4617, agent episode reward: [-38.52882147976403], time: 62.723
steps: 1699536, episodes: 42000, mean episode reward: -38.56820721572636, num_cumulative_constraints: 4716, agent episode reward: [-38.56820721572636], time: 62.529
steps: 1739749, episodes: 43000, mean episode reward: -38.539987898577294, num_cumulative_constraints: 4834, agent episode reward: [-38.539987898577294], time: 63.371
steps: 1780113, episodes: 44000, mean episode reward: -38.65133173931602, num_cumulative_constraints: 4938, agent episode reward: [-38.65133173931602], time: 62.87
steps: 1820312, episodes: 45000, mean episode reward: -38.31367063213085, num_cumulative_constraints: 5010, agent episode reward: [-38.31367063213085], time: 66.73
steps: 1860559, episodes: 46000, mean episode reward: -38.45992162896406, num_cumulative_constraints: 5106, agent episode reward: [-38.45992162896406], time: 77.547
steps: 1900803, episodes: 47000, mean episode reward: -38.434548116032694, num_cumulative_constraints: 5204, agent episode reward: [-38.434548116032694], time: 74.81
steps: 1941096, episodes: 48000, mean episode reward: -38.49147902990061, num_cumulative_constraints: 5285, agent episode reward: [-38.49147902990061], time: 63.596
steps: 1981377, episodes: 49000, mean episode reward: -38.46700630312899, num_cumulative_constraints: 5354, agent episode reward: [-38.46700630312899], time: 62.987
steps: 2021644, episodes: 50000, mean episode reward: -38.399925690342904, num_cumulative_constraints: 5408, agent episode reward: [-38.399925690342904], time: 62.848
steps: 2061819, episodes: 51000, mean episode reward: -38.503326763363816, num_cumulative_constraints: 5528, agent episode reward: [-38.503326763363816], time: 69.558
steps: 2102058, episodes: 52000, mean episode reward: -38.45786159437972, num_cumulative_constraints: 5625, agent episode reward: [-38.45786159437972], time: 64.518
steps: 2142316, episodes: 53000, mean episode reward: -38.55153311797183, num_cumulative_constraints: 5731, agent episode reward: [-38.55153311797183], time: 66.99
steps: 2182510, episodes: 54000, mean episode reward: -38.57459515815332, num_cumulative_constraints: 5895, agent episode reward: [-38.57459515815332], time: 70.961
steps: 2222777, episodes: 55000, mean episode reward: -38.55885349359273, num_cumulative_constraints: 6002, agent episode reward: [-38.55885349359273], time: 74.397
steps: 2263063, episodes: 56000, mean episode reward: -38.4368820866321, num_cumulative_constraints: 6087, agent episode reward: [-38.4368820866321], time: 73.171
steps: 2303407, episodes: 57000, mean episode reward: -38.66746650188739, num_cumulative_constraints: 6224, agent episode reward: [-38.66746650188739], time: 74.241
steps: 2343699, episodes: 58000, mean episode reward: -38.5789854149868, num_cumulative_constraints: 6323, agent episode reward: [-38.5789854149868], time: 75.392
steps: 2383904, episodes: 59000, mean episode reward: -38.538249122131134, num_cumulative_constraints: 6446, agent episode reward: [-38.538249122131134], time: 69.909
steps: 2424096, episodes: 60000, mean episode reward: -38.40544861002711, num_cumulative_constraints: 6554, agent episode reward: [-38.40544861002711], time: 75.303
steps: 2464489, episodes: 61000, mean episode reward: -38.59641617779452, num_cumulative_constraints: 6664, agent episode reward: [-38.59641617779452], time: 75.811
steps: 2504912, episodes: 62000, mean episode reward: -38.44137985303538, num_cumulative_constraints: 6754, agent episode reward: [-38.44137985303538], time: 72.796
steps: 2545274, episodes: 63000, mean episode reward: -38.64497044182213, num_cumulative_constraints: 6909, agent episode reward: [-38.64497044182213], time: 70.522
steps: 2585704, episodes: 64000, mean episode reward: -38.60560680248903, num_cumulative_constraints: 7027, agent episode reward: [-38.60560680248903], time: 64.651
steps: 2626049, episodes: 65000, mean episode reward: -38.534847410319344, num_cumulative_constraints: 7122, agent episode reward: [-38.534847410319344], time: 75.38
steps: 2666570, episodes: 66000, mean episode reward: -38.71672693113776, num_cumulative_constraints: 7222, agent episode reward: [-38.71672693113776], time: 71.818
steps: 2706868, episodes: 67000, mean episode reward: -38.35316963060863, num_cumulative_constraints: 7292, agent episode reward: [-38.35316963060863], time: 77.108
steps: 2747088, episodes: 68000, mean episode reward: -38.38215793669765, num_cumulative_constraints: 7372, agent episode reward: [-38.38215793669765], time: 63.152
steps: 2787665, episodes: 69000, mean episode reward: -38.53255950176369, num_cumulative_constraints: 7450, agent episode reward: [-38.53255950176369], time: 77.035
steps: 2828369, episodes: 70000, mean episode reward: -38.762135209804605, num_cumulative_constraints: 7614, agent episode reward: [-38.762135209804605], time: 72.484
steps: 2869210, episodes: 71000, mean episode reward: -38.917770097522094, num_cumulative_constraints: 7768, agent episode reward: [-38.917770097522094], time: 82.524
steps: 2909719, episodes: 72000, mean episode reward: -38.47399819460415, num_cumulative_constraints: 7855, agent episode reward: [-38.47399819460415], time: 70.43
steps: 2950260, episodes: 73000, mean episode reward: -38.8559971519568, num_cumulative_constraints: 8033, agent episode reward: [-38.8559971519568], time: 64.5
steps: 2990663, episodes: 74000, mean episode reward: -38.64789747467755, num_cumulative_constraints: 8139, agent episode reward: [-38.64789747467755], time: 70.085
steps: 3030928, episodes: 75000, mean episode reward: -38.51242273259562, num_cumulative_constraints: 8248, agent episode reward: [-38.51242273259562], time: 35.349
steps: 3071251, episodes: 76000, mean episode reward: -38.527293398768634, num_cumulative_constraints: 8363, agent episode reward: [-38.527293398768634], time: 34.414
steps: 3111579, episodes: 77000, mean episode reward: -38.325638694935336, num_cumulative_constraints: 8422, agent episode reward: [-38.325638694935336], time: 35.679
steps: 3151988, episodes: 78000, mean episode reward: -38.463760997444545, num_cumulative_constraints: 8495, agent episode reward: [-38.463760997444545], time: 34.24
steps: 3192220, episodes: 79000, mean episode reward: -38.37347816342286, num_cumulative_constraints: 8604, agent episode reward: [-38.37347816342286], time: 33.491
steps: 3232580, episodes: 80000, mean episode reward: -38.4362011987626, num_cumulative_constraints: 8686, agent episode reward: [-38.4362011987626], time: 36.123
steps: 3272906, episodes: 81000, mean episode reward: -38.41766078222948, num_cumulative_constraints: 8760, agent episode reward: [-38.41766078222948], time: 34.884
steps: 3313279, episodes: 82000, mean episode reward: -38.44844715123387, num_cumulative_constraints: 8820, agent episode reward: [-38.44844715123387], time: 52.832
steps: 3353626, episodes: 83000, mean episode reward: -38.53938116727985, num_cumulative_constraints: 8921, agent episode reward: [-38.53938116727985], time: 37.933
steps: 3393986, episodes: 84000, mean episode reward: -38.292653638985314, num_cumulative_constraints: 8968, agent episode reward: [-38.292653638985314], time: 44.255
steps: 3434315, episodes: 85000, mean episode reward: -38.44925425037666, num_cumulative_constraints: 9080, agent episode reward: [-38.44925425037666], time: 39.499


with safety
steps: 40360, episodes: 1000, mean episode reward: -38.47713334367935, num_cumulative_constraints: 45, agent episode reward: [-38.47713334367935], time: 157.293
steps: 80699, episodes: 2000, mean episode reward: -38.20982423577791, num_cumulative_constraints: 74, agent episode reward: [-38.20982423577791], time: 161.794
steps: 121201, episodes: 3000, mean episode reward: -38.39989175522424, num_cumulative_constraints: 93, agent episode reward: [-38.39989175522424], time: 170.648
steps: 161642, episodes: 4000, mean episode reward: -38.42161522588355, num_cumulative_constraints: 127, agent episode reward: [-38.42161522588355], time: 191.168
steps: 201987, episodes: 5000, mean episode reward: -38.387108685460234, num_cumulative_constraints: 160, agent episode reward: [-38.387108685460234], time: 180.426
steps: 242497, episodes: 6000, mean episode reward: -38.630370343014846, num_cumulative_constraints: 214, agent episode reward: [-38.630370343014846], time: 253.129
steps: 283069, episodes: 7000, mean episode reward: -38.60326728610335, num_cumulative_constraints: 251, agent episode reward: [-38.60326728610335], time: 305.281


omege_diff < 0.3 without safety_layer
steps: 41058, episodes: 1000, mean episode reward: -48.10054988997059, num_cumulative_constraints: 242, agent episode reward: [-48.10054988997059], time: 36.074
steps: 82662, episodes: 2000, mean episode reward: -48.674404973858735, num_cumulative_constraints: 543, agent episode reward: [-48.674404973858735], time: 39.365
steps: 124790, episodes: 3000, mean episode reward: -48.66378849255694, num_cumulative_constraints: 861, agent episode reward: [-48.66378849255694], time: 46.092
steps: 166954, episodes: 4000, mean episode reward: -48.603599262121975, num_cumulative_constraints: 1180, agent episode reward: [-48.603599262121975], time: 49.186
steps: 208849, episodes: 5000, mean episode reward: -48.07247931459012, num_cumulative_constraints: 1458, agent episode reward: [-48.07247931459012], time: 45.049
steps: 250537, episodes: 6000, mean episode reward: -47.930508933846106, num_cumulative_constraints: 1732, agent episode reward: [-47.930508933846106], time: 47.4
steps: 292438, episodes: 7000, mean episode reward: -48.102714364551694, num_cumulative_constraints: 2127, agent episode reward: [-48.102714364551694], time: 45.499
steps: 334446, episodes: 8000, mean episode reward: -48.11638670545773, num_cumulative_constraints: 2521, agent episode reward: [-48.11638670545773], time: 45.213
steps: 377404, episodes: 9000, mean episode reward: -48.97183949450954, num_cumulative_constraints: 2961, agent episode reward: [-48.97183949450954], time: 43.897
steps: 419852, episodes: 10000, mean episode reward: -48.42125790346498, num_cumulative_constraints: 3335, agent episode reward: [-48.42125790346498], time: 46.777
steps: 462799, episodes: 11000, mean episode reward: -48.232223612447356, num_cumulative_constraints: 3624, agent episode reward: [-48.232223612447356], time: 48.118
steps: 504530, episodes: 12000, mean episode reward: -47.92112579859894, num_cumulative_constraints: 3988, agent episode reward: [-47.92112579859894], time: 46.005
steps: 545970, episodes: 13000, mean episode reward: -47.897944872330044, num_cumulative_constraints: 4324, agent episode reward: [-47.897944872330044], time: 44.364
steps: 587270, episodes: 14000, mean episode reward: -47.46896660510864, num_cumulative_constraints: 4661, agent episode reward: [-47.46896660510864], time: 43.727
steps: 628838, episodes: 15000, mean episode reward: -48.320047824380474, num_cumulative_constraints: 5053, agent episode reward: [-48.320047824380474], time: 44.388
steps: 670348, episodes: 16000, mean episode reward: -47.889863384928894, num_cumulative_constraints: 5375, agent episode reward: [-47.889863384928894], time: 43.26
steps: 711866, episodes: 17000, mean episode reward: -48.012692663648615, num_cumulative_constraints: 5709, agent episode reward: [-48.012692663648615], time: 43.902
steps: 753568, episodes: 18000, mean episode reward: -47.96802769817831, num_cumulative_constraints: 6037, agent episode reward: [-47.96802769817831], time: 44.808
steps: 795099, episodes: 19000, mean episode reward: -47.43706738999908, num_cumulative_constraints: 6321, agent episode reward: [-47.43706738999908], time: 42.496
steps: 836460, episodes: 20000, mean episode reward: -47.43334568811377, num_cumulative_constraints: 6663, agent episode reward: [-47.43334568811377], time: 44.199
steps: 878049, episodes: 21000, mean episode reward: -47.52483002918988, num_cumulative_constraints: 6978, agent episode reward: [-47.52483002918988], time: 45.09
steps: 919609, episodes: 22000, mean episode reward: -48.15566231119983, num_cumulative_constraints: 7375, agent episode reward: [-48.15566231119983], time: 42.997
steps: 961013, episodes: 23000, mean episode reward: -47.47423776825056, num_cumulative_constraints: 7712, agent episode reward: [-47.47423776825056], time: 46.165
steps: 1002505, episodes: 24000, mean episode reward: -47.628876661105416, num_cumulative_constraints: 8060, agent episode reward: [-47.628876661105416], time: 47.235
steps: 1044039, episodes: 25000, mean episode reward: -47.51292919335529, num_cumulative_constraints: 8346, agent episode reward: [-47.51292919335529], time: 44.085
steps: 1085441, episodes: 26000, mean episode reward: -47.54274428318713, num_cumulative_constraints: 8700, agent episode reward: [-47.54274428318713], time: 46.02
steps: 1126855, episodes: 27000, mean episode reward: -47.297652021657065, num_cumulative_constraints: 8994, agent episode reward: [-47.297652021657065], time: 48.754
steps: 1168275, episodes: 28000, mean episode reward: -47.44880754212056, num_cumulative_constraints: 9297, agent episode reward: [-47.44880754212056], time: 45.262
steps: 1209954, episodes: 29000, mean episode reward: -47.71701522675813, num_cumulative_constraints: 9593, agent episode reward: [-47.71701522675813], time: 48.893
steps: 1251309, episodes: 30000, mean episode reward: -47.40569308052427, num_cumulative_constraints: 9906, agent episode reward: [-47.40569308052427], time: 48.106
steps: 1292708, episodes: 31000, mean episode reward: -47.47882728239718, num_cumulative_constraints: 10283, agent episode reward: [-47.47882728239718], time: 47.317
steps: 1334460, episodes: 32000, mean episode reward: -47.520729979758194, num_cumulative_constraints: 10561, agent episode reward: [-47.520729979758194], time: 50.073
steps: 1375884, episodes: 33000, mean episode reward: -47.351798343883, num_cumulative_constraints: 10927, agent episode reward: [-47.351798343883], time: 49.584
steps: 1417469, episodes: 34000, mean episode reward: -47.68405756195372, num_cumulative_constraints: 11267, agent episode reward: [-47.68405756195372], time: 46.942
steps: 1458751, episodes: 35000, mean episode reward: -47.20270697236732, num_cumulative_constraints: 11589, agent episode reward: [-47.20270697236732], time: 48.709
steps: 1500235, episodes: 36000, mean episode reward: -47.710551212674254, num_cumulative_constraints: 11975, agent episode reward: [-47.710551212674254], time: 45.947
steps: 1541904, episodes: 37000, mean episode reward: -47.53578493546273, num_cumulative_constraints: 12399, agent episode reward: [-47.53578493546273], time: 46.586
steps: 1583611, episodes: 38000, mean episode reward: -47.73229738540253, num_cumulative_constraints: 12763, agent episode reward: [-47.73229738540253], time: 48.85
steps: 1625091, episodes: 39000, mean episode reward: -47.327626796945694, num_cumulative_constraints: 13084, agent episode reward: [-47.327626796945694], time: 52.496
steps: 1666476, episodes: 40000, mean episode reward: -47.20205132733195, num_cumulative_constraints: 13395, agent episode reward: [-47.20205132733195], time: 49.888
steps: 1707959, episodes: 41000, mean episode reward: -47.20150559178129, num_cumulative_constraints: 13658, agent episode reward: [-47.20150559178129], time: 51.603
steps: 1749641, episodes: 42000, mean episode reward: -47.76855751576022, num_cumulative_constraints: 14005, agent episode reward: [-47.76855751576022], time: 47.49
steps: 1791058, episodes: 43000, mean episode reward: -47.83927111296514, num_cumulative_constraints: 14367, agent episode reward: [-47.83927111296514], time: 49.706
steps: 1832400, episodes: 44000, mean episode reward: -47.551599883793074, num_cumulative_constraints: 14697, agent episode reward: [-47.551599883793074], time: 48.136
steps: 1873774, episodes: 45000, mean episode reward: -47.66888656989286, num_cumulative_constraints: 15056, agent episode reward: [-47.66888656989286], time: 47.37
steps: 1915186, episodes: 46000, mean episode reward: -47.551466103648565, num_cumulative_constraints: 15363, agent episode reward: [-47.551466103648565], time: 47.605
steps: 1956561, episodes: 47000, mean episode reward: -47.070993085943115, num_cumulative_constraints: 15668, agent episode reward: [-47.070993085943115], time: 46.594
steps: 1998067, episodes: 48000, mean episode reward: -47.574478072432775, num_cumulative_constraints: 16002, agent episode reward: [-47.574478072432775], time: 47.786
steps: 2039720, episodes: 49000, mean episode reward: -47.15878979451597, num_cumulative_constraints: 16254, agent episode reward: [-47.15878979451597], time: 44.635
steps: 2081535, episodes: 50000, mean episode reward: -47.70533750053501, num_cumulative_constraints: 16563, agent episode reward: [-47.70533750053501], time: 61.907
steps: 2122962, episodes: 51000, mean episode reward: -47.42721662622668, num_cumulative_constraints: 17004, agent episode reward: [-47.42721662622668], time: 45.591
steps: 2164538, episodes: 52000, mean episode reward: -47.17465493113194, num_cumulative_constraints: 17254, agent episode reward: [-47.17465493113194], time: 47.091
steps: 2205952, episodes: 53000, mean episode reward: -47.07808797619276, num_cumulative_constraints: 17552, agent episode reward: [-47.07808797619276], time: 48.322
steps: 2247543, episodes: 54000, mean episode reward: -47.461446103949505, num_cumulative_constraints: 17868, agent episode reward: [-47.461446103949505], time: 50.617
steps: 2288875, episodes: 55000, mean episode reward: -46.78117855706898, num_cumulative_constraints: 18125, agent episode reward: [-46.78117855706898], time: 46.942
steps: 2330299, episodes: 56000, mean episode reward: -47.09108903351854, num_cumulative_constraints: 18385, agent episode reward: [-47.09108903351854], time: 51.11
steps: 2371659, episodes: 57000, mean episode reward: -47.04946812281171, num_cumulative_constraints: 18645, agent episode reward: [-47.04946812281171], time: 46.892
steps: 2413248, episodes: 58000, mean episode reward: -47.29915715115979, num_cumulative_constraints: 18904, agent episode reward: [-47.29915715115979], time: 51.672
steps: 2455138, episodes: 59000, mean episode reward: -47.34431545162487, num_cumulative_constraints: 19278, agent episode reward: [-47.34431545162487], time: 47.19
steps: 2497195, episodes: 60000, mean episode reward: -47.57320688994578, num_cumulative_constraints: 19573, agent episode reward: [-47.57320688994578], time: 48.868
steps: 2539371, episodes: 61000, mean episode reward: -47.913410551114346, num_cumulative_constraints: 19957, agent episode reward: [-47.913410551114346], time: 50.258
steps: 2581370, episodes: 62000, mean episode reward: -47.63694664423957, num_cumulative_constraints: 20285, agent episode reward: [-47.63694664423957], time: 53.245
steps: 2623551, episodes: 63000, mean episode reward: -47.656606689169145, num_cumulative_constraints: 20598, agent episode reward: [-47.656606689169145], time: 47.108
steps: 2665836, episodes: 64000, mean episode reward: -47.677811476539574, num_cumulative_constraints: 20845, agent episode reward: [-47.677811476539574], time: 57.501
steps: 2708112, episodes: 65000, mean episode reward: -47.61374604991468, num_cumulative_constraints: 21126, agent episode reward: [-47.61374604991468], time: 50.659
steps: 2750054, episodes: 66000, mean episode reward: -47.400910680436894, num_cumulative_constraints: 21427, agent episode reward: [-47.400910680436894], time: 82.734
steps: 2792146, episodes: 67000, mean episode reward: -47.400967718123226, num_cumulative_constraints: 21728, agent episode reward: [-47.400967718123226], time: 58.094
steps: 2834059, episodes: 68000, mean episode reward: -47.38359243956262, num_cumulative_constraints: 22036, agent episode reward: [-47.38359243956262], time: 48.847
steps: 2876134, episodes: 69000, mean episode reward: -47.33728702524496, num_cumulative_constraints: 22339, agent episode reward: [-47.33728702524496], time: 47.8
steps: 2918527, episodes: 70000, mean episode reward: -47.171571047895114, num_cumulative_constraints: 22641, agent episode reward: [-47.171571047895114], time: 46.547
steps: 2961371, episodes: 71000, mean episode reward: -47.60912382643349, num_cumulative_constraints: 22977, agent episode reward: [-47.60912382643349], time: 44.723
steps: 3004112, episodes: 72000, mean episode reward: -47.462265699023064, num_cumulative_constraints: 23313, agent episode reward: [-47.462265699023064], time: 45.743
steps: 3047126, episodes: 73000, mean episode reward: -47.5053098301553, num_cumulative_constraints: 23571, agent episode reward: [-47.5053098301553], time: 46.698
steps: 3091035, episodes: 74000, mean episode reward: -48.15668133580557, num_cumulative_constraints: 23867, agent episode reward: [-48.15668133580557], time: 48.428
steps: 3135615, episodes: 75000, mean episode reward: -48.90294686034907, num_cumulative_constraints: 24279, agent episode reward: [-48.90294686034907], time: 48.48
steps: 3179656, episodes: 76000, mean episode reward: -48.729879828525775, num_cumulative_constraints: 24675, agent episode reward: [-48.729879828525775], time: 48.697
steps: 3222724, episodes: 77000, mean episode reward: -47.782343730987904, num_cumulative_constraints: 25021, agent episode reward: [-47.782343730987904], time: 44.807
steps: 3265040, episodes: 78000, mean episode reward: -47.5492992542959, num_cumulative_constraints: 25399, agent episode reward: [-47.5492992542959], time: 45.771
steps: 3307419, episodes: 79000, mean episode reward: -47.80659167971801, num_cumulative_constraints: 25775, agent episode reward: [-47.80659167971801], time: 48.266
steps: 3349232, episodes: 80000, mean episode reward: -47.16770250951929, num_cumulative_constraints: 26109, agent episode reward: [-47.16770250951929], time: 45.376
steps: 3390965, episodes: 81000, mean episode reward: -47.39205906365407, num_cumulative_constraints: 26428, agent episode reward: [-47.39205906365407], time: 49.29
steps: 3432650, episodes: 82000, mean episode reward: -47.168697503816, num_cumulative_constraints: 26777, agent episode reward: [-47.168697503816], time: 46.724
steps: 3474209, episodes: 83000, mean episode reward: -47.328257290650456, num_cumulative_constraints: 27106, agent episode reward: [-47.328257290650456], time: 46.616
steps: 3515534, episodes: 84000, mean episode reward: -47.10215096312704, num_cumulative_constraints: 27499, agent episode reward: [-47.10215096312704], time: 46.419
steps: 3557136, episodes: 85000, mean episode reward: -47.6417596916499, num_cumulative_constraints: 27883, agent episode reward: [-47.6417596916499], time: 44.793
steps: 3598834, episodes: 86000, mean episode reward: -47.57833488380155, num_cumulative_constraints: 28196, agent episode reward: [-47.57833488380155], time: 44.884
steps: 3640351, episodes: 87000, mean episode reward: -47.50674630422323, num_cumulative_constraints: 28637, agent episode reward: [-47.50674630422323], time: 46.024
steps: 3681674, episodes: 88000, mean episode reward: -47.09981747212917, num_cumulative_constraints: 28940, agent episode reward: [-47.09981747212917], time: 47.392
steps: 3722924, episodes: 89000, mean episode reward: -47.0900764241806, num_cumulative_constraints: 29238, agent episode reward: [-47.0900764241806], time: 49.55
steps: 3764367, episodes: 90000, mean episode reward: -47.26057804949918, num_cumulative_constraints: 29526, agent episode reward: [-47.26057804949918], time: 48.84
steps: 3805624, episodes: 91000, mean episode reward: -47.2365916272673, num_cumulative_constraints: 29905, agent episode reward: [-47.2365916272673], time: 44.638
steps: 3846751, episodes: 92000, mean episode reward: -47.20667715252931, num_cumulative_constraints: 30277, agent episode reward: [-47.20667715252931], time: 45.545
steps: 3887877, episodes: 93000, mean episode reward: -46.55778518760581, num_cumulative_constraints: 30512, agent episode reward: [-46.55778518760581], time: 45.897
steps: 3928923, episodes: 94000, mean episode reward: -47.05666950307208, num_cumulative_constraints: 30815, agent episode reward: [-47.05666950307208], time: 47.024
steps: 3970273, episodes: 95000, mean episode reward: -47.6224645477525, num_cumulative_constraints: 31207, agent episode reward: [-47.6224645477525], time: 49.295
steps: 4011703, episodes: 96000, mean episode reward: -47.414224096680925, num_cumulative_constraints: 31534, agent episode reward: [-47.414224096680925], time: 49.096
steps: 4053099, episodes: 97000, mean episode reward: -47.15575726906077, num_cumulative_constraints: 31880, agent episode reward: [-47.15575726906077], time: 46.197
steps: 4094452, episodes: 98000, mean episode reward: -47.40261964566864, num_cumulative_constraints: 32208, agent episode reward: [-47.40261964566864], time: 45.302
steps: 4135651, episodes: 99000, mean episode reward: -47.19248200889584, num_cumulative_constraints: 32507, agent episode reward: [-47.19248200889584], time: 45.927
steps: 4176940, episodes: 100000, mean episode reward: -46.949792524050785, num_cumulative_constraints: 32792, agent episode reward: [-46.949792524050785], time: 46.085
steps: 4218155, episodes: 101000, mean episode reward: -47.2528021988697, num_cumulative_constraints: 33155, agent episode reward: [-47.2528021988697], time: 46.34
steps: 4259447, episodes: 102000, mean episode reward: -47.34705215252658, num_cumulative_constraints: 33527, agent episode reward: [-47.34705215252658], time: 48.698
steps: 4300891, episodes: 103000, mean episode reward: -47.421159149607135, num_cumulative_constraints: 33925, agent episode reward: [-47.421159149607135], time: 48.338
steps: 4342113, episodes: 104000, mean episode reward: -47.205572494213826, num_cumulative_constraints: 34266, agent episode reward: [-47.205572494213826], time: 45.424
steps: 4383481, episodes: 105000, mean episode reward: -47.61010368803591, num_cumulative_constraints: 34691, agent episode reward: [-47.61010368803591], time: 45.873
steps: 4425076, episodes: 106000, mean episode reward: -47.51666292855391, num_cumulative_constraints: 35038, agent episode reward: [-47.51666292855391], time: 46.563
steps: 4466406, episodes: 107000, mean episode reward: -47.360849786911146, num_cumulative_constraints: 35437, agent episode reward: [-47.360849786911146], time: 46.89
steps: 4507976, episodes: 108000, mean episode reward: -47.78454818165394, num_cumulative_constraints: 35876, agent episode reward: [-47.78454818165394], time: 47.296
steps: 4549405, episodes: 109000, mean episode reward: -47.31069870952531, num_cumulative_constraints: 36263, agent episode reward: [-47.31069870952531], time: 48.503
steps: 4590905, episodes: 110000, mean episode reward: -47.40280891945973, num_cumulative_constraints: 36649, agent episode reward: [-47.40280891945973], time: 49.587
steps: 4632073, episodes: 111000, mean episode reward: -47.05581570099095, num_cumulative_constraints: 36997, agent episode reward: [-47.05581570099095], time: 47.104
steps: 4673381, episodes: 112000, mean episode reward: -47.637300775299806, num_cumulative_constraints: 37431, agent episode reward: [-47.637300775299806], time: 44.918
steps: 4714702, episodes: 113000, mean episode reward: -47.26544854307518, num_cumulative_constraints: 37790, agent episode reward: [-47.26544854307518], time: 45.796
steps: 4756035, episodes: 114000, mean episode reward: -47.23944520994542, num_cumulative_constraints: 38222, agent episode reward: [-47.23944520994542], time: 46.312
steps: 4797292, episodes: 115000, mean episode reward: -47.485964572450705, num_cumulative_constraints: 38673, agent episode reward: [-47.485964572450705], time: 50.08
steps: 4838579, episodes: 116000, mean episode reward: -47.303225548702464, num_cumulative_constraints: 39094, agent episode reward: [-47.303225548702464], time: 48.041
steps: 4879648, episodes: 117000, mean episode reward: -46.91058103293568, num_cumulative_constraints: 39391, agent episode reward: [-46.91058103293568], time: 47.757
steps: 4920890, episodes: 118000, mean episode reward: -47.12495500504663, num_cumulative_constraints: 39705, agent episode reward: [-47.12495500504663], time: 46.207
steps: 4962208, episodes: 119000, mean episode reward: -46.83214561900512, num_cumulative_constraints: 40038, agent episode reward: [-46.83214561900512], time: 47.506
steps: 5003347, episodes: 120000, mean episode reward: -47.11339799794716, num_cumulative_constraints: 40383, agent episode reward: [-47.11339799794716], time: 45.534
steps: 5044527, episodes: 121000, mean episode reward: -47.11313105720842, num_cumulative_constraints: 40756, agent episode reward: [-47.11313105720842], time: 46.089
steps: 5085570, episodes: 122000, mean episode reward: -47.11180026088262, num_cumulative_constraints: 41134, agent episode reward: [-47.11180026088262], time: 45.919
steps: 5126962, episodes: 123000, mean episode reward: -47.18312490972725, num_cumulative_constraints: 41457, agent episode reward: [-47.18312490972725], time: 48.06
steps: 5167918, episodes: 124000, mean episode reward: -46.78576161342222, num_cumulative_constraints: 41864, agent episode reward: [-46.78576161342222], time: 46.305
steps: 5209062, episodes: 125000, mean episode reward: -46.64439547570052, num_cumulative_constraints: 42208, agent episode reward: [-46.64439547570052], time: 47.602
steps: 5250457, episodes: 126000, mean episode reward: -46.772352940079955, num_cumulative_constraints: 42503, agent episode reward: [-46.772352940079955], time: 48.031
steps: 5291587, episodes: 127000, mean episode reward: -46.71697347941909, num_cumulative_constraints: 42827, agent episode reward: [-46.71697347941909], time: 44.654
steps: 5332928, episodes: 128000, mean episode reward: -47.016222848486485, num_cumulative_constraints: 43173, agent episode reward: [-47.016222848486485], time: 46.197
steps: 5374452, episodes: 129000, mean episode reward: -47.16754143522504, num_cumulative_constraints: 43469, agent episode reward: [-47.16754143522504], time: 46.425
steps: 5415856, episodes: 130000, mean episode reward: -47.27711444235618, num_cumulative_constraints: 43835, agent episode reward: [-47.27711444235618], time: 47.539
steps: 5457060, episodes: 131000, mean episode reward: -46.5845165988359, num_cumulative_constraints: 44130, agent episode reward: [-46.5845165988359], time: 46.075
steps: 5498490, episodes: 132000, mean episode reward: -47.01296814893163, num_cumulative_constraints: 44410, agent episode reward: [-47.01296814893163], time: 46.145
steps: 5539752, episodes: 133000, mean episode reward: -46.96794290898035, num_cumulative_constraints: 44776, agent episode reward: [-46.96794290898035], time: 49.107
steps: 5581039, episodes: 134000, mean episode reward: -46.848496445373605, num_cumulative_constraints: 45080, agent episode reward: [-46.848496445373605], time: 45.738
steps: 5622349, episodes: 135000, mean episode reward: -46.915555270101315, num_cumulative_constraints: 45418, agent episode reward: [-46.915555270101315], time: 46.158
steps: 5663597, episodes: 136000, mean episode reward: -46.636260071907365, num_cumulative_constraints: 45759, agent episode reward: [-46.636260071907365], time: 46.512
steps: 5705217, episodes: 137000, mean episode reward: -46.8163159273163, num_cumulative_constraints: 46064, agent episode reward: [-46.8163159273163], time: 47.605
steps: 5746297, episodes: 138000, mean episode reward: -46.65683622578156, num_cumulative_constraints: 46445, agent episode reward: [-46.65683622578156], time: 46.505
steps: 5787471, episodes: 139000, mean episode reward: -46.88901402295391, num_cumulative_constraints: 46830, agent episode reward: [-46.88901402295391], time: 47.565
steps: 5828862, episodes: 140000, mean episode reward: -46.77693501262121, num_cumulative_constraints: 47218, agent episode reward: [-46.77693501262121], time: 47.674
steps: 5870200, episodes: 141000, mean episode reward: -47.352970550350534, num_cumulative_constraints: 47651, agent episode reward: [-47.352970550350534], time: 46.232
steps: 5911231, episodes: 142000, mean episode reward: -46.66682711231474, num_cumulative_constraints: 48020, agent episode reward: [-46.66682711231474], time: 46.508
steps: 5952490, episodes: 143000, mean episode reward: -46.79305531995568, num_cumulative_constraints: 48348, agent episode reward: [-46.79305531995568], time: 46.13
steps: 5993751, episodes: 144000, mean episode reward: -47.038029484485264, num_cumulative_constraints: 48763, agent episode reward: [-47.038029484485264], time: 47.145
steps: 6034970, episodes: 145000, mean episode reward: -46.865224334081944, num_cumulative_constraints: 49124, agent episode reward: [-46.865224334081944], time: 47.002
steps: 6076360, episodes: 146000, mean episode reward: -46.92801411874046, num_cumulative_constraints: 49510, agent episode reward: [-46.92801411874046], time: 46.312
steps: 6118013, episodes: 147000, mean episode reward: -46.990708464888826, num_cumulative_constraints: 49817, agent episode reward: [-46.990708464888826], time: 47.874
steps: 6159086, episodes: 148000, mean episode reward: -46.56713024376775, num_cumulative_constraints: 50130, agent episode reward: [-46.56713024376775], time: 46.884
steps: 6200325, episodes: 149000, mean episode reward: -46.44263153768642, num_cumulative_constraints: 50322, agent episode reward: [-46.44263153768642], time: 48.483
steps: 6241547, episodes: 150000, mean episode reward: -46.86904791129962, num_cumulative_constraints: 50680, agent episode reward: [-46.86904791129962], time: 47.61
steps: 6282983, episodes: 151000, mean episode reward: -46.593576337619396, num_cumulative_constraints: 50911, agent episode reward: [-46.593576337619396], time: 46.122
steps: 6324301, episodes: 152000, mean episode reward: -46.81197031029769, num_cumulative_constraints: 51211, agent episode reward: [-46.81197031029769], time: 46.443
steps: 6365663, episodes: 153000, mean episode reward: -47.10195055017938, num_cumulative_constraints: 51596, agent episode reward: [-47.10195055017938], time: 46.888
steps: 6406881, episodes: 154000, mean episode reward: -46.50598470278518, num_cumulative_constraints: 51884, agent episode reward: [-46.50598470278518], time: 47.554
steps: 6448077, episodes: 155000, mean episode reward: -46.79814331703847, num_cumulative_constraints: 52208, agent episode reward: [-46.79814331703847], time: 46.974
steps: 6489270, episodes: 156000, mean episode reward: -47.01985224806736, num_cumulative_constraints: 52555, agent episode reward: [-47.01985224806736], time: 48.298
steps: 6530095, episodes: 157000, mean episode reward: -46.57339212389924, num_cumulative_constraints: 52825, agent episode reward: [-46.57339212389924], time: 45.919
steps: 6571526, episodes: 158000, mean episode reward: -47.52216123903307, num_cumulative_constraints: 53260, agent episode reward: [-47.52216123903307], time: 47.482
steps: 6612549, episodes: 159000, mean episode reward: -46.99667861460876, num_cumulative_constraints: 53651, agent episode reward: [-46.99667861460876], time: 46.003
steps: 6653903, episodes: 160000, mean episode reward: -47.23896223317564, num_cumulative_constraints: 54020, agent episode reward: [-47.23896223317564], time: 46.514
steps: 6695161, episodes: 161000, mean episode reward: -46.91363459332152, num_cumulative_constraints: 54376, agent episode reward: [-46.91363459332152], time: 45.888
steps: 6736113, episodes: 162000, mean episode reward: -46.94254087358389, num_cumulative_constraints: 54718, agent episode reward: [-46.94254087358389], time: 46.599
steps: 6777381, episodes: 163000, mean episode reward: -47.0184547996069, num_cumulative_constraints: 55047, agent episode reward: [-47.0184547996069], time: 46.812
steps: 6818572, episodes: 164000, mean episode reward: -47.12101456248463, num_cumulative_constraints: 55422, agent episode reward: [-47.12101456248463], time: 47.32
steps: 6859624, episodes: 165000, mean episode reward: -46.93058619957945, num_cumulative_constraints: 55767, agent episode reward: [-46.93058619957945], time: 49.723
steps: 6900714, episodes: 166000, mean episode reward: -46.596246292100915, num_cumulative_constraints: 56066, agent episode reward: [-46.596246292100915], time: 47.875
steps: 6941774, episodes: 167000, mean episode reward: -47.22100517296734, num_cumulative_constraints: 56527, agent episode reward: [-47.22100517296734], time: 46.638
steps: 6983078, episodes: 168000, mean episode reward: -46.89673982629631, num_cumulative_constraints: 56888, agent episode reward: [-46.89673982629631], time: 47.027
steps: 7024512, episodes: 169000, mean episode reward: -47.42947265599048, num_cumulative_constraints: 57273, agent episode reward: [-47.42947265599048], time: 46.407
steps: 7065935, episodes: 170000, mean episode reward: -47.32561574736511, num_cumulative_constraints: 57661, agent episode reward: [-47.32561574736511], time: 46.11
steps: 7107250, episodes: 171000, mean episode reward: -46.81200251066802, num_cumulative_constraints: 58044, agent episode reward: [-46.81200251066802], time: 47.446
steps: 7148740, episodes: 172000, mean episode reward: -47.02861530217331, num_cumulative_constraints: 58353, agent episode reward: [-47.02861530217331], time: 47.935
steps: 7189942, episodes: 173000, mean episode reward: -46.944056313848705, num_cumulative_constraints: 58689, agent episode reward: [-46.944056313848705], time: 47.349
steps: 7231446, episodes: 174000, mean episode reward: -47.17306542668989, num_cumulative_constraints: 59035, agent episode reward: [-47.17306542668989], time: 46.767
steps: 7273049, episodes: 175000, mean episode reward: -47.409216604298535, num_cumulative_constraints: 59422, agent episode reward: [-47.409216604298535], time: 47.863
steps: 7314596, episodes: 176000, mean episode reward: -47.01043139174898, num_cumulative_constraints: 59735, agent episode reward: [-47.01043139174898], time: 47.74
steps: 7356101, episodes: 177000, mean episode reward: -47.70189705982821, num_cumulative_constraints: 60130, agent episode reward: [-47.70189705982821], time: 48.555
steps: 7397936, episodes: 178000, mean episode reward: -47.63392334952956, num_cumulative_constraints: 60520, agent episode reward: [-47.63392334952956], time: 46.754
steps: 7439726, episodes: 179000, mean episode reward: -48.02519704703476, num_cumulative_constraints: 60969, agent episode reward: [-48.02519704703476], time: 47.08
steps: 7481998, episodes: 180000, mean episode reward: -47.976818085032676, num_cumulative_constraints: 61401, agent episode reward: [-47.976818085032676], time: 46.478
steps: 7523858, episodes: 181000, mean episode reward: -47.52362333008453, num_cumulative_constraints: 61760, agent episode reward: [-47.52362333008453], time: 48.151
steps: 7565805, episodes: 182000, mean episode reward: -47.373818604456886, num_cumulative_constraints: 62066, agent episode reward: [-47.373818604456886], time: 47.546
steps: 7607791, episodes: 183000, mean episode reward: -47.34045038967203, num_cumulative_constraints: 62365, agent episode reward: [-47.34045038967203], time: 47.564
steps: 7649372, episodes: 184000, mean episode reward: -47.27848416660149, num_cumulative_constraints: 62721, agent episode reward: [-47.27848416660149], time: 47.297
steps: 7691955, episodes: 185000, mean episode reward: -47.82204113860215, num_cumulative_constraints: 63013, agent episode reward: [-47.82204113860215], time: 49.025
steps: 7733438, episodes: 186000, mean episode reward: -46.761711460108735, num_cumulative_constraints: 63265, agent episode reward: [-46.761711460108735], time: 48.838
steps: 7774744, episodes: 187000, mean episode reward: -46.937539564568596, num_cumulative_constraints: 63552, agent episode reward: [-46.937539564568596], time: 45.85
steps: 7816675, episodes: 188000, mean episode reward: -47.25952291979662, num_cumulative_constraints: 63829, agent episode reward: [-47.25952291979662], time: 48.843
steps: 7858325, episodes: 189000, mean episode reward: -47.80498855350953, num_cumulative_constraints: 64274, agent episode reward: [-47.80498855350953], time: 47.805
steps: 7899847, episodes: 190000, mean episode reward: -47.526097623776856, num_cumulative_constraints: 64658, agent episode reward: [-47.526097623776856], time: 47.258
steps: 7941610, episodes: 191000, mean episode reward: -47.59096153487122, num_cumulative_constraints: 64948, agent episode reward: [-47.59096153487122], time: 46.839
steps: 7983276, episodes: 192000, mean episode reward: -47.72433196237753, num_cumulative_constraints: 65305, agent episode reward: [-47.72433196237753], time: 47.276
steps: 8024959, episodes: 193000, mean episode reward: -47.529822868608214, num_cumulative_constraints: 65621, agent episode reward: [-47.529822868608214], time: 50.622
steps: 8066414, episodes: 194000, mean episode reward: -47.1926328971285, num_cumulative_constraints: 65951, agent episode reward: [-47.1926328971285], time: 47.087
steps: 8108078, episodes: 195000, mean episode reward: -47.576851281003314, num_cumulative_constraints: 66200, agent episode reward: [-47.576851281003314], time: 50.635
steps: 8150139, episodes: 196000, mean episode reward: -48.127352501164964, num_cumulative_constraints: 66520, agent episode reward: [-48.127352501164964], time: 49.05
steps: 8192001, episodes: 197000, mean episode reward: -48.211161054206094, num_cumulative_constraints: 66886, agent episode reward: [-48.211161054206094], time: 47.635
steps: 8233733, episodes: 198000, mean episode reward: -48.04126263219516, num_cumulative_constraints: 67225, agent episode reward: [-48.04126263219516], time: 45.423
steps: 8275389, episodes: 199000, mean episode reward: -48.0036100535609, num_cumulative_constraints: 67557, agent episode reward: [-48.0036100535609], time: 46.523
steps: 8316791, episodes: 200000, mean episode reward: -47.140828474114144, num_cumulative_constraints: 67890, agent episode reward: [-47.140828474114144], time: 46.585
steps: 8358173, episodes: 201000, mean episode reward: -47.034888058203606, num_cumulative_constraints: 68127, agent episode reward: [-47.034888058203606], time: 46.668
steps: 8399761, episodes: 202000, mean episode reward: -47.30936016094881, num_cumulative_constraints: 68442, agent episode reward: [-47.30936016094881], time: 46.719
steps: 8440859, episodes: 203000, mean episode reward: -47.175503178972704, num_cumulative_constraints: 68799, agent episode reward: [-47.175503178972704], time: 49.367
steps: 8482111, episodes: 204000, mean episode reward: -47.148196586482925, num_cumulative_constraints: 69104, agent episode reward: [-47.148196586482925], time: 49.597
steps: 8523412, episodes: 205000, mean episode reward: -47.16622953083897, num_cumulative_constraints: 69490, agent episode reward: [-47.16622953083897], time: 46.339
steps: 8564889, episodes: 206000, mean episode reward: -47.27962407540716, num_cumulative_constraints: 69799, agent episode reward: [-47.27962407540716], time: 47.939
steps: 8606419, episodes: 207000, mean episode reward: -47.25515075406959, num_cumulative_constraints: 70104, agent episode reward: [-47.25515075406959], time: 46.374
steps: 8647812, episodes: 208000, mean episode reward: -47.11629235652593, num_cumulative_constraints: 70425, agent episode reward: [-47.11629235652593], time: 45.491
steps: 8689606, episodes: 209000, mean episode reward: -47.19594243167208, num_cumulative_constraints: 70728, agent episode reward: [-47.19594243167208], time: 47.317
steps: 8731271, episodes: 210000, mean episode reward: -47.56324122396657, num_cumulative_constraints: 71062, agent episode reward: [-47.56324122396657], time: 48.498
steps: 8772570, episodes: 211000, mean episode reward: -47.22014858039901, num_cumulative_constraints: 71413, agent episode reward: [-47.22014858039901], time: 47.216
steps: 8814322, episodes: 212000, mean episode reward: -47.681065956214795, num_cumulative_constraints: 71736, agent episode reward: [-47.681065956214795], time: 47.641
steps: 8855760, episodes: 213000, mean episode reward: -47.228000305733744, num_cumulative_constraints: 72026, agent episode reward: [-47.228000305733744], time: 47.075
steps: 8897152, episodes: 214000, mean episode reward: -47.47010942891617, num_cumulative_constraints: 72316, agent episode reward: [-47.47010942891617], time: 48.081
steps: 8938834, episodes: 215000, mean episode reward: -47.69753627875283, num_cumulative_constraints: 72631, agent episode reward: [-47.69753627875283], time: 46.765
steps: 8980363, episodes: 216000, mean episode reward: -47.36539271084701, num_cumulative_constraints: 72965, agent episode reward: [-47.36539271084701], time: 47.215
steps: 9021731, episodes: 217000, mean episode reward: -47.47350480416123, num_cumulative_constraints: 73283, agent episode reward: [-47.47350480416123], time: 47.008
steps: 9063375, episodes: 218000, mean episode reward: -47.67384759747602, num_cumulative_constraints: 73677, agent episode reward: [-47.67384759747602], time: 47.712
steps: 9104651, episodes: 219000, mean episode reward: -47.39280879750784, num_cumulative_constraints: 74002, agent episode reward: [-47.39280879750784], time: 47.89
steps: 9145895, episodes: 220000, mean episode reward: -47.32308466706585, num_cumulative_constraints: 74370, agent episode reward: [-47.32308466706585], time: 46.626
steps: 9187319, episodes: 221000, mean episode reward: -47.3464991354721, num_cumulative_constraints: 74695, agent episode reward: [-47.3464991354721], time: 46.757
steps: 9228582, episodes: 222000, mean episode reward: -47.01074491881179, num_cumulative_constraints: 74999, agent episode reward: [-47.01074491881179], time: 47.68
steps: 9270121, episodes: 223000, mean episode reward: -47.39981131526257, num_cumulative_constraints: 75331, agent episode reward: [-47.39981131526257], time: 49.839
steps: 9311416, episodes: 224000, mean episode reward: -47.148062880635564, num_cumulative_constraints: 75658, agent episode reward: [-47.148062880635564], time: 48.01
steps: 9352819, episodes: 225000, mean episode reward: -47.342980619750286, num_cumulative_constraints: 75947, agent episode reward: [-47.342980619750286], time: 48.63
steps: 9393971, episodes: 226000, mean episode reward: -47.243570398475775, num_cumulative_constraints: 76274, agent episode reward: [-47.243570398475775], time: 46.978
steps: 9435243, episodes: 227000, mean episode reward: -46.932805310388176, num_cumulative_constraints: 76519, agent episode reward: [-46.932805310388176], time: 47.604
steps: 9476518, episodes: 228000, mean episode reward: -47.66217324243147, num_cumulative_constraints: 76892, agent episode reward: [-47.66217324243147], time: 45.988
steps: 9517627, episodes: 229000, mean episode reward: -46.909875558427935, num_cumulative_constraints: 77130, agent episode reward: [-46.909875558427935], time: 47.273
steps: 9558959, episodes: 230000, mean episode reward: -47.626190234923, num_cumulative_constraints: 77481, agent episode reward: [-47.626190234923], time: 46.005
steps: 9600418, episodes: 231000, mean episode reward: -47.23592008117589, num_cumulative_constraints: 77781, agent episode reward: [-47.23592008117589], time: 47.356
steps: 9641362, episodes: 232000, mean episode reward: -46.985647067727975, num_cumulative_constraints: 78091, agent episode reward: [-46.985647067727975], time: 46.55
steps: 9682423, episodes: 233000, mean episode reward: -47.284143933425284, num_cumulative_constraints: 78413, agent episode reward: [-47.284143933425284], time: 48.899
steps: 9723666, episodes: 234000, mean episode reward: -47.89042193556538, num_cumulative_constraints: 78778, agent episode reward: [-47.89042193556538], time: 47.112
steps: 9765092, episodes: 235000, mean episode reward: -47.87766200727945, num_cumulative_constraints: 79155, agent episode reward: [-47.87766200727945], time: 46.901
steps: 9806338, episodes: 236000, mean episode reward: -47.42131588550113, num_cumulative_constraints: 79478, agent episode reward: [-47.42131588550113], time: 48.182
steps: 9847539, episodes: 237000, mean episode reward: -47.56721251746037, num_cumulative_constraints: 79779, agent episode reward: [-47.56721251746037], time: 47.65
steps: 9889075, episodes: 238000, mean episode reward: -47.77883648341375, num_cumulative_constraints: 80119, agent episode reward: [-47.77883648341375], time: 47.238
steps: 9931143, episodes: 239000, mean episode reward: -48.55273048661553, num_cumulative_constraints: 80607, agent episode reward: [-48.55273048661553], time: 48.824
steps: 9973094, episodes: 240000, mean episode reward: -47.59687803380598, num_cumulative_constraints: 80949, agent episode reward: [-47.59687803380598], time: 48.974
steps: 10015059, episodes: 241000, mean episode reward: -48.03182583497463, num_cumulative_constraints: 81273, agent episode reward: [-48.03182583497463], time: 47.504
steps: 10056773, episodes: 242000, mean episode reward: -47.52137771671853, num_cumulative_constraints: 81576, agent episode reward: [-47.52137771671853], time: 45.726
steps: 10098894, episodes: 243000, mean episode reward: -48.33661375986298, num_cumulative_constraints: 82040, agent episode reward: [-48.33661375986298], time: 47.68
steps: 10141124, episodes: 244000, mean episode reward: -48.10350733661598, num_cumulative_constraints: 82413, agent episode reward: [-48.10350733661598], time: 46.889
steps: 10183842, episodes: 245000, mean episode reward: -48.267594442107004, num_cumulative_constraints: 82783, agent episode reward: [-48.267594442107004], time: 48.473
steps: 10225754, episodes: 246000, mean episode reward: -47.95543181425384, num_cumulative_constraints: 83200, agent episode reward: [-47.95543181425384], time: 47.073
steps: 10267855, episodes: 247000, mean episode reward: -48.002885568222275, num_cumulative_constraints: 83580, agent episode reward: [-48.002885568222275], time: 49.838
steps: 10309795, episodes: 248000, mean episode reward: -47.66936073943411, num_cumulative_constraints: 83994, agent episode reward: [-47.66936073943411], time: 49.178
steps: 10351443, episodes: 249000, mean episode reward: -47.689561630623125, num_cumulative_constraints: 84364, agent episode reward: [-47.689561630623125], time: 47.971
steps: 10392959, episodes: 250000, mean episode reward: -47.405697971155035, num_cumulative_constraints: 84694, agent episode reward: [-47.405697971155035], time: 47.754
steps: 10435049, episodes: 251000, mean episode reward: -48.071971696025415, num_cumulative_constraints: 85014, agent episode reward: [-48.071971696025415], time: 46.289
steps: 10477223, episodes: 252000, mean episode reward: -47.88904216577965, num_cumulative_constraints: 85370, agent episode reward: [-47.88904216577965], time: 47.102
steps: 10519480, episodes: 253000, mean episode reward: -48.18345206965102, num_cumulative_constraints: 85775, agent episode reward: [-48.18345206965102], time: 46.29
steps: 10561718, episodes: 254000, mean episode reward: -48.00682968216985, num_cumulative_constraints: 86191, agent episode reward: [-48.00682968216985], time: 47.775
steps: 10603710, episodes: 255000, mean episode reward: -47.9658883347425, num_cumulative_constraints: 86538, agent episode reward: [-47.9658883347425], time: 47.521
steps: 10645518, episodes: 256000, mean episode reward: -47.64259709029159, num_cumulative_constraints: 86801, agent episode reward: [-47.64259709029159], time: 46.485
steps: 10686905, episodes: 257000, mean episode reward: -47.54269404608363, num_cumulative_constraints: 87161, agent episode reward: [-47.54269404608363], time: 48.475
steps: 10728820, episodes: 258000, mean episode reward: -47.57887717586357, num_cumulative_constraints: 87499, agent episode reward: [-47.57887717586357], time: 48.955
steps: 10770632, episodes: 259000, mean episode reward: -47.79016333661535, num_cumulative_constraints: 87869, agent episode reward: [-47.79016333661535], time: 46.575
steps: 10812492, episodes: 260000, mean episode reward: -47.187724713961266, num_cumulative_constraints: 88084, agent episode reward: [-47.187724713961266], time: 47.806
steps: 10854304, episodes: 261000, mean episode reward: -47.6481268247524, num_cumulative_constraints: 88393, agent episode reward: [-47.6481268247524], time: 47.627
steps: 10896061, episodes: 262000, mean episode reward: -48.021047257119626, num_cumulative_constraints: 88803, agent episode reward: [-48.021047257119626], time: 45.794
steps: 10937989, episodes: 263000, mean episode reward: -48.061411802718226, num_cumulative_constraints: 89148, agent episode reward: [-48.061411802718226], time: 48.002
steps: 10979751, episodes: 264000, mean episode reward: -47.18910640380796, num_cumulative_constraints: 89389, agent episode reward: [-47.18910640380796], time: 46.064
steps: 11021882, episodes: 265000, mean episode reward: -48.226268819680364, num_cumulative_constraints: 89780, agent episode reward: [-48.226268819680364], time: 48.041
steps: 11064876, episodes: 266000, mean episode reward: -48.444356607268084, num_cumulative_constraints: 90115, agent episode reward: [-48.444356607268084], time: 49.186
steps: 11107639, episodes: 267000, mean episode reward: -47.85724072112783, num_cumulative_constraints: 90417, agent episode reward: [-47.85724072112783], time: 50.417
steps: 11150105, episodes: 268000, mean episode reward: -48.33699608536397, num_cumulative_constraints: 90752, agent episode reward: [-48.33699608536397], time: 50.048
steps: 11193024, episodes: 269000, mean episode reward: -48.811803582403165, num_cumulative_constraints: 91126, agent episode reward: [-48.811803582403165], time: 49.788
steps: 11235352, episodes: 270000, mean episode reward: -48.348876036960675, num_cumulative_constraints: 91473, agent episode reward: [-48.348876036960675], time: 47.646
steps: 11276418, episodes: 271000, mean episode reward: -47.402551653948976, num_cumulative_constraints: 91886, agent episode reward: [-47.402551653948976], time: 46.213
steps: 11317669, episodes: 272000, mean episode reward: -47.82296245369658, num_cumulative_constraints: 92231, agent episode reward: [-47.82296245369658], time: 48.119
steps: 11359187, episodes: 273000, mean episode reward: -47.55061160128402, num_cumulative_constraints: 92564, agent episode reward: [-47.55061160128402], time: 47.301
steps: 11400716, episodes: 274000, mean episode reward: -47.42897345270055, num_cumulative_constraints: 92868, agent episode reward: [-47.42897345270055], time: 50.06
steps: 11442074, episodes: 275000, mean episode reward: -47.81832095155667, num_cumulative_constraints: 93249, agent episode reward: [-47.81832095155667], time: 46.962
steps: 11483657, episodes: 276000, mean episode reward: -47.732550068627766, num_cumulative_constraints: 93593, agent episode reward: [-47.732550068627766], time: 49.112
steps: 11525657, episodes: 277000, mean episode reward: -48.06643091023369, num_cumulative_constraints: 93944, agent episode reward: [-48.06643091023369], time: 49.31
steps: 11567596, episodes: 278000, mean episode reward: -48.302221246529996, num_cumulative_constraints: 94362, agent episode reward: [-48.302221246529996], time: 47.689
steps: 11609409, episodes: 279000, mean episode reward: -47.70880603564223, num_cumulative_constraints: 94654, agent episode reward: [-47.70880603564223], time: 46.033
steps: 11650908, episodes: 280000, mean episode reward: -47.482943280279464, num_cumulative_constraints: 94930, agent episode reward: [-47.482943280279464], time: 46.952
steps: 11692891, episodes: 281000, mean episode reward: -47.59832512654182, num_cumulative_constraints: 95255, agent episode reward: [-47.59832512654182], time: 47.332
steps: 11734914, episodes: 282000, mean episode reward: -47.32422777108033, num_cumulative_constraints: 95494, agent episode reward: [-47.32422777108033], time: 66.452
steps: 11776886, episodes: 283000, mean episode reward: -47.875018318282926, num_cumulative_constraints: 95857, agent episode reward: [-47.875018318282926], time: 97.445
steps: 11818846, episodes: 284000, mean episode reward: -47.52478438052023, num_cumulative_constraints: 96073, agent episode reward: [-47.52478438052023], time: 52.835
steps: 11860385, episodes: 285000, mean episode reward: -47.50166701361694, num_cumulative_constraints: 96447, agent episode reward: [-47.50166701361694], time: 58.204
steps: 11901937, episodes: 286000, mean episode reward: -46.996554664082154, num_cumulative_constraints: 96708, agent episode reward: [-46.996554664082154], time: 52.608
steps: 11943469, episodes: 287000, mean episode reward: -47.1548566189685, num_cumulative_constraints: 96918, agent episode reward: [-47.1548566189685], time: 52.611
steps: 11985358, episodes: 288000, mean episode reward: -47.58522415764684, num_cumulative_constraints: 97190, agent episode reward: [-47.58522415764684], time: 57.783
steps: 12027002, episodes: 289000, mean episode reward: -47.6860062164555, num_cumulative_constraints: 97530, agent episode reward: [-47.6860062164555], time: 54.8
steps: 12068723, episodes: 290000, mean episode reward: -47.6505067295967, num_cumulative_constraints: 97889, agent episode reward: [-47.6505067295967], time: 55.871
steps: 12110512, episodes: 291000, mean episode reward: -47.17308734733195, num_cumulative_constraints: 98160, agent episode reward: [-47.17308734733195], time: 48.96
steps: 12152455, episodes: 292000, mean episode reward: -47.30191036521948, num_cumulative_constraints: 98483, agent episode reward: [-47.30191036521948], time: 48.247
steps: 12194223, episodes: 293000, mean episode reward: -46.968778566024206, num_cumulative_constraints: 98750, agent episode reward: [-46.968778566024206], time: 50.38
steps: 12236049, episodes: 294000, mean episode reward: -47.109655169849766, num_cumulative_constraints: 99052, agent episode reward: [-47.109655169849766], time: 52.741
steps: 12278327, episodes: 295000, mean episode reward: -47.45897975677647, num_cumulative_constraints: 99416, agent episode reward: [-47.45897975677647], time: 45.913
steps: 12320828, episodes: 296000, mean episode reward: -48.12497121347324, num_cumulative_constraints: 99868, agent episode reward: [-48.12497121347324], time: 48.713
steps: 12362931, episodes: 297000, mean episode reward: -47.28355879389303, num_cumulative_constraints: 100226, agent episode reward: [-47.28355879389303], time: 48.367
steps: 12404881, episodes: 298000, mean episode reward: -47.18171433482836, num_cumulative_constraints: 100570, agent episode reward: [-47.18171433482836], time: 50.398
steps: 12446642, episodes: 299000, mean episode reward: -47.192865207428575, num_cumulative_constraints: 100909, agent episode reward: [-47.192865207428575], time: 47.346
steps: 12488188, episodes: 300000, mean episode reward: -46.63914714830545, num_cumulative_constraints: 101171, agent episode reward: [-46.63914714830545], time: 48.051
steps: 12529980, episodes: 301000, mean episode reward: -46.916415949851626, num_cumulative_constraints: 101415, agent episode reward: [-46.916415949851626], time: 49.403
steps: 12571428, episodes: 302000, mean episode reward: -46.37035327123757, num_cumulative_constraints: 101624, agent episode reward: [-46.37035327123757], time: 45.206
steps: 12612913, episodes: 303000, mean episode reward: -46.96358017328842, num_cumulative_constraints: 101907, agent episode reward: [-46.96358017328842], time: 47.217
steps: 12654595, episodes: 304000, mean episode reward: -46.9037869828735, num_cumulative_constraints: 102232, agent episode reward: [-46.9037869828735], time: 48.314
steps: 12696336, episodes: 305000, mean episode reward: -46.90938184694065, num_cumulative_constraints: 102492, agent episode reward: [-46.90938184694065], time: 46.469
steps: 12738044, episodes: 306000, mean episode reward: -47.17728049203196, num_cumulative_constraints: 102810, agent episode reward: [-47.17728049203196], time: 49.66
steps: 12779977, episodes: 307000, mean episode reward: -47.2322049193373, num_cumulative_constraints: 103104, agent episode reward: [-47.2322049193373], time: 47.659
steps: 12821704, episodes: 308000, mean episode reward: -47.13726762872321, num_cumulative_constraints: 103400, agent episode reward: [-47.13726762872321], time: 45.365
steps: 12863302, episodes: 309000, mean episode reward: -46.72379961981492, num_cumulative_constraints: 103678, agent episode reward: [-46.72379961981492], time: 41.214
steps: 12905069, episodes: 310000, mean episode reward: -47.04952616788636, num_cumulative_constraints: 103937, agent episode reward: [-47.04952616788636], time: 43.929
steps: 12946838, episodes: 311000, mean episode reward: -47.18061960518268, num_cumulative_constraints: 104214, agent episode reward: [-47.18061960518268], time: 47.758
steps: 12988772, episodes: 312000, mean episode reward: -47.085839918383016, num_cumulative_constraints: 104466, agent episode reward: [-47.085839918383016], time: 48.876
steps: 13030704, episodes: 313000, mean episode reward: -47.13910799677249, num_cumulative_constraints: 104718, agent episode reward: [-47.13910799677249], time: 45.912
steps: 13072492, episodes: 314000, mean episode reward: -46.97146734515687, num_cumulative_constraints: 104996, agent episode reward: [-46.97146734515687], time: 46.691
steps: 13114554, episodes: 315000, mean episode reward: -47.14437001413891, num_cumulative_constraints: 105253, agent episode reward: [-47.14437001413891], time: 46.225
steps: 13156401, episodes: 316000, mean episode reward: -47.04110094324981, num_cumulative_constraints: 105527, agent episode reward: [-47.04110094324981], time: 47.897
steps: 13198378, episodes: 317000, mean episode reward: -47.15777694226378, num_cumulative_constraints: 105846, agent episode reward: [-47.15777694226378], time: 49.719
steps: 13240225, episodes: 318000, mean episode reward: -46.865100113653476, num_cumulative_constraints: 106092, agent episode reward: [-46.865100113653476], time: 45.128
steps: 13282359, episodes: 319000, mean episode reward: -47.17600371200463, num_cumulative_constraints: 106374, agent episode reward: [-47.17600371200463], time: 48.117
steps: 13324267, episodes: 320000, mean episode reward: -47.093063457846014, num_cumulative_constraints: 106627, agent episode reward: [-47.093063457846014], time: 49.918
steps: 13366089, episodes: 321000, mean episode reward: -46.82680781047582, num_cumulative_constraints: 106901, agent episode reward: [-46.82680781047582], time: 47.37
steps: 13408106, episodes: 322000, mean episode reward: -47.02298216733662, num_cumulative_constraints: 107198, agent episode reward: [-47.02298216733662], time: 47.066
steps: 13449937, episodes: 323000, mean episode reward: -46.720811425851416, num_cumulative_constraints: 107432, agent episode reward: [-46.720811425851416], time: 48.531
steps: 13491563, episodes: 324000, mean episode reward: -46.68967149945834, num_cumulative_constraints: 107735, agent episode reward: [-46.68967149945834], time: 40.911
steps: 13533214, episodes: 325000, mean episode reward: -46.95879645631841, num_cumulative_constraints: 108052, agent episode reward: [-46.95879645631841], time: 40.738
steps: 13574904, episodes: 326000, mean episode reward: -47.02340284372527, num_cumulative_constraints: 108355, agent episode reward: [-47.02340284372527], time: 38.449
steps: 13616604, episodes: 327000, mean episode reward: -47.09308845981813, num_cumulative_constraints: 108643, agent episode reward: [-47.09308845981813], time: 40.413
steps: 13658720, episodes: 328000, mean episode reward: -47.46283536956266, num_cumulative_constraints: 108965, agent episode reward: [-47.46283536956266], time: 39.118
steps: 13700545, episodes: 329000, mean episode reward: -46.85584200608598, num_cumulative_constraints: 109248, agent episode reward: [-46.85584200608598], time: 39.047
steps: 13742199, episodes: 330000, mean episode reward: -47.127674133506225, num_cumulative_constraints: 109588, agent episode reward: [-47.127674133506225], time: 38.758
steps: 13783705, episodes: 331000, mean episode reward: -46.75093811922641, num_cumulative_constraints: 109863, agent episode reward: [-46.75093811922641], time: 39.775
steps: 13825223, episodes: 332000, mean episode reward: -46.918068349784335, num_cumulative_constraints: 110141, agent episode reward: [-46.918068349784335], time: 39.401
steps: 13866813, episodes: 333000, mean episode reward: -47.22534334812772, num_cumulative_constraints: 110447, agent episode reward: [-47.22534334812772], time: 39.61
steps: 13908593, episodes: 334000, mean episode reward: -47.38890227177883, num_cumulative_constraints: 110751, agent episode reward: [-47.38890227177883], time: 39.857
steps: 13950087, episodes: 335000, mean episode reward: -46.75438695085491, num_cumulative_constraints: 111043, agent episode reward: [-46.75438695085491], time: 38.895
steps: 13991740, episodes: 336000, mean episode reward: -46.89102635207206, num_cumulative_constraints: 111307, agent episode reward: [-46.89102635207206], time: 38.713
steps: 14033395, episodes: 337000, mean episode reward: -47.28286737613893, num_cumulative_constraints: 111630, agent episode reward: [-47.28286737613893], time: 40.381
steps: 14074865, episodes: 338000, mean episode reward: -46.736871775869, num_cumulative_constraints: 111909, agent episode reward: [-46.736871775869], time: 38.924
steps: 14116555, episodes: 339000, mean episode reward: -46.928352165682035, num_cumulative_constraints: 112164, agent episode reward: [-46.928352165682035], time: 39.642
steps: 14158262, episodes: 340000, mean episode reward: -46.91103132528427, num_cumulative_constraints: 112432, agent episode reward: [-46.91103132528427], time: 39.067
steps: 14199416, episodes: 341000, mean episode reward: -46.301848498050326, num_cumulative_constraints: 112707, agent episode reward: [-46.301848498050326], time: 39.306
steps: 14240946, episodes: 342000, mean episode reward: -46.66737245805658, num_cumulative_constraints: 112937, agent episode reward: [-46.66737245805658], time: 39.58
steps: 14282863, episodes: 343000, mean episode reward: -47.223277083840195, num_cumulative_constraints: 113234, agent episode reward: [-47.223277083840195], time: 39.099
steps: 14324362, episodes: 344000, mean episode reward: -46.81725857873781, num_cumulative_constraints: 113491, agent episode reward: [-46.81725857873781], time: 41.052
steps: 14366090, episodes: 345000, mean episode reward: -46.87059062640467, num_cumulative_constraints: 113735, agent episode reward: [-46.87059062640467], time: 40.129
steps: 14407875, episodes: 346000, mean episode reward: -46.98750012956919, num_cumulative_constraints: 113985, agent episode reward: [-46.98750012956919], time: 40.452
steps: 14449360, episodes: 347000, mean episode reward: -46.68879008969165, num_cumulative_constraints: 114232, agent episode reward: [-46.68879008969165], time: 38.909
steps: 14491145, episodes: 348000, mean episode reward: -47.330895326859206, num_cumulative_constraints: 114519, agent episode reward: [-47.330895326859206], time: 39.047
steps: 14532819, episodes: 349000, mean episode reward: -47.02535881245046, num_cumulative_constraints: 114801, agent episode reward: [-47.02535881245046], time: 40.028
steps: 14574659, episodes: 350000, mean episode reward: -46.744150235861746, num_cumulative_constraints: 115004, agent episode reward: [-46.744150235861746], time: 39.223
steps: 14616443, episodes: 351000, mean episode reward: -46.730238370918656, num_cumulative_constraints: 115204, agent episode reward: [-46.730238370918656], time: 40.881
steps: 14658699, episodes: 352000, mean episode reward: -47.19911975045664, num_cumulative_constraints: 115447, agent episode reward: [-47.19911975045664], time: 39.324
steps: 14700969, episodes: 353000, mean episode reward: -47.022730672841696, num_cumulative_constraints: 115647, agent episode reward: [-47.022730672841696], time: 40.67
steps: 14743174, episodes: 354000, mean episode reward: -47.30701379845933, num_cumulative_constraints: 115886, agent episode reward: [-47.30701379845933], time: 41.549
steps: 14785459, episodes: 355000, mean episode reward: -47.42852411975569, num_cumulative_constraints: 116184, agent episode reward: [-47.42852411975569], time: 39.237
steps: 14827293, episodes: 356000, mean episode reward: -46.869881088058115, num_cumulative_constraints: 116431, agent episode reward: [-46.869881088058115], time: 39.717
steps: 14869582, episodes: 357000, mean episode reward: -47.076774807602, num_cumulative_constraints: 116656, agent episode reward: [-47.076774807602], time: 40.442
steps: 14911449, episodes: 358000, mean episode reward: -47.05525956859246, num_cumulative_constraints: 116905, agent episode reward: [-47.05525956859246], time: 41.69
steps: 14953733, episodes: 359000, mean episode reward: -47.346486257794986, num_cumulative_constraints: 117184, agent episode reward: [-47.346486257794986], time: 40.025
steps: 14995757, episodes: 360000, mean episode reward: -47.27350806515993, num_cumulative_constraints: 117518, agent episode reward: [-47.27350806515993], time: 40.683
steps: 15038109, episodes: 361000, mean episode reward: -47.37616271644332, num_cumulative_constraints: 117770, agent episode reward: [-47.37616271644332], time: 38.916
steps: 15080337, episodes: 362000, mean episode reward: -46.91912530963741, num_cumulative_constraints: 117967, agent episode reward: [-46.91912530963741], time: 39.652
steps: 15122361, episodes: 363000, mean episode reward: -47.09375778347257, num_cumulative_constraints: 118214, agent episode reward: [-47.09375778347257], time: 39.669
steps: 15164486, episodes: 364000, mean episode reward: -46.8544913176813, num_cumulative_constraints: 118453, agent episode reward: [-46.8544913176813], time: 39.501
steps: 15206328, episodes: 365000, mean episode reward: -47.03995825617468, num_cumulative_constraints: 118696, agent episode reward: [-47.03995825617468], time: 39.584
steps: 15248134, episodes: 366000, mean episode reward: -47.251699860542146, num_cumulative_constraints: 118941, agent episode reward: [-47.251699860542146], time: 39.012
steps: 15289718, episodes: 367000, mean episode reward: -46.880917182455896, num_cumulative_constraints: 119174, agent episode reward: [-46.880917182455896], time: 41.684
steps: 15331365, episodes: 368000, mean episode reward: -47.01207923212102, num_cumulative_constraints: 119431, agent episode reward: [-47.01207923212102], time: 38.39
steps: 15373195, episodes: 369000, mean episode reward: -47.05396461534711, num_cumulative_constraints: 119682, agent episode reward: [-47.05396461534711], time: 40.713
steps: 15415012, episodes: 370000, mean episode reward: -46.940165095010855, num_cumulative_constraints: 119924, agent episode reward: [-46.940165095010855], time: 40.121
steps: 15456679, episodes: 371000, mean episode reward: -47.248274951226115, num_cumulative_constraints: 120269, agent episode reward: [-47.248274951226115], time: 39.942
steps: 15498224, episodes: 372000, mean episode reward: -46.83938887267161, num_cumulative_constraints: 120505, agent episode reward: [-46.83938887267161], time: 38.544
steps: 15539853, episodes: 373000, mean episode reward: -46.87843670294991, num_cumulative_constraints: 120727, agent episode reward: [-46.87843670294991], time: 38.158
steps: 15581457, episodes: 374000, mean episode reward: -47.167093039357084, num_cumulative_constraints: 120977, agent episode reward: [-47.167093039357084], time: 39.496
steps: 15623034, episodes: 375000, mean episode reward: -46.83857688672718, num_cumulative_constraints: 121206, agent episode reward: [-46.83857688672718], time: 40.429
steps: 15664967, episodes: 376000, mean episode reward: -47.489495310750804, num_cumulative_constraints: 121512, agent episode reward: [-47.489495310750804], time: 39.319
steps: 15706725, episodes: 377000, mean episode reward: -47.32293597674997, num_cumulative_constraints: 121757, agent episode reward: [-47.32293597674997], time: 40.349
steps: 15748417, episodes: 378000, mean episode reward: -47.01665243016111, num_cumulative_constraints: 121977, agent episode reward: [-47.01665243016111], time: 38.813
steps: 15790091, episodes: 379000, mean episode reward: -46.94662168208869, num_cumulative_constraints: 122218, agent episode reward: [-46.94662168208869], time: 40.994
steps: 15831646, episodes: 380000, mean episode reward: -46.8674431183186, num_cumulative_constraints: 122496, agent episode reward: [-46.8674431183186], time: 38.838
steps: 15873355, episodes: 381000, mean episode reward: -47.08434576799539, num_cumulative_constraints: 122792, agent episode reward: [-47.08434576799539], time: 38.859
steps: 15915133, episodes: 382000, mean episode reward: -47.164426048209194, num_cumulative_constraints: 123080, agent episode reward: [-47.164426048209194], time: 40.358
steps: 15956948, episodes: 383000, mean episode reward: -46.587255068549936, num_cumulative_constraints: 123261, agent episode reward: [-46.587255068549936], time: 39.237
steps: 15998665, episodes: 384000, mean episode reward: -47.15072275061336, num_cumulative_constraints: 123576, agent episode reward: [-47.15072275061336], time: 39.866
steps: 16040356, episodes: 385000, mean episode reward: -46.61274114845458, num_cumulative_constraints: 123843, agent episode reward: [-46.61274114845458], time: 40.671
steps: 16082156, episodes: 386000, mean episode reward: -46.57706821419765, num_cumulative_constraints: 124086, agent episode reward: [-46.57706821419765], time: 39.527
steps: 16124080, episodes: 387000, mean episode reward: -47.17454433059289, num_cumulative_constraints: 124331, agent episode reward: [-47.17454433059289], time: 38.802
steps: 16165897, episodes: 388000, mean episode reward: -47.11395307084518, num_cumulative_constraints: 124621, agent episode reward: [-47.11395307084518], time: 38.503
steps: 16207619, episodes: 389000, mean episode reward: -46.73508975446751, num_cumulative_constraints: 124905, agent episode reward: [-46.73508975446751], time: 40.214
steps: 16249297, episodes: 390000, mean episode reward: -46.692237287377836, num_cumulative_constraints: 125135, agent episode reward: [-46.692237287377836], time: 38.456
steps: 16291036, episodes: 391000, mean episode reward: -46.716441796946185, num_cumulative_constraints: 125452, agent episode reward: [-46.716441796946185], time: 39.419
steps: 16332845, episodes: 392000, mean episode reward: -46.7355790016665, num_cumulative_constraints: 125654, agent episode reward: [-46.7355790016665], time: 41.362
steps: 16374561, episodes: 393000, mean episode reward: -46.554429314661824, num_cumulative_constraints: 125904, agent episode reward: [-46.554429314661824], time: 39.405
steps: 16416542, episodes: 394000, mean episode reward: -47.14405955105342, num_cumulative_constraints: 126188, agent episode reward: [-47.14405955105342], time: 39.795
steps: 16458507, episodes: 395000, mean episode reward: -46.88407164972588, num_cumulative_constraints: 126483, agent episode reward: [-46.88407164972588], time: 39.948
steps: 16500444, episodes: 396000, mean episode reward: -47.045564688775016, num_cumulative_constraints: 126746, agent episode reward: [-47.045564688775016], time: 39.098
steps: 16542537, episodes: 397000, mean episode reward: -46.71522588943796, num_cumulative_constraints: 127033, agent episode reward: [-46.71522588943796], time: 41.799
steps: 16584599, episodes: 398000, mean episode reward: -47.03568349976107, num_cumulative_constraints: 127347, agent episode reward: [-47.03568349976107], time: 40.844
steps: 16626433, episodes: 399000, mean episode reward: -46.83604629365368, num_cumulative_constraints: 127598, agent episode reward: [-46.83604629365368], time: 40.254
steps: 16668277, episodes: 400000, mean episode reward: -46.838997766224566, num_cumulative_constraints: 127905, agent episode reward: [-46.838997766224566], time: 39.911
steps: 16710399, episodes: 401000, mean episode reward: -46.83494334518575, num_cumulative_constraints: 128164, agent episode reward: [-46.83494334518575], time: 39.332
steps: 16752543, episodes: 402000, mean episode reward: -47.0764601393669, num_cumulative_constraints: 128466, agent episode reward: [-47.0764601393669], time: 39.76
steps: 16794567, episodes: 403000, mean episode reward: -47.00070284834083, num_cumulative_constraints: 128755, agent episode reward: [-47.00070284834083], time: 39.376
steps: 16836996, episodes: 404000, mean episode reward: -46.78579981674333, num_cumulative_constraints: 129031, agent episode reward: [-46.78579981674333], time: 38.696
steps: 16879414, episodes: 405000, mean episode reward: -46.923866439671, num_cumulative_constraints: 129284, agent episode reward: [-46.923866439671], time: 39.589
steps: 16921633, episodes: 406000, mean episode reward: -47.15431298964143, num_cumulative_constraints: 129543, agent episode reward: [-47.15431298964143], time: 41.559
steps: 16963673, episodes: 407000, mean episode reward: -46.91559899592187, num_cumulative_constraints: 129810, agent episode reward: [-46.91559899592187], time: 40.193
steps: 17005596, episodes: 408000, mean episode reward: -46.65426976769466, num_cumulative_constraints: 130070, agent episode reward: [-46.65426976769466], time: 40.138
steps: 17047473, episodes: 409000, mean episode reward: -46.84188568440838, num_cumulative_constraints: 130325, agent episode reward: [-46.84188568440838], time: 39.757
steps: 17089368, episodes: 410000, mean episode reward: -46.799708346732366, num_cumulative_constraints: 130633, agent episode reward: [-46.799708346732366], time: 39.183
steps: 17131040, episodes: 411000, mean episode reward: -46.59236681022727, num_cumulative_constraints: 130921, agent episode reward: [-46.59236681022727], time: 39.185
steps: 17172940, episodes: 412000, mean episode reward: -46.723991137737414, num_cumulative_constraints: 131170, agent episode reward: [-46.723991137737414], time: 39.463
steps: 17215155, episodes: 413000, mean episode reward: -47.17410350010512, num_cumulative_constraints: 131469, agent episode reward: [-47.17410350010512], time: 42.273
steps: 17256914, episodes: 414000, mean episode reward: -46.773184231794865, num_cumulative_constraints: 131725, agent episode reward: [-46.773184231794865], time: 38.587
steps: 17298675, episodes: 415000, mean episode reward: -46.89561461330914, num_cumulative_constraints: 132089, agent episode reward: [-46.89561461330914], time: 39.65
steps: 17340562, episodes: 416000, mean episode reward: -46.92245141838269, num_cumulative_constraints: 132376, agent episode reward: [-46.92245141838269], time: 39.248
steps: 17382727, episodes: 417000, mean episode reward: -47.07525493601296, num_cumulative_constraints: 132631, agent episode reward: [-47.07525493601296], time: 40.176
steps: 17424468, episodes: 418000, mean episode reward: -46.80608951649304, num_cumulative_constraints: 132862, agent episode reward: [-46.80608951649304], time: 39.612
steps: 17466393, episodes: 419000, mean episode reward: -47.113149394261775, num_cumulative_constraints: 133206, agent episode reward: [-47.113149394261775], time: 39.308
steps: 17508265, episodes: 420000, mean episode reward: -46.779545818405964, num_cumulative_constraints: 133508, agent episode reward: [-46.779545818405964], time: 41.374
steps: 17550186, episodes: 421000, mean episode reward: -46.70764827965973, num_cumulative_constraints: 133763, agent episode reward: [-46.70764827965973], time: 40.173
steps: 17592008, episodes: 422000, mean episode reward: -46.82899219133719, num_cumulative_constraints: 134061, agent episode reward: [-46.82899219133719], time: 39.582
steps: 17634296, episodes: 423000, mean episode reward: -46.83085583351514, num_cumulative_constraints: 134344, agent episode reward: [-46.83085583351514], time: 38.991
steps: 17676368, episodes: 424000, mean episode reward: -46.79638165831766, num_cumulative_constraints: 134664, agent episode reward: [-46.79638165831766], time: 40.497
steps: 17718366, episodes: 425000, mean episode reward: -46.892825154844395, num_cumulative_constraints: 134946, agent episode reward: [-46.892825154844395], time: 40.167
steps: 17760221, episodes: 426000, mean episode reward: -46.52713274329333, num_cumulative_constraints: 135174, agent episode reward: [-46.52713274329333], time: 40.695
steps: 17802198, episodes: 427000, mean episode reward: -46.56398958639974, num_cumulative_constraints: 135428, agent episode reward: [-46.56398958639974], time: 40.846
steps: 17844264, episodes: 428000, mean episode reward: -46.74184371283031, num_cumulative_constraints: 135735, agent episode reward: [-46.74184371283031], time: 39.46
steps: 17886295, episodes: 429000, mean episode reward: -46.97781317425487, num_cumulative_constraints: 136036, agent episode reward: [-46.97781317425487], time: 39.872
steps: 17928716, episodes: 430000, mean episode reward: -47.03772938783532, num_cumulative_constraints: 136287, agent episode reward: [-47.03772938783532], time: 39.734
steps: 17971058, episodes: 431000, mean episode reward: -47.21233924562851, num_cumulative_constraints: 136631, agent episode reward: [-47.21233924562851], time: 40.556
steps: 18013357, episodes: 432000, mean episode reward: -46.96914552591021, num_cumulative_constraints: 136859, agent episode reward: [-46.96914552591021], time: 39.115
steps: 18055451, episodes: 433000, mean episode reward: -46.72675339268895, num_cumulative_constraints: 137130, agent episode reward: [-46.72675339268895], time: 40.378
steps: 18097425, episodes: 434000, mean episode reward: -46.57504062861088, num_cumulative_constraints: 137362, agent episode reward: [-46.57504062861088], time: 40.166
steps: 18139416, episodes: 435000, mean episode reward: -46.70859577746286, num_cumulative_constraints: 137635, agent episode reward: [-46.70859577746286], time: 42.145
steps: 18181479, episodes: 436000, mean episode reward: -46.61805959250635, num_cumulative_constraints: 137880, agent episode reward: [-46.61805959250635], time: 39.423
steps: 18223588, episodes: 437000, mean episode reward: -47.13962643004999, num_cumulative_constraints: 138195, agent episode reward: [-47.13962643004999], time: 39.351
steps: 18265536, episodes: 438000, mean episode reward: -46.71424191864296, num_cumulative_constraints: 138457, agent episode reward: [-46.71424191864296], time: 40.354
steps: 18308006, episodes: 439000, mean episode reward: -47.02842131202343, num_cumulative_constraints: 138721, agent episode reward: [-47.02842131202343], time: 40.741
steps: 18349967, episodes: 440000, mean episode reward: -46.6533054428448, num_cumulative_constraints: 138984, agent episode reward: [-46.6533054428448], time: 39.044
steps: 18391861, episodes: 441000, mean episode reward: -46.52176856987229, num_cumulative_constraints: 139239, agent episode reward: [-46.52176856987229], time: 40.765
steps: 18433946, episodes: 442000, mean episode reward: -47.002489490460405, num_cumulative_constraints: 139486, agent episode reward: [-47.002489490460405], time: 39.802
steps: 18476170, episodes: 443000, mean episode reward: -46.873617899332515, num_cumulative_constraints: 139737, agent episode reward: [-46.873617899332515], time: 44.308
steps: 18518301, episodes: 444000, mean episode reward: -47.03177040063645, num_cumulative_constraints: 140095, agent episode reward: [-47.03177040063645], time: 49.946
steps: 18560472, episodes: 445000, mean episode reward: -46.719583759749916, num_cumulative_constraints: 140323, agent episode reward: [-46.719583759749916], time: 38.426
steps: 18602452, episodes: 446000, mean episode reward: -46.939620608235344, num_cumulative_constraints: 140639, agent episode reward: [-46.939620608235344], time: 40.567
steps: 18644525, episodes: 447000, mean episode reward: -47.020049696417104, num_cumulative_constraints: 140918, agent episode reward: [-47.020049696417104], time: 40.082
steps: 18686733, episodes: 448000, mean episode reward: -47.256417694618726, num_cumulative_constraints: 141265, agent episode reward: [-47.256417694618726], time: 40.855
steps: 18728911, episodes: 449000, mean episode reward: -46.92011781486353, num_cumulative_constraints: 141561, agent episode reward: [-46.92011781486353], time: 39.511
steps: 18770921, episodes: 450000, mean episode reward: -46.45449901710951, num_cumulative_constraints: 141791, agent episode reward: [-46.45449901710951], time: 38.694
steps: 18813139, episodes: 451000, mean episode reward: -46.85736582690135, num_cumulative_constraints: 142025, agent episode reward: [-46.85736582690135], time: 38.826
steps: 18855184, episodes: 452000, mean episode reward: -46.59891492526424, num_cumulative_constraints: 142262, agent episode reward: [-46.59891492526424], time: 40.595
steps: 18897260, episodes: 453000, mean episode reward: -46.87361889008206, num_cumulative_constraints: 142562, agent episode reward: [-46.87361889008206], time: 39.362
steps: 18939551, episodes: 454000, mean episode reward: -47.216957662184036, num_cumulative_constraints: 142863, agent episode reward: [-47.216957662184036], time: 39.596
steps: 18981931, episodes: 455000, mean episode reward: -46.993294747258666, num_cumulative_constraints: 143140, agent episode reward: [-46.993294747258666], time: 40.387
steps: 19024610, episodes: 456000, mean episode reward: -47.10846044380846, num_cumulative_constraints: 143347, agent episode reward: [-47.10846044380846], time: 39.932
steps: 19067130, episodes: 457000, mean episode reward: -46.799915090440045, num_cumulative_constraints: 143568, agent episode reward: [-46.799915090440045], time: 40.355
steps: 19109169, episodes: 458000, mean episode reward: -46.855999412089645, num_cumulative_constraints: 143829, agent episode reward: [-46.855999412089645], time: 40.121
steps: 19151634, episodes: 459000, mean episode reward: -47.215718984514254, num_cumulative_constraints: 144101, agent episode reward: [-47.215718984514254], time: 40.226
steps: 19194115, episodes: 460000, mean episode reward: -46.92295586991246, num_cumulative_constraints: 144355, agent episode reward: [-46.92295586991246], time: 40.34
steps: 19236641, episodes: 461000, mean episode reward: -47.25667652177677, num_cumulative_constraints: 144661, agent episode reward: [-47.25667652177677], time: 40.396
steps: 19278885, episodes: 462000, mean episode reward: -46.67442674741261, num_cumulative_constraints: 144917, agent episode reward: [-46.67442674741261], time: 39.689
steps: 19321362, episodes: 463000, mean episode reward: -46.90012577254213, num_cumulative_constraints: 145153, agent episode reward: [-46.90012577254213], time: 39.731
steps: 19363561, episodes: 464000, mean episode reward: -46.839136028374064, num_cumulative_constraints: 145439, agent episode reward: [-46.839136028374064], time: 39.809
steps: 19405261, episodes: 465000, mean episode reward: -46.84055968241279, num_cumulative_constraints: 145731, agent episode reward: [-46.84055968241279], time: 38.797
steps: 19446809, episodes: 466000, mean episode reward: -46.554798849312924, num_cumulative_constraints: 145962, agent episode reward: [-46.554798849312924], time: 37.852
steps: 19488545, episodes: 467000, mean episode reward: -46.63452416835522, num_cumulative_constraints: 146209, agent episode reward: [-46.63452416835522], time: 40.137
steps: 19530242, episodes: 468000, mean episode reward: -46.60265683146572, num_cumulative_constraints: 146434, agent episode reward: [-46.60265683146572], time: 39.228
steps: 19572027, episodes: 469000, mean episode reward: -47.026625614193044, num_cumulative_constraints: 146663, agent episode reward: [-47.026625614193044], time: 39.615
steps: 19613592, episodes: 470000, mean episode reward: -46.46869736368743, num_cumulative_constraints: 146891, agent episode reward: [-46.46869736368743], time: 40.2
steps: 19655596, episodes: 471000, mean episode reward: -46.5021458791573, num_cumulative_constraints: 147140, agent episode reward: [-46.5021458791573], time: 38.628
steps: 19697566, episodes: 472000, mean episode reward: -46.70922664082366, num_cumulative_constraints: 147380, agent episode reward: [-46.70922664082366], time: 40.262
steps: 19739293, episodes: 473000, mean episode reward: -46.72090506688741, num_cumulative_constraints: 147650, agent episode reward: [-46.72090506688741], time: 39.06
steps: 19781261, episodes: 474000, mean episode reward: -47.139529976617695, num_cumulative_constraints: 147882, agent episode reward: [-47.139529976617695], time: 39.71
steps: 19823196, episodes: 475000, mean episode reward: -46.79187749827565, num_cumulative_constraints: 148118, agent episode reward: [-46.79187749827565], time: 38.949
steps: 19865199, episodes: 476000, mean episode reward: -46.90318412651602, num_cumulative_constraints: 148365, agent episode reward: [-46.90318412651602], time: 38.963
steps: 19907070, episodes: 477000, mean episode reward: -47.09865120425872, num_cumulative_constraints: 148615, agent episode reward: [-47.09865120425872], time: 39.805
steps: 19948991, episodes: 478000, mean episode reward: -46.97481295309376, num_cumulative_constraints: 148900, agent episode reward: [-46.97481295309376], time: 40.112
steps: 19990939, episodes: 479000, mean episode reward: -46.87742378793527, num_cumulative_constraints: 149115, agent episode reward: [-46.87742378793527], time: 39.626
steps: 20032973, episodes: 480000, mean episode reward: -47.341192535393525, num_cumulative_constraints: 149440, agent episode reward: [-47.341192535393525], time: 40.686
steps: 20075007, episodes: 481000, mean episode reward: -47.052301332406586, num_cumulative_constraints: 149722, agent episode reward: [-47.052301332406586], time: 39.768
steps: 20117002, episodes: 482000, mean episode reward: -46.670473131744494, num_cumulative_constraints: 149985, agent episode reward: [-46.670473131744494], time: 39.275
steps: 20159042, episodes: 483000, mean episode reward: -47.076994037077014, num_cumulative_constraints: 150223, agent episode reward: [-47.076994037077014], time: 40.301
steps: 20201295, episodes: 484000, mean episode reward: -46.94333664416675, num_cumulative_constraints: 150457, agent episode reward: [-46.94333664416675], time: 39.594
steps: 20243495, episodes: 485000, mean episode reward: -46.90949964803974, num_cumulative_constraints: 150699, agent episode reward: [-46.90949964803974], time: 41.414
steps: 20285815, episodes: 486000, mean episode reward: -47.24467868039265, num_cumulative_constraints: 150994, agent episode reward: [-47.24467868039265], time: 41.464
steps: 20328260, episodes: 487000, mean episode reward: -47.53259053538699, num_cumulative_constraints: 151409, agent episode reward: [-47.53259053538699], time: 39.308
steps: 20370731, episodes: 488000, mean episode reward: -47.289079384874626, num_cumulative_constraints: 151735, agent episode reward: [-47.289079384874626], time: 39.883
steps: 20413241, episodes: 489000, mean episode reward: -47.48188705900313, num_cumulative_constraints: 152019, agent episode reward: [-47.48188705900313], time: 39.478
steps: 20455335, episodes: 490000, mean episode reward: -47.12026886527912, num_cumulative_constraints: 152268, agent episode reward: [-47.12026886527912], time: 40.653
steps: 20497518, episodes: 491000, mean episode reward: -47.324281624647355, num_cumulative_constraints: 152638, agent episode reward: [-47.324281624647355], time: 39.325
steps: 20539439, episodes: 492000, mean episode reward: -47.027003954050514, num_cumulative_constraints: 152913, agent episode reward: [-47.027003954050514], time: 39.089
steps: 20581848, episodes: 493000, mean episode reward: -47.34857188715006, num_cumulative_constraints: 153199, agent episode reward: [-47.34857188715006], time: 40.204
steps: 20624183, episodes: 494000, mean episode reward: -47.089730650327475, num_cumulative_constraints: 153459, agent episode reward: [-47.089730650327475], time: 39.127
steps: 20665969, episodes: 495000, mean episode reward: -46.99593169017774, num_cumulative_constraints: 153801, agent episode reward: [-46.99593169017774], time: 38.364
steps: 20707752, episodes: 496000, mean episode reward: -46.82566501035604, num_cumulative_constraints: 154100, agent episode reward: [-46.82566501035604], time: 40.294
steps: 20750030, episodes: 497000, mean episode reward: -47.4135513684917, num_cumulative_constraints: 154475, agent episode reward: [-47.4135513684917], time: 39.534
steps: 20792373, episodes: 498000, mean episode reward: -47.784221340739656, num_cumulative_constraints: 154796, agent episode reward: [-47.784221340739656], time: 40.514
steps: 20834543, episodes: 499000, mean episode reward: -47.01114187669547, num_cumulative_constraints: 155047, agent episode reward: [-47.01114187669547], time: 38.862
steps: 20876386, episodes: 500000, mean episode reward: -47.3249432525838, num_cumulative_constraints: 155371, agent episode reward: [-47.3249432525838], time: 40.198
steps: 20918204, episodes: 501000, mean episode reward: -47.10005660835783, num_cumulative_constraints: 155656, agent episode reward: [-47.10005660835783], time: 37.882
steps: 20959769, episodes: 502000, mean episode reward: -47.10994210382206, num_cumulative_constraints: 155988, agent episode reward: [-47.10994210382206], time: 40.586
steps: 21001501, episodes: 503000, mean episode reward: -46.84380382122091, num_cumulative_constraints: 156258, agent episode reward: [-46.84380382122091], time: 40.055
steps: 21043038, episodes: 504000, mean episode reward: -46.982703460295404, num_cumulative_constraints: 156578, agent episode reward: [-46.982703460295404], time: 38.61
steps: 21084652, episodes: 505000, mean episode reward: -47.185675780270344, num_cumulative_constraints: 156869, agent episode reward: [-47.185675780270344], time: 38.505
steps: 21125950, episodes: 506000, mean episode reward: -46.883501957962636, num_cumulative_constraints: 157204, agent episode reward: [-46.883501957962636], time: 40.408
steps: 21167627, episodes: 507000, mean episode reward: -46.988824208508234, num_cumulative_constraints: 157584, agent episode reward: [-46.988824208508234], time: 39.616
steps: 21209112, episodes: 508000, mean episode reward: -47.166824628706124, num_cumulative_constraints: 157975, agent episode reward: [-47.166824628706124], time: 38.929
steps: 21250876, episodes: 509000, mean episode reward: -47.07589359442088, num_cumulative_constraints: 158256, agent episode reward: [-47.07589359442088], time: 38.835
steps: 21292842, episodes: 510000, mean episode reward: -47.54090506859841, num_cumulative_constraints: 158615, agent episode reward: [-47.54090506859841], time: 39.258
steps: 21334477, episodes: 511000, mean episode reward: -47.13919450522745, num_cumulative_constraints: 158892, agent episode reward: [-47.13919450522745], time: 38.732
steps: 21376006, episodes: 512000, mean episode reward: -46.998997302553875, num_cumulative_constraints: 159241, agent episode reward: [-46.998997302553875], time: 39.655
steps: 21418109, episodes: 513000, mean episode reward: -47.37172042616889, num_cumulative_constraints: 159557, agent episode reward: [-47.37172042616889], time: 41.218
steps: 21459988, episodes: 514000, mean episode reward: -47.33360272539412, num_cumulative_constraints: 159855, agent episode reward: [-47.33360272539412], time: 38.796
steps: 21502066, episodes: 515000, mean episode reward: -46.896567588265015, num_cumulative_constraints: 160049, agent episode reward: [-46.896567588265015], time: 38.742
steps: 21544161, episodes: 516000, mean episode reward: -47.14314224695075, num_cumulative_constraints: 160306, agent episode reward: [-47.14314224695075], time: 39.067
steps: 21586150, episodes: 517000, mean episode reward: -47.110753109653245, num_cumulative_constraints: 160581, agent episode reward: [-47.110753109653245], time: 38.356
steps: 21628008, episodes: 518000, mean episode reward: -47.08105259647247, num_cumulative_constraints: 160848, agent episode reward: [-47.08105259647247], time: 40.567
steps: 21670074, episodes: 519000, mean episode reward: -46.85167053544024, num_cumulative_constraints: 161109, agent episode reward: [-46.85167053544024], time: 38.749
steps: 21712026, episodes: 520000, mean episode reward: -46.89445119225935, num_cumulative_constraints: 161463, agent episode reward: [-46.89445119225935], time: 39.573
steps: 21753770, episodes: 521000, mean episode reward: -46.86270164436693, num_cumulative_constraints: 161755, agent episode reward: [-46.86270164436693], time: 38.614
steps: 21795675, episodes: 522000, mean episode reward: -47.13004401590957, num_cumulative_constraints: 162104, agent episode reward: [-47.13004401590957], time: 39.123
steps: 21837447, episodes: 523000, mean episode reward: -47.009005204084424, num_cumulative_constraints: 162503, agent episode reward: [-47.009005204084424], time: 39.821
steps: 21879201, episodes: 524000, mean episode reward: -46.981433903636706, num_cumulative_constraints: 162797, agent episode reward: [-46.981433903636706], time: 38.414
steps: 21920824, episodes: 525000, mean episode reward: -46.88586751418137, num_cumulative_constraints: 163123, agent episode reward: [-46.88586751418137], time: 39.899
steps: 21962769, episodes: 526000, mean episode reward: -46.56519098423635, num_cumulative_constraints: 163338, agent episode reward: [-46.56519098423635], time: 41.061
steps: 22004368, episodes: 527000, mean episode reward: -46.65323811153877, num_cumulative_constraints: 163597, agent episode reward: [-46.65323811153877], time: 39.011
steps: 22046182, episodes: 528000, mean episode reward: -47.034129339464556, num_cumulative_constraints: 163884, agent episode reward: [-47.034129339464556], time: 39.333
steps: 22087966, episodes: 529000, mean episode reward: -47.02258173641378, num_cumulative_constraints: 164168, agent episode reward: [-47.02258173641378], time: 39.594
steps: 22130181, episodes: 530000, mean episode reward: -46.76227511161698, num_cumulative_constraints: 164369, agent episode reward: [-46.76227511161698], time: 40.019
steps: 22171961, episodes: 531000, mean episode reward: -46.75357826633297, num_cumulative_constraints: 164639, agent episode reward: [-46.75357826633297], time: 37.893
steps: 22213846, episodes: 532000, mean episode reward: -46.7767007686278, num_cumulative_constraints: 164941, agent episode reward: [-46.7767007686278], time: 39.295
steps: 22255901, episodes: 533000, mean episode reward: -46.91962385579723, num_cumulative_constraints: 165220, agent episode reward: [-46.91962385579723], time: 41.406
steps: 22297794, episodes: 534000, mean episode reward: -47.097045401127524, num_cumulative_constraints: 165488, agent episode reward: [-47.097045401127524], time: 38.759
steps: 22339441, episodes: 535000, mean episode reward: -46.68812443459826, num_cumulative_constraints: 165763, agent episode reward: [-46.68812443459826], time: 39.06
steps: 22380949, episodes: 536000, mean episode reward: -46.54237342182499, num_cumulative_constraints: 165998, agent episode reward: [-46.54237342182499], time: 38.498
steps: 22422680, episodes: 537000, mean episode reward: -46.80346376107364, num_cumulative_constraints: 166251, agent episode reward: [-46.80346376107364], time: 39.524
steps: 22464065, episodes: 538000, mean episode reward: -46.46190718161817, num_cumulative_constraints: 166485, agent episode reward: [-46.46190718161817], time: 38.479
steps: 22505605, episodes: 539000, mean episode reward: -47.06395555235224, num_cumulative_constraints: 166752, agent episode reward: [-47.06395555235224], time: 39.7
steps: 22547355, episodes: 540000, mean episode reward: -46.85781342451635, num_cumulative_constraints: 167014, agent episode reward: [-46.85781342451635], time: 39.019
steps: 22589025, episodes: 541000, mean episode reward: -46.47525349657783, num_cumulative_constraints: 167251, agent episode reward: [-46.47525349657783], time: 38.454
steps: 22630685, episodes: 542000, mean episode reward: -46.51959872575491, num_cumulative_constraints: 167511, agent episode reward: [-46.51959872575491], time: 38.886
steps: 22672101, episodes: 543000, mean episode reward: -46.54519102253366, num_cumulative_constraints: 167800, agent episode reward: [-46.54519102253366], time: 38.372
steps: 22713731, episodes: 544000, mean episode reward: -46.818887073024094, num_cumulative_constraints: 168066, agent episode reward: [-46.818887073024094], time: 38.486
steps: 22755174, episodes: 545000, mean episode reward: -46.875019458733796, num_cumulative_constraints: 168381, agent episode reward: [-46.875019458733796], time: 39.277
steps: 22796948, episodes: 546000, mean episode reward: -46.91403033713571, num_cumulative_constraints: 168623, agent episode reward: [-46.91403033713571], time: 38.398
steps: 22838387, episodes: 547000, mean episode reward: -46.50350897422383, num_cumulative_constraints: 168848, agent episode reward: [-46.50350897422383], time: 39.085
steps: 22879574, episodes: 548000, mean episode reward: -46.432947560060846, num_cumulative_constraints: 169146, agent episode reward: [-46.432947560060846], time: 38.775
steps: 22921584, episodes: 549000, mean episode reward: -46.862070485761464, num_cumulative_constraints: 169425, agent episode reward: [-46.862070485761464], time: 39.189
steps: 22963105, episodes: 550000, mean episode reward: -46.51998249977804, num_cumulative_constraints: 169655, agent episode reward: [-46.51998249977804], time: 39.347
steps: 23004628, episodes: 551000, mean episode reward: -46.85802772002404, num_cumulative_constraints: 169915, agent episode reward: [-46.85802772002404], time: 39.582
steps: 23046314, episodes: 552000, mean episode reward: -46.84998684663895, num_cumulative_constraints: 170148, agent episode reward: [-46.84998684663895], time: 41.236
steps: 23087891, episodes: 553000, mean episode reward: -46.64675626822397, num_cumulative_constraints: 170419, agent episode reward: [-46.64675626822397], time: 38.997
steps: 23129377, episodes: 554000, mean episode reward: -46.65374181167046, num_cumulative_constraints: 170675, agent episode reward: [-46.65374181167046], time: 38.692
steps: 23170804, episodes: 555000, mean episode reward: -46.498104619933876, num_cumulative_constraints: 170944, agent episode reward: [-46.498104619933876], time: 39.715
steps: 23212408, episodes: 556000, mean episode reward: -46.68659080476281, num_cumulative_constraints: 171210, agent episode reward: [-46.68659080476281], time: 38.276
steps: 23253878, episodes: 557000, mean episode reward: -46.87215611147965, num_cumulative_constraints: 171480, agent episode reward: [-46.87215611147965], time: 39.339
steps: 23295207, episodes: 558000, mean episode reward: -46.3222392987155, num_cumulative_constraints: 171704, agent episode reward: [-46.3222392987155], time: 38.912
steps: 23336336, episodes: 559000, mean episode reward: -46.536643181658, num_cumulative_constraints: 171999, agent episode reward: [-46.536643181658], time: 39.685
steps: 23377619, episodes: 560000, mean episode reward: -46.74012930999835, num_cumulative_constraints: 172305, agent episode reward: [-46.74012930999835], time: 39.508
steps: 23418952, episodes: 561000, mean episode reward: -46.170925492718155, num_cumulative_constraints: 172548, agent episode reward: [-46.170925492718155], time: 37.615
steps: 23460511, episodes: 562000, mean episode reward: -46.70167226493683, num_cumulative_constraints: 172789, agent episode reward: [-46.70167226493683], time: 38.887
steps: 23502153, episodes: 563000, mean episode reward: -46.72338083165494, num_cumulative_constraints: 173113, agent episode reward: [-46.72338083165494], time: 39.212
steps: 23544150, episodes: 564000, mean episode reward: -46.70932549294223, num_cumulative_constraints: 173390, agent episode reward: [-46.70932549294223], time: 38.75
steps: 23585861, episodes: 565000, mean episode reward: -46.84568001872962, num_cumulative_constraints: 173709, agent episode reward: [-46.84568001872962], time: 39.187
steps: 23628009, episodes: 566000, mean episode reward: -46.719739715597875, num_cumulative_constraints: 173990, agent episode reward: [-46.719739715597875], time: 40.437
steps: 23669626, episodes: 567000, mean episode reward: -46.621209720075335, num_cumulative_constraints: 174281, agent episode reward: [-46.621209720075335], time: 39.251
steps: 23712177, episodes: 568000, mean episode reward: -47.06559183145821, num_cumulative_constraints: 174543, agent episode reward: [-47.06559183145821], time: 39.291
steps: 23754260, episodes: 569000, mean episode reward: -46.861552328446834, num_cumulative_constraints: 174821, agent episode reward: [-46.861552328446834], time: 38.987
steps: 23796277, episodes: 570000, mean episode reward: -47.16231670530741, num_cumulative_constraints: 175122, agent episode reward: [-47.16231670530741], time: 38.807
steps: 23838049, episodes: 571000, mean episode reward: -46.53460032922821, num_cumulative_constraints: 175373, agent episode reward: [-46.53460032922821], time: 38.838
steps: 23879578, episodes: 572000, mean episode reward: -46.275767871415766, num_cumulative_constraints: 175601, agent episode reward: [-46.275767871415766], time: 38.544
steps: 23920957, episodes: 573000, mean episode reward: -46.74781011482154, num_cumulative_constraints: 175876, agent episode reward: [-46.74781011482154], time: 38.388
steps: 23962312, episodes: 574000, mean episode reward: -46.63939292542695, num_cumulative_constraints: 176179, agent episode reward: [-46.63939292542695], time: 38.974
steps: 24003721, episodes: 575000, mean episode reward: -46.88629672489738, num_cumulative_constraints: 176467, agent episode reward: [-46.88629672489738], time: 39.07
steps: 24045120, episodes: 576000, mean episode reward: -46.63556370861665, num_cumulative_constraints: 176773, agent episode reward: [-46.63556370861665], time: 38.373
steps: 24086406, episodes: 577000, mean episode reward: -46.60527895490836, num_cumulative_constraints: 177066, agent episode reward: [-46.60527895490836], time: 40.078
steps: 24127784, episodes: 578000, mean episode reward: -46.24975889930802, num_cumulative_constraints: 177308, agent episode reward: [-46.24975889930802], time: 37.682
steps: 24169323, episodes: 579000, mean episode reward: -46.73778356490358, num_cumulative_constraints: 177596, agent episode reward: [-46.73778356490358], time: 39.439
steps: 24210626, episodes: 580000, mean episode reward: -46.46361489802622, num_cumulative_constraints: 177850, agent episode reward: [-46.46361489802622], time: 39.326
steps: 24252107, episodes: 581000, mean episode reward: -46.451923442141286, num_cumulative_constraints: 178113, agent episode reward: [-46.451923442141286], time: 40.14
steps: 24293889, episodes: 582000, mean episode reward: -46.94197181948587, num_cumulative_constraints: 178361, agent episode reward: [-46.94197181948587], time: 38.743
steps: 24336007, episodes: 583000, mean episode reward: -47.501000377949545, num_cumulative_constraints: 178668, agent episode reward: [-47.501000377949545], time: 38.232
steps: 24377332, episodes: 584000, mean episode reward: -46.71138350537736, num_cumulative_constraints: 178966, agent episode reward: [-46.71138350537736], time: 38.671
steps: 24418638, episodes: 585000, mean episode reward: -46.67257726261285, num_cumulative_constraints: 179268, agent episode reward: [-46.67257726261285], time: 38.815
steps: 24459868, episodes: 586000, mean episode reward: -46.90059517808749, num_cumulative_constraints: 179568, agent episode reward: [-46.90059517808749], time: 37.914
steps: 24501167, episodes: 587000, mean episode reward: -46.661351707733076, num_cumulative_constraints: 179835, agent episode reward: [-46.661351707733076], time: 39.081
steps: 24542589, episodes: 588000, mean episode reward: -46.46904435975, num_cumulative_constraints: 180063, agent episode reward: [-46.46904435975], time: 38.891
steps: 24584213, episodes: 589000, mean episode reward: -46.97052102670692, num_cumulative_constraints: 180330, agent episode reward: [-46.97052102670692], time: 39.973
steps: 24625760, episodes: 590000, mean episode reward: -46.490505822227206, num_cumulative_constraints: 180534, agent episode reward: [-46.490505822227206], time: 38.662
steps: 24667422, episodes: 591000, mean episode reward: -46.84220645343712, num_cumulative_constraints: 180792, agent episode reward: [-46.84220645343712], time: 39.022
steps: 24709325, episodes: 592000, mean episode reward: -46.99028334448744, num_cumulative_constraints: 181036, agent episode reward: [-46.99028334448744], time: 39.445
steps: 24751328, episodes: 593000, mean episode reward: -46.78904214848707, num_cumulative_constraints: 181247, agent episode reward: [-46.78904214848707], time: 38.494
steps: 24793095, episodes: 594000, mean episode reward: -46.725669257005734, num_cumulative_constraints: 181480, agent episode reward: [-46.725669257005734], time: 39.578
steps: 24835021, episodes: 595000, mean episode reward: -46.85646612204126, num_cumulative_constraints: 181665, agent episode reward: [-46.85646612204126], time: 39.552
steps: 24876748, episodes: 596000, mean episode reward: -46.7665541777661, num_cumulative_constraints: 181882, agent episode reward: [-46.7665541777661], time: 40.573
steps: 24918847, episodes: 597000, mean episode reward: -47.38823367097161, num_cumulative_constraints: 182134, agent episode reward: [-47.38823367097161], time: 39.798
steps: 24960809, episodes: 598000, mean episode reward: -46.54479005310692, num_cumulative_constraints: 182351, agent episode reward: [-46.54479005310692], time: 38.556
steps: 25003003, episodes: 599000, mean episode reward: -47.04857418117888, num_cumulative_constraints: 182628, agent episode reward: [-47.04857418117888], time: 39.823
steps: 25044942, episodes: 600000, mean episode reward: -46.85025017983863, num_cumulative_constraints: 182910, agent episode reward: [-46.85025017983863], time: 39.884



smooth diff without safety_layer
steps: 41820, episodes: 1000, mean episode reward: -47.50978814103029, num_cumulative_constraints: 190, agent episode reward: [-47.50978814103029], time: 27.203
steps: 83735, episodes: 2000, mean episode reward: -47.4581858476071, num_cumulative_constraints: 395, agent episode reward: [-47.4581858476071], time: 29.781
steps: 125348, episodes: 3000, mean episode reward: -47.2098890727013, num_cumulative_constraints: 673, agent episode reward: [-47.2098890727013], time: 34.067
steps: 167057, episodes: 4000, mean episode reward: -47.39407446707949, num_cumulative_constraints: 975, agent episode reward: [-47.39407446707949], time: 34.96
steps: 208741, episodes: 5000, mean episode reward: -47.609342166135654, num_cumulative_constraints: 1223, agent episode reward: [-47.609342166135654], time: 34.515
steps: 250359, episodes: 6000, mean episode reward: -47.14690483943063, num_cumulative_constraints: 1442, agent episode reward: [-47.14690483943063], time: 34.668
steps: 291837, episodes: 7000, mean episode reward: -47.09986423864381, num_cumulative_constraints: 1652, agent episode reward: [-47.09986423864381], time: 35.576
steps: 333209, episodes: 8000, mean episode reward: -46.955753804393844, num_cumulative_constraints: 1845, agent episode reward: [-46.955753804393844], time: 35.442
steps: 374886, episodes: 9000, mean episode reward: -47.375130384060554, num_cumulative_constraints: 2110, agent episode reward: [-47.375130384060554], time: 35.047
steps: 416525, episodes: 10000, mean episode reward: -47.08480237109748, num_cumulative_constraints: 2333, agent episode reward: [-47.08480237109748], time: 35.442
steps: 458102, episodes: 11000, mean episode reward: -47.331769604579165, num_cumulative_constraints: 2596, agent episode reward: [-47.331769604579165], time: 35.703
steps: 499481, episodes: 12000, mean episode reward: -46.98679626969247, num_cumulative_constraints: 2779, agent episode reward: [-46.98679626969247], time: 34.695
steps: 541179, episodes: 13000, mean episode reward: -46.9945010376037, num_cumulative_constraints: 2956, agent episode reward: [-46.9945010376037], time: 34.751
steps: 582528, episodes: 14000, mean episode reward: -46.880216748223354, num_cumulative_constraints: 3133, agent episode reward: [-46.880216748223354], time: 36.647
steps: 624044, episodes: 15000, mean episode reward: -46.94975373356412, num_cumulative_constraints: 3303, agent episode reward: [-46.94975373356412], time: 43.122
steps: 665598, episodes: 16000, mean episode reward: -47.086864522128444, num_cumulative_constraints: 3463, agent episode reward: [-47.086864522128444], time: 35.115
steps: 706971, episodes: 17000, mean episode reward: -46.7591844443249, num_cumulative_constraints: 3657, agent episode reward: [-46.7591844443249], time: 36.955
steps: 748321, episodes: 18000, mean episode reward: -46.839759991689625, num_cumulative_constraints: 3848, agent episode reward: [-46.839759991689625], time: 33.686
steps: 789783, episodes: 19000, mean episode reward: -46.78663144748568, num_cumulative_constraints: 4022, agent episode reward: [-46.78663144748568], time: 33.551
steps: 831315, episodes: 20000, mean episode reward: -47.05517562993836, num_cumulative_constraints: 4219, agent episode reward: [-47.05517562993836], time: 34.918
steps: 872982, episodes: 21000, mean episode reward: -47.17616034506332, num_cumulative_constraints: 4446, agent episode reward: [-47.17616034506332], time: 34.53
steps: 914663, episodes: 22000, mean episode reward: -47.18741566759459, num_cumulative_constraints: 4620, agent episode reward: [-47.18741566759459], time: 33.325
steps: 955976, episodes: 23000, mean episode reward: -46.94121152354359, num_cumulative_constraints: 4846, agent episode reward: [-46.94121152354359], time: 32.701
steps: 997602, episodes: 24000, mean episode reward: -47.2668688758628, num_cumulative_constraints: 5067, agent episode reward: [-47.2668688758628], time: 33.758
steps: 1039116, episodes: 25000, mean episode reward: -47.230607077492515, num_cumulative_constraints: 5283, agent episode reward: [-47.230607077492515], time: 33.665
steps: 1080737, episodes: 26000, mean episode reward: -46.92349787972085, num_cumulative_constraints: 5467, agent episode reward: [-46.92349787972085], time: 33.826
steps: 1121983, episodes: 27000, mean episode reward: -46.80072028547725, num_cumulative_constraints: 5646, agent episode reward: [-46.80072028547725], time: 33.988
steps: 1163389, episodes: 28000, mean episode reward: -46.88083173197742, num_cumulative_constraints: 5808, agent episode reward: [-46.88083173197742], time: 35.808
steps: 1204788, episodes: 29000, mean episode reward: -46.99906368009481, num_cumulative_constraints: 6028, agent episode reward: [-46.99906368009481], time: 34.608
steps: 1246135, episodes: 30000, mean episode reward: -46.99059835545633, num_cumulative_constraints: 6222, agent episode reward: [-46.99059835545633], time: 33.734
steps: 1287485, episodes: 31000, mean episode reward: -47.131142438025684, num_cumulative_constraints: 6478, agent episode reward: [-47.131142438025684], time: 35.848
steps: 1328916, episodes: 32000, mean episode reward: -46.697801294220774, num_cumulative_constraints: 6703, agent episode reward: [-46.697801294220774], time: 35.079
steps: 1370189, episodes: 33000, mean episode reward: -47.027992491058974, num_cumulative_constraints: 6892, agent episode reward: [-47.027992491058974], time: 44.864
steps: 1411533, episodes: 34000, mean episode reward: -46.865400164297405, num_cumulative_constraints: 7091, agent episode reward: [-46.865400164297405], time: 62.77
steps: 1452708, episodes: 35000, mean episode reward: -46.79945734071664, num_cumulative_constraints: 7282, agent episode reward: [-46.79945734071664], time: 55.398
steps: 1493685, episodes: 36000, mean episode reward: -46.258335389516816, num_cumulative_constraints: 7428, agent episode reward: [-46.258335389516816], time: 55.138
steps: 1534739, episodes: 37000, mean episode reward: -46.58431176793563, num_cumulative_constraints: 7626, agent episode reward: [-46.58431176793563], time: 56.995
steps: 1575865, episodes: 38000, mean episode reward: -46.58735995842175, num_cumulative_constraints: 7801, agent episode reward: [-46.58735995842175], time: 50.779
steps: 1616981, episodes: 39000, mean episode reward: -46.90272142950072, num_cumulative_constraints: 7971, agent episode reward: [-46.90272142950072], time: 55.66

Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 59275, episodes: 1000, mean episode reward: -99.03739049960434, num_cumulative_constraints: 5934, agent episode reward: [-99.03739049960434], time: 72.641
steps: 118614, episodes: 2000, mean episode reward: -97.75929812970149, num_cumulative_constraints: 12779, agent episode reward: [-97.75929812970149], time: 97.584
steps: 177046, episodes: 3000, mean episode reward: -76.7261274395715, num_cumulative_constraints: 19289, agent episode reward: [-76.7261274395715], time: 108.388
steps: 234674, episodes: 4000, mean episode reward: -65.65787825355729, num_cumulative_constraints: 23018, agent episode reward: [-65.65787825355729], time: 119.947
steps: 291599, episodes: 5000, mean episode reward: -54.65189808766584, num_cumulative_constraints: 24183, agent episode reward: [-54.65189808766584], time: 122.301
steps: 344354, episodes: 6000, mean episode reward: -47.433393797862884, num_cumulative_constraints: 24827, agent episode reward: [-47.433393797862884], time: 92.53
steps: 394596, episodes: 7000, mean episode reward: -45.96893630681061, num_cumulative_constraints: 25573, agent episode reward: [-45.96893630681061], time: 96.316
steps: 442410, episodes: 8000, mean episode reward: -44.54873009320652, num_cumulative_constraints: 26304, agent episode reward: [-44.54873009320652], time: 85.63
steps: 486803, episodes: 9000, mean episode reward: -42.57897398908081, num_cumulative_constraints: 26924, agent episode reward: [-42.57897398908081], time: 70.385
steps: 528779, episodes: 10000, mean episode reward: -41.17093323219754, num_cumulative_constraints: 27446, agent episode reward: [-41.17093323219754], time: 64.846
steps: 570520, episodes: 11000, mean episode reward: -40.78405068242921, num_cumulative_constraints: 27914, agent episode reward: [-40.78405068242921], time: 67.337
steps: 612079, episodes: 12000, mean episode reward: -40.62406839010756, num_cumulative_constraints: 28338, agent episode reward: [-40.62406839010756], time: 60.52
steps: 653503, episodes: 13000, mean episode reward: -40.78103014685649, num_cumulative_constraints: 28857, agent episode reward: [-40.78103014685649], time: 74.486
steps: 694874, episodes: 14000, mean episode reward: -40.9856093652969, num_cumulative_constraints: 29438, agent episode reward: [-40.9856093652969], time: 81.331



another time
C:\Users\xichenyang\AppData\Local\Continuum\anaconda3\python.exe C:/Users/xichenyang/Desktop/UAV-RL-real-dynamics/maddpg-master/experiments/train.py
2019-07-21 17:02:49.731749: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From C:\Users\xichenyang\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Using good policy maddpg and adv policy maddpg
Loading previous state...
WARNING:tensorflow:From C:\Users\xichenyang\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\training\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Starting iterations...
steps: 41801, episodes: 1000, mean episode reward: -45.986390301876554, num_cumulative_constraints: 2190, agent episode reward: [-45.986390301876554], time: 24.656
steps: 83433, episodes: 2000, mean episode reward: -45.330709075111514, num_cumulative_constraints: 4119, agent episode reward: [-45.330709075111514], time: 28.97
steps: 125102, episodes: 3000, mean episode reward: -42.44788906218007, num_cumulative_constraints: 5083, agent episode reward: [-42.44788906218007], time: 30.755
steps: 166880, episodes: 4000, mean episode reward: -41.5011704871472, num_cumulative_constraints: 5706, agent episode reward: [-41.5011704871472], time: 31.62
steps: 208804, episodes: 5000, mean episode reward: -41.37239857209604, num_cumulative_constraints: 6294, agent episode reward: [-41.37239857209604], time: 30.599
steps: 250637, episodes: 6000, mean episode reward: -41.198031823068185, num_cumulative_constraints: 6856, agent episode reward: [-41.198031823068185], time: 31.334
steps: 291980, episodes: 7000, mean episode reward: -40.86888858184284, num_cumulative_constraints: 7360, agent episode reward: [-40.86888858184284], time: 31.888
steps: 333063, episodes: 8000, mean episode reward: -40.83713868670516, num_cumulative_constraints: 7892, agent episode reward: [-40.83713868670516], time: 30.365
steps: 374176, episodes: 9000, mean episode reward: -40.983527783411866, num_cumulative_constraints: 8444, agent episode reward: [-40.983527783411866], time: 31.756
steps: 415239, episodes: 10000, mean episode reward: -41.10180423289885, num_cumulative_constraints: 9064, agent episode reward: [-41.10180423289885], time: 29.799
steps: 456280, episodes: 11000, mean episode reward: -41.29722553111888, num_cumulative_constraints: 9730, agent episode reward: [-41.29722553111888], time: 31.392
steps: 497403, episodes: 12000, mean episode reward: -40.92231767844039, num_cumulative_constraints: 10250, agent episode reward: [-40.92231767844039], time: 31.162
steps: 538538, episodes: 13000, mean episode reward: -40.76223124207965, num_cumulative_constraints: 10720, agent episode reward: [-40.76223124207965], time: 30.901
steps: 579553, episodes: 14000, mean episode reward: -40.755195021274126, num_cumulative_constraints: 11224, agent episode reward: [-40.755195021274126], time: 30.783
steps: 620816, episodes: 15000, mean episode reward: -40.88003683001372, num_cumulative_constraints: 11722, agent episode reward: [-40.88003683001372], time: 30.864
steps: 662048, episodes: 16000, mean episode reward: -41.22042473180912, num_cumulative_constraints: 12285, agent episode reward: [-41.22042473180912], time: 31.296

Process finished with exit code -1



1 landmark
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 59548, episodes: 1000, mean episode reward: -88.57534921200029, num_cumulative_constraints: 1279, agent episode reward: [-88.57534921200029], time: 47.678
steps: 118467, episodes: 2000, mean episode reward: -76.08166509327857, num_cumulative_constraints: 3091, agent episode reward: [-76.08166509327857], time: 71.614
steps: 172486, episodes: 3000, mean episode reward: -51.59999641915736, num_cumulative_constraints: 4587, agent episode reward: [-51.59999641915736], time: 54.693
steps: 221530, episodes: 4000, mean episode reward: -45.97752314333855, num_cumulative_constraints: 5563, agent episode reward: [-45.97752314333855], time: 67.104
steps: 271808, episodes: 5000, mean episode reward: -45.30753506555583, num_cumulative_constraints: 6101, agent episode reward: [-45.30753506555583], time: 71.167
steps: 321050, episodes: 6000, mean episode reward: -43.27169490367739, num_cumulative_constraints: 6154, agent episode reward: [-43.27169490367739], time: 58.908
steps: 367431, episodes: 7000, mean episode reward: -41.41993877062692, num_cumulative_constraints: 6175, agent episode reward: [-41.41993877062692], time: 65.515
steps: 409711, episodes: 8000, mean episode reward: -39.22670842177663, num_cumulative_constraints: 6215, agent episode reward: [-39.22670842177663], time: 51.344
steps: 450681, episodes: 9000, mean episode reward: -38.64047079193772, num_cumulative_constraints: 6264, agent episode reward: [-38.64047079193772], time: 45.207
steps: 491418, episodes: 10000, mean episode reward: -38.55162527299274, num_cumulative_constraints: 6298, agent episode reward: [-38.55162527299274], time: 51.524
steps: 531772, episodes: 11000, mean episode reward: -38.35417908714002, num_cumulative_constraints: 6335, agent episode reward: [-38.35417908714002], time: 55.854
steps: 572744, episodes: 12000, mean episode reward: -38.52889705271986, num_cumulative_constraints: 6364, agent episode reward: [-38.52889705271986], time: 49.395

steps: 39896, episodes: 1000, mean episode reward: -38.146757405998024, num_cumulative_constraints: 17, agent episode reward: [-38.146757405998024], time: 36.944
steps: 80786, episodes: 2000, mean episode reward: -38.66540580191134, num_cumulative_constraints: 58, agent episode reward: [-38.66540580191134], time: 45.387
steps: 121187, episodes: 3000, mean episode reward: -38.77065105772451, num_cumulative_constraints: 212, agent episode reward: [-38.77065105772451], time: 52.647
steps: 161630, episodes: 4000, mean episode reward: -39.0507286827249, num_cumulative_constraints: 511, agent episode reward: [-39.0507286827249], time: 50.002
steps: 202393, episodes: 5000, mean episode reward: -39.509499789316926, num_cumulative_constraints: 831, agent episode reward: [-39.509499789316926], time: 49.35
steps: 242902, episodes: 6000, mean episode reward: -38.67042439247768, num_cumulative_constraints: 921, agent episode reward: [-38.67042439247768], time: 53.002
steps: 282874, episodes: 7000, mean episode reward: -38.34067076834887, num_cumulative_constraints: 1032, agent episode reward: [-38.34067076834887], time: 53.282
steps: 322991, episodes: 8000, mean episode reward: -38.396064771214064, num_cumulative_constraints: 1112, agent episode reward: [-38.396064771214064], time: 57.567

with safety_layer

steps: 39965, episodes: 1000, mean episode reward: -38.26160638910133, num_cumulative_constraints: 47, agent episode reward: [-38.26160638910133], time: 57.019
steps: 80060, episodes: 2000, mean episode reward: -38.337656588566986, num_cumulative_constraints: 90, agent episode reward: [-38.337656588566986], time: 69.637
steps: 120162, episodes: 3000, mean episode reward: -38.33986890237994, num_cumulative_constraints: 170, agent episode reward: [-38.33986890237994], time: 66.044
steps: 160232, episodes: 4000, mean episode reward: -38.193383387300884, num_cumulative_constraints: 240, agent episode reward: [-38.193383387300884], time: 66.015
steps: 200531, episodes: 5000, mean episode reward: -38.30009753673443, num_cumulative_constraints: 335, agent episode reward: [-38.30009753673443], time: 83.672
steps: 240904, episodes: 6000, mean episode reward: -38.54849751897353, num_cumulative_constraints: 508, agent episode reward: [-38.54849751897353], time: 76.702


6 landmarks without safety_layer
steps: 59220, episodes: 1000, mean episode reward: -99.66015173085468, num_cumulative_constraints: 7586, agent episode reward: [-99.66015173085468], time: 75.915
steps: 118193, episodes: 2000, mean episode reward: -94.70314121263178, num_cumulative_constraints: 15069, agent episode reward: [-94.70314121263178], time: 89.712
steps: 176498, episodes: 3000, mean episode reward: -75.38650998853004, num_cumulative_constraints: 21067, agent episode reward: [-75.38650998853004], time: 94.619
steps: 233801, episodes: 4000, mean episode reward: -65.00951923533887, num_cumulative_constraints: 23921, agent episode reward: [-65.00951923533887], time: 81.614
steps: 288634, episodes: 5000, mean episode reward: -49.588709740704225, num_cumulative_constraints: 24703, agent episode reward: [-49.588709740704225], time: 85.349
steps: 339457, episodes: 6000, mean episode reward: -46.113108510446686, num_cumulative_constraints: 25300, agent episode reward: [-46.113108510446686], time: 78.122
steps: 386144, episodes: 7000, mean episode reward: -43.753245175537245, num_cumulative_constraints: 25913, agent episode reward: [-43.753245175537245], time: 73.096
steps: 430281, episodes: 8000, mean episode reward: -42.38602514211941, num_cumulative_constraints: 26532, agent episode reward: [-42.38602514211941], time: 77.92
steps: 473863, episodes: 9000, mean episode reward: -42.28115075469634, num_cumulative_constraints: 27169, agent episode reward: [-42.28115075469634], time: 79.027
steps: 517765, episodes: 10000, mean episode reward: -42.15659468359263, num_cumulative_constraints: 27747, agent episode reward: [-42.15659468359263], time: 69.367
steps: 561606, episodes: 11000, mean episode reward: -41.96234218281111, num_cumulative_constraints: 28259, agent episode reward: [-41.96234218281111], time: 66.348
steps: 605082, episodes: 12000, mean episode reward: -42.090903237123314, num_cumulative_constraints: 28862, agent episode reward: [-42.090903237123314], time: 67.426
steps: 648443, episodes: 13000, mean episode reward: -41.951374386435276, num_cumulative_constraints: 29426, agent episode reward: [-41.951374386435276], time: 83.33
steps: 691177, episodes: 14000, mean episode reward: -41.5640441983278, num_cumulative_constraints: 29997, agent episode reward: [-41.5640441983278], time: 67.98
steps: 733511, episodes: 15000, mean episode reward: -41.13744772942601, num_cumulative_constraints: 30479, agent episode reward: [-41.13744772942601], time: 62.771
steps: 776100, episodes: 16000, mean episode reward: -41.144898351950424, num_cumulative_constraints: 30876, agent episode reward: [-41.144898351950424], time: 67.357
steps: 817839, episodes: 17000, mean episode reward: -40.97404371550394, num_cumulative_constraints: 31380, agent episode reward: [-40.97404371550394], time: 61.67
steps: 859486, episodes: 18000, mean episode reward: -40.95000829292416, num_cumulative_constraints: 31847, agent episode reward: [-40.95000829292416], time: 71.354
steps: 900914, episodes: 19000, mean episode reward: -40.666280498911924, num_cumulative_constraints: 32317, agent episode reward: [-40.666280498911924], time: 81.7
steps: 942372, episodes: 20000, mean episode reward: -40.89425909327231, num_cumulative_constraints: 32847, agent episode reward: [-40.89425909327231], time: 60.601
steps: 983554, episodes: 21000, mean episode reward: -40.636650554510915, num_cumulative_constraints: 33348, agent episode reward: [-40.636650554510915], time: 65.555
steps: 1024685, episodes: 22000, mean episode reward: -40.707321090702784, num_cumulative_constraints: 33872, agent episode reward: [-40.707321090702784], time: 59.048
steps: 1065839, episodes: 23000, mean episode reward: -40.662222526143395, num_cumulative_constraints: 34358, agent episode reward: [-40.662222526143395], time: 65.471
steps: 1106816, episodes: 24000, mean episode reward: -40.60963757204622, num_cumulative_constraints: 34867, agent episode reward: [-40.60963757204622], time: 70.605
steps: 1147799, episodes: 25000, mean episode reward: -40.41430924886609, num_cumulative_constraints: 35302, agent episode reward: [-40.41430924886609], time: 68.736
steps: 1188835, episodes: 26000, mean episode reward: -40.49854478999861, num_cumulative_constraints: 35716, agent episode reward: [-40.49854478999861], time: 73.659
steps: 1229723, episodes: 27000, mean episode reward: -40.429151235724625, num_cumulative_constraints: 36142, agent episode reward: [-40.429151235724625], time: 73.141
steps: 1270591, episodes: 28000, mean episode reward: -40.43052337346581, num_cumulative_constraints: 36588, agent episode reward: [-40.43052337346581], time: 64.461
steps: 1311479, episodes: 29000, mean episode reward: -40.33672351855592, num_cumulative_constraints: 36984, agent episode reward: [-40.33672351855592], time: 56.468
steps: 1352459, episodes: 30000, mean episode reward: -40.576558563888184, num_cumulative_constraints: 37445, agent episode reward: [-40.576558563888184], time: 63.638
steps: 1393339, episodes: 31000, mean episode reward: -40.3334408934471, num_cumulative_constraints: 37887, agent episode reward: [-40.3334408934471], time: 65.244
steps: 1434316, episodes: 32000, mean episode reward: -40.57931431990694, num_cumulative_constraints: 38349, agent episode reward: [-40.57931431990694], time: 77.471
steps: 1475331, episodes: 33000, mean episode reward: -40.243798969903175, num_cumulative_constraints: 38718, agent episode reward: [-40.243798969903175], time: 71.861
steps: 1516291, episodes: 34000, mean episode reward: -40.71337673927813, num_cumulative_constraints: 39234, agent episode reward: [-40.71337673927813], time: 83.971
steps: 1557193, episodes: 35000, mean episode reward: -40.36365042468671, num_cumulative_constraints: 39678, agent episode reward: [-40.36365042468671], time: 74.848
steps: 1598268, episodes: 36000, mean episode reward: -40.500049918208305, num_cumulative_constraints: 40106, agent episode reward: [-40.500049918208305], time: 78.668
steps: 1639343, episodes: 37000, mean episode reward: -40.135247182550444, num_cumulative_constraints: 40445, agent episode reward: [-40.135247182550444], time: 72.803
steps: 1680452, episodes: 38000, mean episode reward: -40.33294877474105, num_cumulative_constraints: 40825, agent episode reward: [-40.33294877474105], time: 80.947
steps: 1721500, episodes: 39000, mean episode reward: -40.2827029254425, num_cumulative_constraints: 41192, agent episode reward: [-40.2827029254425], time: 87.101
steps: 1762494, episodes: 40000, mean episode reward: -40.41684358970876, num_cumulative_constraints: 41619, agent episode reward: [-40.41684358970876], time: 74.45
steps: 1803459, episodes: 41000, mean episode reward: -40.048255146329026, num_cumulative_constraints: 41920, agent episode reward: [-40.048255146329026], time: 73.487
steps: 1844330, episodes: 42000, mean episode reward: -40.06868549605066, num_cumulative_constraints: 42282, agent episode reward: [-40.06868549605066], time: 79.626
steps: 1885853, episodes: 43000, mean episode reward: -40.369506799669274, num_cumulative_constraints: 42594, agent episode reward: [-40.369506799669274], time: 81.175
steps: 1927837, episodes: 44000, mean episode reward: -40.27033108861924, num_cumulative_constraints: 42840, agent episode reward: [-40.27033108861924], time: 76.606
steps: 1968688, episodes: 45000, mean episode reward: -39.78642298669807, num_cumulative_constraints: 43120, agent episode reward: [-39.78642298669807], time: 77.494
steps: 2009697, episodes: 46000, mean episode reward: -39.937997461257225, num_cumulative_constraints: 43423, agent episode reward: [-39.937997461257225], time: 78.205
steps: 2050487, episodes: 47000, mean episode reward: -39.86526917432494, num_cumulative_constraints: 43720, agent episode reward: [-39.86526917432494], time: 76.838
steps: 2091325, episodes: 48000, mean episode reward: -39.81772170868104, num_cumulative_constraints: 44031, agent episode reward: [-39.81772170868104], time: 73.87
steps: 2132120, episodes: 49000, mean episode reward: -39.6696127455722, num_cumulative_constraints: 44300, agent episode reward: [-39.6696127455722], time: 70.227
steps: 2172702, episodes: 50000, mean episode reward: -39.41940418882118, num_cumulative_constraints: 44544, agent episode reward: [-39.41940418882118], time: 70.025
steps: 2213479, episodes: 51000, mean episode reward: -39.67563199863864, num_cumulative_constraints: 44801, agent episode reward: [-39.67563199863864], time: 71.67
steps: 2253991, episodes: 52000, mean episode reward: -39.50175151568297, num_cumulative_constraints: 45049, agent episode reward: [-39.50175151568297], time: 70.311
steps: 2294576, episodes: 53000, mean episode reward: -39.33456930413689, num_cumulative_constraints: 45270, agent episode reward: [-39.33456930413689], time: 69.89
steps: 2335199, episodes: 54000, mean episode reward: -39.66406108205098, num_cumulative_constraints: 45549, agent episode reward: [-39.66406108205098], time: 76.375
steps: 2375679, episodes: 55000, mean episode reward: -39.437239606649385, num_cumulative_constraints: 45836, agent episode reward: [-39.437239606649385], time: 79.634
steps: 2416145, episodes: 56000, mean episode reward: -39.22135745425063, num_cumulative_constraints: 46032, agent episode reward: [-39.22135745425063], time: 79.623
steps: 2456735, episodes: 57000, mean episode reward: -39.30087411434887, num_cumulative_constraints: 46198, agent episode reward: [-39.30087411434887], time: 74.878
steps: 2497304, episodes: 58000, mean episode reward: -39.37729192054605, num_cumulative_constraints: 46415, agent episode reward: [-39.37729192054605], time: 70.848
steps: 2537869, episodes: 59000, mean episode reward: -39.4347365614155, num_cumulative_constraints: 46638, agent episode reward: [-39.4347365614155], time: 64.537
steps: 2578393, episodes: 60000, mean episode reward: -39.094732253610275, num_cumulative_constraints: 46781, agent episode reward: [-39.094732253610275], time: 58.226
steps: 2618889, episodes: 61000, mean episode reward: -39.375337396639345, num_cumulative_constraints: 47042, agent episode reward: [-39.375337396639345], time: 59.556
steps: 2659532, episodes: 62000, mean episode reward: -39.778044231813716, num_cumulative_constraints: 47382, agent episode reward: [-39.778044231813716], time: 64.93
steps: 2700153, episodes: 63000, mean episode reward: -39.42402696800148, num_cumulative_constraints: 47597, agent episode reward: [-39.42402696800148], time: 55.36
steps: 2740698, episodes: 64000, mean episode reward: -39.23436915661146, num_cumulative_constraints: 47765, agent episode reward: [-39.23436915661146], time: 58.542
steps: 2781158, episodes: 65000, mean episode reward: -39.10928023231053, num_cumulative_constraints: 47935, agent episode reward: [-39.10928023231053], time: 57.374
steps: 2821850, episodes: 66000, mean episode reward: -39.47022171953322, num_cumulative_constraints: 48139, agent episode reward: [-39.47022171953322], time: 68.603
steps: 2862293, episodes: 67000, mean episode reward: -39.13809197892863, num_cumulative_constraints: 48302, agent episode reward: [-39.13809197892863], time: 63.9
steps: 2902824, episodes: 68000, mean episode reward: -39.22926496579655, num_cumulative_constraints: 48483, agent episode reward: [-39.22926496579655], time: 58.259
steps: 2943394, episodes: 69000, mean episode reward: -39.43624382330335, num_cumulative_constraints: 48707, agent episode reward: [-39.43624382330335], time: 66.811
steps: 2983756, episodes: 70000, mean episode reward: -39.022881883934474, num_cumulative_constraints: 48878, agent episode reward: [-39.022881883934474], time: 59.209
steps: 3024201, episodes: 71000, mean episode reward: -39.07935757251733, num_cumulative_constraints: 49029, agent episode reward: [-39.07935757251733], time: 71.638
steps: 3064693, episodes: 72000, mean episode reward: -39.28527919763531, num_cumulative_constraints: 49246, agent episode reward: [-39.28527919763531], time: 58.574
steps: 3105539, episodes: 73000, mean episode reward: -39.33272210193561, num_cumulative_constraints: 49428, agent episode reward: [-39.33272210193561], time: 57.142
steps: 3146152, episodes: 74000, mean episode reward: -39.37754445563944, num_cumulative_constraints: 49620, agent episode reward: [-39.37754445563944], time: 51.851
steps: 3186892, episodes: 75000, mean episode reward: -39.39753039563033, num_cumulative_constraints: 49852, agent episode reward: [-39.39753039563033], time: 61.209
steps: 3227612, episodes: 76000, mean episode reward: -39.524406200604005, num_cumulative_constraints: 50089, agent episode reward: [-39.524406200604005], time: 67.112
steps: 3268198, episodes: 77000, mean episode reward: -39.38461839103192, num_cumulative_constraints: 50306, agent episode reward: [-39.38461839103192], time: 60.106
steps: 3308862, episodes: 78000, mean episode reward: -39.3149329525372, num_cumulative_constraints: 50479, agent episode reward: [-39.3149329525372], time: 70.186
steps: 3349528, episodes: 79000, mean episode reward: -39.12294373148529, num_cumulative_constraints: 50628, agent episode reward: [-39.12294373148529], time: 62.467
steps: 3390449, episodes: 80000, mean episode reward: -39.62529183449889, num_cumulative_constraints: 50835, agent episode reward: [-39.62529183449889], time: 73.755
steps: 3431048, episodes: 81000, mean episode reward: -39.16699336161417, num_cumulative_constraints: 51003, agent episode reward: [-39.16699336161417], time: 67.363
steps: 3471610, episodes: 82000, mean episode reward: -39.248503699196604, num_cumulative_constraints: 51196, agent episode reward: [-39.248503699196604], time: 67.877
steps: 3512109, episodes: 83000, mean episode reward: -39.28034793504175, num_cumulative_constraints: 51389, agent episode reward: [-39.28034793504175], time: 72.57
steps: 3552769, episodes: 84000, mean episode reward: -39.533539324884885, num_cumulative_constraints: 51621, agent episode reward: [-39.533539324884885], time: 61.281
steps: 3593350, episodes: 85000, mean episode reward: -39.48229749753691, num_cumulative_constraints: 51862, agent episode reward: [-39.48229749753691], time: 68.431
steps: 3634109, episodes: 86000, mean episode reward: -39.43848379663004, num_cumulative_constraints: 52074, agent episode reward: [-39.43848379663004], time: 65.903
steps: 3674972, episodes: 87000, mean episode reward: -39.29763415067298, num_cumulative_constraints: 52233, agent episode reward: [-39.29763415067298], time: 65.66
steps: 3715694, episodes: 88000, mean episode reward: -39.44582544938818, num_cumulative_constraints: 52437, agent episode reward: [-39.44582544938818], time: 52.449
steps: 3756257, episodes: 89000, mean episode reward: -39.002458815998644, num_cumulative_constraints: 52571, agent episode reward: [-39.002458815998644], time: 71.566
steps: 3796852, episodes: 90000, mean episode reward: -39.41583678243957, num_cumulative_constraints: 52797, agent episode reward: [-39.41583678243957], time: 71.713
steps: 3837405, episodes: 91000, mean episode reward: -39.258036960511156, num_cumulative_constraints: 52981, agent episode reward: [-39.258036960511156], time: 63.557
steps: 3877889, episodes: 92000, mean episode reward: -39.37591663519165, num_cumulative_constraints: 53193, agent episode reward: [-39.37591663519165], time: 59.022
steps: 3918343, episodes: 93000, mean episode reward: -39.30589644281924, num_cumulative_constraints: 53424, agent episode reward: [-39.30589644281924], time: 54.135
steps: 3958688, episodes: 94000, mean episode reward: -39.09627662874783, num_cumulative_constraints: 53610, agent episode reward: [-39.09627662874783], time: 67.548
steps: 3999193, episodes: 95000, mean episode reward: -39.28592136210157, num_cumulative_constraints: 53778, agent episode reward: [-39.28592136210157], time: 61.311
steps: 4039688, episodes: 96000, mean episode reward: -39.189645468346626, num_cumulative_constraints: 53963, agent episode reward: [-39.189645468346626], time: 66.578
steps: 4080213, episodes: 97000, mean episode reward: -39.15136896798339, num_cumulative_constraints: 54132, agent episode reward: [-39.15136896798339], time: 66.141
steps: 4120824, episodes: 98000, mean episode reward: -39.15211147354518, num_cumulative_constraints: 54267, agent episode reward: [-39.15211147354518], time: 66.841
steps: 4161268, episodes: 99000, mean episode reward: -39.121866056788534, num_cumulative_constraints: 54454, agent episode reward: [-39.121866056788534], time: 61.609
steps: 4201883, episodes: 100000, mean episode reward: -39.36625765287765, num_cumulative_constraints: 54639, agent episode reward: [-39.36625765287765], time: 67.817
steps: 4242512, episodes: 101000, mean episode reward: -39.35468933163785, num_cumulative_constraints: 54861, agent episode reward: [-39.35468933163785], time: 68.408
steps: 4283360, episodes: 102000, mean episode reward: -39.39984126659922, num_cumulative_constraints: 55081, agent episode reward: [-39.39984126659922], time: 69.412
steps: 4323988, episodes: 103000, mean episode reward: -39.26173590111568, num_cumulative_constraints: 55240, agent episode reward: [-39.26173590111568], time: 68.516
steps: 4364662, episodes: 104000, mean episode reward: -39.32940676723484, num_cumulative_constraints: 55440, agent episode reward: [-39.32940676723484], time: 64.909
steps: 4405262, episodes: 105000, mean episode reward: -39.251411793098725, num_cumulative_constraints: 55632, agent episode reward: [-39.251411793098725], time: 69.727
steps: 4445681, episodes: 106000, mean episode reward: -38.99754376253183, num_cumulative_constraints: 55784, agent episode reward: [-38.99754376253183], time: 71.331
steps: 4486131, episodes: 107000, mean episode reward: -39.02901466775507, num_cumulative_constraints: 55914, agent episode reward: [-39.02901466775507], time: 61.19
steps: 4526631, episodes: 108000, mean episode reward: -39.262196878326485, num_cumulative_constraints: 56110, agent episode reward: [-39.262196878326485], time: 72.796
steps: 4567141, episodes: 109000, mean episode reward: -39.03552057269757, num_cumulative_constraints: 56243, agent episode reward: [-39.03552057269757], time: 65.229
steps: 4607629, episodes: 110000, mean episode reward: -39.02343299582867, num_cumulative_constraints: 56368, agent episode reward: [-39.02343299582867], time: 72.514
steps: 4648095, episodes: 111000, mean episode reward: -39.37166395648796, num_cumulative_constraints: 56607, agent episode reward: [-39.37166395648796], time: 70.952
steps: 4688595, episodes: 112000, mean episode reward: -39.2813110602544, num_cumulative_constraints: 56795, agent episode reward: [-39.2813110602544], time: 61.525
steps: 4728995, episodes: 113000, mean episode reward: -38.947616011175036, num_cumulative_constraints: 56932, agent episode reward: [-38.947616011175036], time: 65.808
steps: 4769537, episodes: 114000, mean episode reward: -39.11319627782985, num_cumulative_constraints: 57100, agent episode reward: [-39.11319627782985], time: 62.862
steps: 4809929, episodes: 115000, mean episode reward: -39.11511491838835, num_cumulative_constraints: 57284, agent episode reward: [-39.11511491838835], time: 62.466
steps: 4850424, episodes: 116000, mean episode reward: -39.277023199512506, num_cumulative_constraints: 57512, agent episode reward: [-39.277023199512506], time: 70.498
steps: 4890849, episodes: 117000, mean episode reward: -39.33270986852026, num_cumulative_constraints: 57770, agent episode reward: [-39.33270986852026], time: 52.94
steps: 4931255, episodes: 118000, mean episode reward: -39.079108247847515, num_cumulative_constraints: 57958, agent episode reward: [-39.079108247847515], time: 69.735
steps: 4971643, episodes: 119000, mean episode reward: -39.23855727858844, num_cumulative_constraints: 58189, agent episode reward: [-39.23855727858844], time: 60.048
steps: 5012169, episodes: 120000, mean episode reward: -39.37673556681124, num_cumulative_constraints: 58452, agent episode reward: [-39.37673556681124], time: 51.512
steps: 5052771, episodes: 121000, mean episode reward: -39.46199374732022, num_cumulative_constraints: 58674, agent episode reward: [-39.46199374732022], time: 61.85
steps: 5093219, episodes: 122000, mean episode reward: -39.14800500572298, num_cumulative_constraints: 58878, agent episode reward: [-39.14800500572298], time: 65.797
steps: 5133853, episodes: 123000, mean episode reward: -39.39723012425618, num_cumulative_constraints: 59076, agent episode reward: [-39.39723012425618], time: 56.56
steps: 5174312, episodes: 124000, mean episode reward: -39.16851342878922, num_cumulative_constraints: 59272, agent episode reward: [-39.16851342878922], time: 62.78
steps: 5214945, episodes: 125000, mean episode reward: -39.405381259701535, num_cumulative_constraints: 59466, agent episode reward: [-39.405381259701535], time: 70.988
steps: 5255360, episodes: 126000, mean episode reward: -39.17872228049896, num_cumulative_constraints: 59688, agent episode reward: [-39.17872228049896], time: 58.386
steps: 5295907, episodes: 127000, mean episode reward: -39.28666429099341, num_cumulative_constraints: 59853, agent episode reward: [-39.28666429099341], time: 57.264
steps: 5336485, episodes: 128000, mean episode reward: -39.327045641368336, num_cumulative_constraints: 60059, agent episode reward: [-39.327045641368336], time: 62.725
steps: 5376791, episodes: 129000, mean episode reward: -38.92240985067996, num_cumulative_constraints: 60210, agent episode reward: [-38.92240985067996], time: 73.089
steps: 5417203, episodes: 130000, mean episode reward: -39.24980759147553, num_cumulative_constraints: 60429, agent episode reward: [-39.24980759147553], time: 69.817
steps: 5457573, episodes: 131000, mean episode reward: -39.15086018428734, num_cumulative_constraints: 60645, agent episode reward: [-39.15086018428734], time: 60.111
steps: 5497992, episodes: 132000, mean episode reward: -39.29368236897365, num_cumulative_constraints: 60845, agent episode reward: [-39.29368236897365], time: 61.374
steps: 5538379, episodes: 133000, mean episode reward: -38.965589317991444, num_cumulative_constraints: 61021, agent episode reward: [-38.965589317991444], time: 60.294
steps: 5578889, episodes: 134000, mean episode reward: -39.43988277833442, num_cumulative_constraints: 61285, agent episode reward: [-39.43988277833442], time: 57.062
steps: 5619387, episodes: 135000, mean episode reward: -39.256685144558524, num_cumulative_constraints: 61489, agent episode reward: [-39.256685144558524], time: 67.347
steps: 5659845, episodes: 136000, mean episode reward: -39.21141562383667, num_cumulative_constraints: 61681, agent episode reward: [-39.21141562383667], time: 70.298
steps: 5700337, episodes: 137000, mean episode reward: -39.10475354842008, num_cumulative_constraints: 61846, agent episode reward: [-39.10475354842008], time: 69.774
steps: 5740951, episodes: 138000, mean episode reward: -39.3509932283063, num_cumulative_constraints: 62063, agent episode reward: [-39.3509932283063], time: 69.574
steps: 5781551, episodes: 139000, mean episode reward: -39.19233081177651, num_cumulative_constraints: 62254, agent episode reward: [-39.19233081177651], time: 70.707
steps: 5822199, episodes: 140000, mean episode reward: -39.29183525963286, num_cumulative_constraints: 62415, agent episode reward: [-39.29183525963286], time: 76.235
steps: 5862838, episodes: 141000, mean episode reward: -39.14790729463646, num_cumulative_constraints: 62583, agent episode reward: [-39.14790729463646], time: 70.983
steps: 5903621, episodes: 142000, mean episode reward: -39.385817103166715, num_cumulative_constraints: 62752, agent episode reward: [-39.385817103166715], time: 69.711
steps: 5944143, episodes: 143000, mean episode reward: -39.242283891431775, num_cumulative_constraints: 62973, agent episode reward: [-39.242283891431775], time: 76.145
steps: 5984717, episodes: 144000, mean episode reward: -39.17114109894837, num_cumulative_constraints: 63130, agent episode reward: [-39.17114109894837], time: 72.939
steps: 6025103, episodes: 145000, mean episode reward: -38.98610339168296, num_cumulative_constraints: 63295, agent episode reward: [-38.98610339168296], time: 75.009
steps: 6065521, episodes: 146000, mean episode reward: -39.044961370362174, num_cumulative_constraints: 63440, agent episode reward: [-39.044961370362174], time: 63.998
steps: 6105998, episodes: 147000, mean episode reward: -39.20358311322561, num_cumulative_constraints: 63647, agent episode reward: [-39.20358311322561], time: 55.38
steps: 6146298, episodes: 148000, mean episode reward: -39.0465552420571, num_cumulative_constraints: 63858, agent episode reward: [-39.0465552420571], time: 60.155
steps: 6186583, episodes: 149000, mean episode reward: -39.2713142020725, num_cumulative_constraints: 64131, agent episode reward: [-39.2713142020725], time: 61.072
steps: 6226985, episodes: 150000, mean episode reward: -39.239084169968955, num_cumulative_constraints: 64351, agent episode reward: [-39.239084169968955], time: 62.341
steps: 6267374, episodes: 151000, mean episode reward: -39.21709711774545, num_cumulative_constraints: 64562, agent episode reward: [-39.21709711774545], time: 59.657
steps: 6307729, episodes: 152000, mean episode reward: -39.058855003728894, num_cumulative_constraints: 64762, agent episode reward: [-39.058855003728894], time: 59.758
steps: 6348245, episodes: 153000, mean episode reward: -39.21786702832472, num_cumulative_constraints: 64975, agent episode reward: [-39.21786702832472], time: 60.108
steps: 6388746, episodes: 154000, mean episode reward: -39.2298264966852, num_cumulative_constraints: 65205, agent episode reward: [-39.2298264966852], time: 63.543
steps: 6429153, episodes: 155000, mean episode reward: -39.26735027562967, num_cumulative_constraints: 65450, agent episode reward: [-39.26735027562967], time: 69.499
steps: 6469618, episodes: 156000, mean episode reward: -39.24454633198772, num_cumulative_constraints: 65670, agent episode reward: [-39.24454633198772], time: 70.682
steps: 6510338, episodes: 157000, mean episode reward: -39.32752843509118, num_cumulative_constraints: 65880, agent episode reward: [-39.32752843509118], time: 61.402
steps: 6550917, episodes: 158000, mean episode reward: -39.40246053633559, num_cumulative_constraints: 66143, agent episode reward: [-39.40246053633559], time: 68.299
steps: 6591746, episodes: 159000, mean episode reward: -39.2963022762905, num_cumulative_constraints: 66339, agent episode reward: [-39.2963022762905], time: 62.139
steps: 6632226, episodes: 160000, mean episode reward: -39.2321101064402, num_cumulative_constraints: 66546, agent episode reward: [-39.2321101064402], time: 69.107
steps: 6672799, episodes: 161000, mean episode reward: -39.05324079111658, num_cumulative_constraints: 66694, agent episode reward: [-39.05324079111658], time: 54.255
steps: 6713455, episodes: 162000, mean episode reward: -39.45705927855013, num_cumulative_constraints: 66925, agent episode reward: [-39.45705927855013], time: 65.659
steps: 6754099, episodes: 163000, mean episode reward: -39.0163388733625, num_cumulative_constraints: 67068, agent episode reward: [-39.0163388733625], time: 70.404
steps: 6794712, episodes: 164000, mean episode reward: -39.00019604865096, num_cumulative_constraints: 67202, agent episode reward: [-39.00019604865096], time: 79.769
steps: 6835257, episodes: 165000, mean episode reward: -39.2636188284808, num_cumulative_constraints: 67454, agent episode reward: [-39.2636188284808], time: 71.191
steps: 6875925, episodes: 166000, mean episode reward: -39.35845240216054, num_cumulative_constraints: 67668, agent episode reward: [-39.35845240216054], time: 63.047
steps: 6916640, episodes: 167000, mean episode reward: -39.31053935769802, num_cumulative_constraints: 67884, agent episode reward: [-39.31053935769802], time: 72.239
steps: 6957239, episodes: 168000, mean episode reward: -39.26167105940962, num_cumulative_constraints: 68106, agent episode reward: [-39.26167105940962], time: 65.291
steps: 6997847, episodes: 169000, mean episode reward: -39.2750812695743, num_cumulative_constraints: 68335, agent episode reward: [-39.2750812695743], time: 58.56
steps: 7038367, episodes: 170000, mean episode reward: -39.14619368251227, num_cumulative_constraints: 68519, agent episode reward: [-39.14619368251227], time: 62.506
steps: 7078837, episodes: 171000, mean episode reward: -38.83445804677643, num_cumulative_constraints: 68636, agent episode reward: [-38.83445804677643], time: 64.632
steps: 7119251, episodes: 172000, mean episode reward: -38.95843173866567, num_cumulative_constraints: 68803, agent episode reward: [-38.95843173866567], time: 68.565
steps: 7159909, episodes: 173000, mean episode reward: -39.18707243765107, num_cumulative_constraints: 68939, agent episode reward: [-39.18707243765107], time: 67.112
steps: 7200580, episodes: 174000, mean episode reward: -38.971668845067754, num_cumulative_constraints: 69103, agent episode reward: [-38.971668845067754], time: 59.15
steps: 7241447, episodes: 175000, mean episode reward: -39.33722005221664, num_cumulative_constraints: 69304, agent episode reward: [-39.33722005221664], time: 59.841
steps: 7282079, episodes: 176000, mean episode reward: -38.93901980565901, num_cumulative_constraints: 69479, agent episode reward: [-38.93901980565901], time: 57.915
steps: 7322686, episodes: 177000, mean episode reward: -39.00225739738522, num_cumulative_constraints: 69620, agent episode reward: [-39.00225739738522], time: 63.779
steps: 7363384, episodes: 178000, mean episode reward: -39.16551776785133, num_cumulative_constraints: 69823, agent episode reward: [-39.16551776785133], time: 58.339
steps: 7404189, episodes: 179000, mean episode reward: -39.447518010688, num_cumulative_constraints: 70031, agent episode reward: [-39.447518010688], time: 67.404
steps: 7444764, episodes: 180000, mean episode reward: -39.04328676250326, num_cumulative_constraints: 70171, agent episode reward: [-39.04328676250326], time: 61.016
steps: 7485537, episodes: 181000, mean episode reward: -39.103498896794726, num_cumulative_constraints: 70329, agent episode reward: [-39.103498896794726], time: 56.976
steps: 7526662, episodes: 182000, mean episode reward: -39.32160289747386, num_cumulative_constraints: 70470, agent episode reward: [-39.32160289747386], time: 61.685
steps: 7567295, episodes: 183000, mean episode reward: -38.99133666427563, num_cumulative_constraints: 70633, agent episode reward: [-38.99133666427563], time: 62.644
steps: 7607906, episodes: 184000, mean episode reward: -39.20367760064086, num_cumulative_constraints: 70840, agent episode reward: [-39.20367760064086], time: 64.681
steps: 7648443, episodes: 185000, mean episode reward: -39.12229399538163, num_cumulative_constraints: 71022, agent episode reward: [-39.12229399538163], time: 57.329
steps: 7689156, episodes: 186000, mean episode reward: -39.13151584578131, num_cumulative_constraints: 71187, agent episode reward: [-39.13151584578131], time: 59.683
steps: 7729749, episodes: 187000, mean episode reward: -39.326679577804924, num_cumulative_constraints: 71409, agent episode reward: [-39.326679577804924], time: 60.173
steps: 7770216, episodes: 188000, mean episode reward: -39.09283883990216, num_cumulative_constraints: 71585, agent episode reward: [-39.09283883990216], time: 67.53
steps: 7810660, episodes: 189000, mean episode reward: -39.057943702445215, num_cumulative_constraints: 71769, agent episode reward: [-39.057943702445215], time: 68.997
steps: 7851084, episodes: 190000, mean episode reward: -39.09660884946512, num_cumulative_constraints: 71955, agent episode reward: [-39.09660884946512], time: 69.316
steps: 7891590, episodes: 191000, mean episode reward: -39.20901251562715, num_cumulative_constraints: 72175, agent episode reward: [-39.20901251562715], time: 66.902
steps: 7932115, episodes: 192000, mean episode reward: -39.23055310266011, num_cumulative_constraints: 72377, agent episode reward: [-39.23055310266011], time: 72.698
steps: 7972521, episodes: 193000, mean episode reward: -39.076281836868965, num_cumulative_constraints: 72599, agent episode reward: [-39.076281836868965], time: 59.768
steps: 8013032, episodes: 194000, mean episode reward: -39.21152083972957, num_cumulative_constraints: 72797, agent episode reward: [-39.21152083972957], time: 69.788
steps: 8053555, episodes: 195000, mean episode reward: -39.109697997446595, num_cumulative_constraints: 72983, agent episode reward: [-39.109697997446595], time: 70.449
steps: 8094109, episodes: 196000, mean episode reward: -39.16838511257976, num_cumulative_constraints: 73183, agent episode reward: [-39.16838511257976], time: 59.51
steps: 8134656, episodes: 197000, mean episode reward: -39.074701923943394, num_cumulative_constraints: 73359, agent episode reward: [-39.074701923943394], time: 70.757
steps: 8175227, episodes: 198000, mean episode reward: -39.21000246889623, num_cumulative_constraints: 73571, agent episode reward: [-39.21000246889623], time: 61.77
steps: 8215872, episodes: 199000, mean episode reward: -39.20978089310651, num_cumulative_constraints: 73731, agent episode reward: [-39.20978089310651], time: 58.692
steps: 8256459, episodes: 200000, mean episode reward: -39.13668162138794, num_cumulative_constraints: 73886, agent episode reward: [-39.13668162138794], time: 65.859
steps: 8296989, episodes: 201000, mean episode reward: -39.239524270652566, num_cumulative_constraints: 74084, agent episode reward: [-39.239524270652566], time: 67.7
steps: 8337557, episodes: 202000, mean episode reward: -39.32905990197217, num_cumulative_constraints: 74301, agent episode reward: [-39.32905990197217], time: 60.301
steps: 8378069, episodes: 203000, mean episode reward: -39.31605550132861, num_cumulative_constraints: 74543, agent episode reward: [-39.31605550132861], time: 63.256
steps: 8418576, episodes: 204000, mean episode reward: -39.38030631453723, num_cumulative_constraints: 74799, agent episode reward: [-39.38030631453723], time: 66.599
steps: 8459108, episodes: 205000, mean episode reward: -39.165108866964964, num_cumulative_constraints: 74992, agent episode reward: [-39.165108866964964], time: 61.039
steps: 8499591, episodes: 206000, mean episode reward: -39.1498115719459, num_cumulative_constraints: 75228, agent episode reward: [-39.1498115719459], time: 62.531
steps: 8540189, episodes: 207000, mean episode reward: -39.00165007648448, num_cumulative_constraints: 75352, agent episode reward: [-39.00165007648448], time: 62.827
steps: 8580822, episodes: 208000, mean episode reward: -39.24922333890505, num_cumulative_constraints: 75527, agent episode reward: [-39.24922333890505], time: 61.986
steps: 8621414, episodes: 209000, mean episode reward: -39.12534521149418, num_cumulative_constraints: 75682, agent episode reward: [-39.12534521149418], time: 61.542
steps: 8662218, episodes: 210000, mean episode reward: -39.215493206322044, num_cumulative_constraints: 75829, agent episode reward: [-39.215493206322044], time: 65.371
steps: 8703022, episodes: 211000, mean episode reward: -39.19714505034937, num_cumulative_constraints: 75958, agent episode reward: [-39.19714505034937], time: 68.672
steps: 8743663, episodes: 212000, mean episode reward: -39.20337941929488, num_cumulative_constraints: 76124, agent episode reward: [-39.20337941929488], time: 67.448
steps: 8784295, episodes: 213000, mean episode reward: -39.20418661171904, num_cumulative_constraints: 76291, agent episode reward: [-39.20418661171904], time: 62.602
steps: 8824990, episodes: 214000, mean episode reward: -39.2104506118116, num_cumulative_constraints: 76436, agent episode reward: [-39.2104506118116], time: 66.976
steps: 8865798, episodes: 215000, mean episode reward: -39.134795514703015, num_cumulative_constraints: 76572, agent episode reward: [-39.134795514703015], time: 65.685
steps: 8906556, episodes: 216000, mean episode reward: -39.38871936066257, num_cumulative_constraints: 76770, agent episode reward: [-39.38871936066257], time: 64.554
steps: 8947459, episodes: 217000, mean episode reward: -39.367280680985395, num_cumulative_constraints: 76914, agent episode reward: [-39.367280680985395], time: 74.576
steps: 8988259, episodes: 218000, mean episode reward: -39.240894167650445, num_cumulative_constraints: 77062, agent episode reward: [-39.240894167650445], time: 71.83
steps: 9028943, episodes: 219000, mean episode reward: -39.25892809648659, num_cumulative_constraints: 77233, agent episode reward: [-39.25892809648659], time: 62.112
steps: 9069745, episodes: 220000, mean episode reward: -39.220701026969685, num_cumulative_constraints: 77386, agent episode reward: [-39.220701026969685], time: 60.846
steps: 9110298, episodes: 221000, mean episode reward: -39.159998028588326, num_cumulative_constraints: 77582, agent episode reward: [-39.159998028588326], time: 66.107
steps: 9151044, episodes: 222000, mean episode reward: -39.069159037575794, num_cumulative_constraints: 77731, agent episode reward: [-39.069159037575794], time: 58.245
steps: 9191505, episodes: 223000, mean episode reward: -38.997103588531864, num_cumulative_constraints: 77899, agent episode reward: [-38.997103588531864], time: 67.149
steps: 9232072, episodes: 224000, mean episode reward: -39.1853565719599, num_cumulative_constraints: 78095, agent episode reward: [-39.1853565719599], time: 59.479
steps: 9272926, episodes: 225000, mean episode reward: -39.43596408609512, num_cumulative_constraints: 78294, agent episode reward: [-39.43596408609512], time: 67.476
steps: 9313494, episodes: 226000, mean episode reward: -39.2679553005796, num_cumulative_constraints: 78519, agent episode reward: [-39.2679553005796], time: 59.487
steps: 9354147, episodes: 227000, mean episode reward: -39.208176120984646, num_cumulative_constraints: 78720, agent episode reward: [-39.208176120984646], time: 56.131
steps: 9394908, episodes: 228000, mean episode reward: -39.23901232824295, num_cumulative_constraints: 78918, agent episode reward: [-39.23901232824295], time: 60.111
steps: 9435646, episodes: 229000, mean episode reward: -39.401485337976425, num_cumulative_constraints: 79166, agent episode reward: [-39.401485337976425], time: 66.22
steps: 9476423, episodes: 230000, mean episode reward: -39.50367030386161, num_cumulative_constraints: 79394, agent episode reward: [-39.50367030386161], time: 77.803
steps: 9517096, episodes: 231000, mean episode reward: -39.39730614402979, num_cumulative_constraints: 79609, agent episode reward: [-39.39730614402979], time: 71.564
steps: 9557698, episodes: 232000, mean episode reward: -39.36504898261602, num_cumulative_constraints: 79870, agent episode reward: [-39.36504898261602], time: 65.895
steps: 9598375, episodes: 233000, mean episode reward: -39.172607282196445, num_cumulative_constraints: 80076, agent episode reward: [-39.172607282196445], time: 57.003
steps: 9639302, episodes: 234000, mean episode reward: -39.70139360524296, num_cumulative_constraints: 80342, agent episode reward: [-39.70139360524296], time: 55.147
steps: 9680135, episodes: 235000, mean episode reward: -39.486017629184175, num_cumulative_constraints: 80553, agent episode reward: [-39.486017629184175], time: 61.28
steps: 9720900, episodes: 236000, mean episode reward: -39.62308962009745, num_cumulative_constraints: 80837, agent episode reward: [-39.62308962009745], time: 63.559
steps: 9761763, episodes: 237000, mean episode reward: -39.534929803359, num_cumulative_constraints: 81047, agent episode reward: [-39.534929803359], time: 65.469
steps: 9802593, episodes: 238000, mean episode reward: -39.33280401787924, num_cumulative_constraints: 81237, agent episode reward: [-39.33280401787924], time: 67.747
steps: 9843389, episodes: 239000, mean episode reward: -39.42399969663214, num_cumulative_constraints: 81446, agent episode reward: [-39.42399969663214], time: 69.281
steps: 9884183, episodes: 240000, mean episode reward: -39.38237036207275, num_cumulative_constraints: 81635, agent episode reward: [-39.38237036207275], time: 51.684
steps: 9924970, episodes: 241000, mean episode reward: -39.51277099689799, num_cumulative_constraints: 81847, agent episode reward: [-39.51277099689799], time: 64.091
steps: 9965703, episodes: 242000, mean episode reward: -39.34271063678467, num_cumulative_constraints: 82045, agent episode reward: [-39.34271063678467], time: 51.59
steps: 10006469, episodes: 243000, mean episode reward: -39.2625753952168, num_cumulative_constraints: 82185, agent episode reward: [-39.2625753952168], time: 59.51
steps: 10047334, episodes: 244000, mean episode reward: -39.46582677946147, num_cumulative_constraints: 82369, agent episode reward: [-39.46582677946147], time: 59.775
steps: 10088128, episodes: 245000, mean episode reward: -39.38437368543085, num_cumulative_constraints: 82556, agent episode reward: [-39.38437368543085], time: 63.086
steps: 10128679, episodes: 246000, mean episode reward: -39.32615371007091, num_cumulative_constraints: 82777, agent episode reward: [-39.32615371007091], time: 61.394
steps: 10169151, episodes: 247000, mean episode reward: -39.12759144456884, num_cumulative_constraints: 82947, agent episode reward: [-39.12759144456884], time: 53.108
steps: 10209686, episodes: 248000, mean episode reward: -39.308945942129505, num_cumulative_constraints: 83159, agent episode reward: [-39.308945942129505], time: 51.522
steps: 10250261, episodes: 249000, mean episode reward: -39.21569420132249, num_cumulative_constraints: 83314, agent episode reward: [-39.21569420132249], time: 56.298
steps: 10290770, episodes: 250000, mean episode reward: -39.00426173588386, num_cumulative_constraints: 83436, agent episode reward: [-39.00426173588386], time: 63.652
steps: 10331418, episodes: 251000, mean episode reward: -39.35027781761529, num_cumulative_constraints: 83618, agent episode reward: [-39.35027781761529], time: 64.003
steps: 10372166, episodes: 252000, mean episode reward: -39.491567292789185, num_cumulative_constraints: 83842, agent episode reward: [-39.491567292789185], time: 67.72
steps: 10412802, episodes: 253000, mean episode reward: -39.22470412741993, num_cumulative_constraints: 84031, agent episode reward: [-39.22470412741993], time: 67.85
steps: 10453427, episodes: 254000, mean episode reward: -39.03602747985092, num_cumulative_constraints: 84167, agent episode reward: [-39.03602747985092], time: 70.324
steps: 10494043, episodes: 255000, mean episode reward: -39.07434456427491, num_cumulative_constraints: 84289, agent episode reward: [-39.07434456427491], time: 68.56
steps: 10534768, episodes: 256000, mean episode reward: -39.29749150683909, num_cumulative_constraints: 84460, agent episode reward: [-39.29749150683909], time: 61.15
steps: 10575618, episodes: 257000, mean episode reward: -39.320231733197, num_cumulative_constraints: 84637, agent episode reward: [-39.320231733197], time: 58.925
steps: 10616680, episodes: 258000, mean episode reward: -39.46099905222931, num_cumulative_constraints: 84805, agent episode reward: [-39.46099905222931], time: 64.512
steps: 10657608, episodes: 259000, mean episode reward: -39.36773175358199, num_cumulative_constraints: 84962, agent episode reward: [-39.36773175358199], time: 68.349
steps: 10698597, episodes: 260000, mean episode reward: -39.35407936431937, num_cumulative_constraints: 85157, agent episode reward: [-39.35407936431937], time: 65.989
steps: 10739848, episodes: 261000, mean episode reward: -39.571510551077054, num_cumulative_constraints: 85365, agent episode reward: [-39.571510551077054], time: 64.048
steps: 10781834, episodes: 262000, mean episode reward: -39.72291099400607, num_cumulative_constraints: 85507, agent episode reward: [-39.72291099400607], time: 67.849
steps: 10823920, episodes: 263000, mean episode reward: -39.842062680787755, num_cumulative_constraints: 85694, agent episode reward: [-39.842062680787755], time: 62.686
steps: 10865154, episodes: 264000, mean episode reward: -39.73081872791562, num_cumulative_constraints: 85958, agent episode reward: [-39.73081872791562], time: 69.475
steps: 10906333, episodes: 265000, mean episode reward: -39.44019113808055, num_cumulative_constraints: 86163, agent episode reward: [-39.44019113808055], time: 62.417
steps: 10947459, episodes: 266000, mean episode reward: -39.359121355244326, num_cumulative_constraints: 86302, agent episode reward: [-39.359121355244326], time: 59.167
steps: 10988552, episodes: 267000, mean episode reward: -39.56675099458677, num_cumulative_constraints: 86532, agent episode reward: [-39.56675099458677], time: 54.64
steps: 11029640, episodes: 268000, mean episode reward: -39.60977783933931, num_cumulative_constraints: 86730, agent episode reward: [-39.60977783933931], time: 58.916
steps: 11070775, episodes: 269000, mean episode reward: -39.38876979041481, num_cumulative_constraints: 86899, agent episode reward: [-39.38876979041481], time: 56.861
steps: 11111793, episodes: 270000, mean episode reward: -39.450563751017896, num_cumulative_constraints: 87084, agent episode reward: [-39.450563751017896], time: 53.284
steps: 11152998, episodes: 271000, mean episode reward: -39.454850831624135, num_cumulative_constraints: 87250, agent episode reward: [-39.454850831624135], time: 58.473
steps: 11194056, episodes: 272000, mean episode reward: -39.33063980699993, num_cumulative_constraints: 87411, agent episode reward: [-39.33063980699993], time: 60.45
steps: 11235225, episodes: 273000, mean episode reward: -39.5003442950834, num_cumulative_constraints: 87615, agent episode reward: [-39.5003442950834], time: 71.832
steps: 11276245, episodes: 274000, mean episode reward: -39.317729903180926, num_cumulative_constraints: 87763, agent episode reward: [-39.317729903180926], time: 64.367
steps: 11317703, episodes: 275000, mean episode reward: -39.68081938061485, num_cumulative_constraints: 87955, agent episode reward: [-39.68081938061485], time: 64.821
steps: 11359043, episodes: 276000, mean episode reward: -39.594011285277475, num_cumulative_constraints: 88139, agent episode reward: [-39.594011285277475], time: 58.348
steps: 11400451, episodes: 277000, mean episode reward: -39.642155828234294, num_cumulative_constraints: 88324, agent episode reward: [-39.642155828234294], time: 72.302
steps: 11441939, episodes: 278000, mean episode reward: -39.73631931233694, num_cumulative_constraints: 88507, agent episode reward: [-39.73631931233694], time: 67.343
steps: 11483489, episodes: 279000, mean episode reward: -39.63390181754797, num_cumulative_constraints: 88667, agent episode reward: [-39.63390181754797], time: 58.426
steps: 11525535, episodes: 280000, mean episode reward: -39.94056110955412, num_cumulative_constraints: 88844, agent episode reward: [-39.94056110955412], time: 65.89
steps: 11567446, episodes: 281000, mean episode reward: -39.76243131593709, num_cumulative_constraints: 89020, agent episode reward: [-39.76243131593709], time: 63.248
steps: 11609629, episodes: 282000, mean episode reward: -40.113272176424275, num_cumulative_constraints: 89238, agent episode reward: [-40.113272176424275], time: 67.702
steps: 11651567, episodes: 283000, mean episode reward: -39.42669710100406, num_cumulative_constraints: 89351, agent episode reward: [-39.42669710100406], time: 58.322
steps: 11693465, episodes: 284000, mean episode reward: -40.00402282424381, num_cumulative_constraints: 89600, agent episode reward: [-40.00402282424381], time: 70.82
steps: 11735375, episodes: 285000, mean episode reward: -39.837477337198905, num_cumulative_constraints: 89800, agent episode reward: [-39.837477337198905], time: 67.294
steps: 11777245, episodes: 286000, mean episode reward: -39.92202454011712, num_cumulative_constraints: 90018, agent episode reward: [-39.92202454011712], time: 62.718
steps: 11819042, episodes: 287000, mean episode reward: -39.64317956382455, num_cumulative_constraints: 90199, agent episode reward: [-39.64317956382455], time: 67.369
steps: 11860919, episodes: 288000, mean episode reward: -39.88844193455841, num_cumulative_constraints: 90430, agent episode reward: [-39.88844193455841], time: 54.533
steps: 11902558, episodes: 289000, mean episode reward: -39.55155440353358, num_cumulative_constraints: 90630, agent episode reward: [-39.55155440353358], time: 59.585
steps: 11944202, episodes: 290000, mean episode reward: -40.10089252879181, num_cumulative_constraints: 90904, agent episode reward: [-40.10089252879181], time: 64.85
steps: 11985542, episodes: 291000, mean episode reward: -39.52826530964234, num_cumulative_constraints: 91073, agent episode reward: [-39.52826530964234], time: 56.413
steps: 12026700, episodes: 292000, mean episode reward: -39.554538264634964, num_cumulative_constraints: 91287, agent episode reward: [-39.554538264634964], time: 55.915
steps: 12067591, episodes: 293000, mean episode reward: -39.41298236750338, num_cumulative_constraints: 91488, agent episode reward: [-39.41298236750338], time: 50.899
steps: 12108716, episodes: 294000, mean episode reward: -39.55196293952305, num_cumulative_constraints: 91694, agent episode reward: [-39.55196293952305], time: 60.558
steps: 12149916, episodes: 295000, mean episode reward: -39.43151985684999, num_cumulative_constraints: 91866, agent episode reward: [-39.43151985684999], time: 62.655
steps: 12190770, episodes: 296000, mean episode reward: -39.299535105200064, num_cumulative_constraints: 92039, agent episode reward: [-39.299535105200064], time: 63.53
steps: 12231895, episodes: 297000, mean episode reward: -39.61661623110974, num_cumulative_constraints: 92253, agent episode reward: [-39.61661623110974], time: 59.693
steps: 12272784, episodes: 298000, mean episode reward: -39.379937334074526, num_cumulative_constraints: 92461, agent episode reward: [-39.379937334074526], time: 65.782
steps: 12313786, episodes: 299000, mean episode reward: -39.598722864430854, num_cumulative_constraints: 92658, agent episode reward: [-39.598722864430854], time: 61.478
steps: 12354582, episodes: 300000, mean episode reward: -39.44941495508122, num_cumulative_constraints: 92876, agent episode reward: [-39.44941495508122], time: 61.336
steps: 12395341, episodes: 301000, mean episode reward: -39.44491393129491, num_cumulative_constraints: 93070, agent episode reward: [-39.44491393129491], time: 59.788
steps: 12435938, episodes: 302000, mean episode reward: -39.24248430602035, num_cumulative_constraints: 93243, agent episode reward: [-39.24248430602035], time: 65.755
steps: 12476383, episodes: 303000, mean episode reward: -39.16935228122446, num_cumulative_constraints: 93428, agent episode reward: [-39.16935228122446], time: 57.716
steps: 12516981, episodes: 304000, mean episode reward: -39.20024422834314, num_cumulative_constraints: 93579, agent episode reward: [-39.20024422834314], time: 60.758
steps: 12557604, episodes: 305000, mean episode reward: -39.427137318375756, num_cumulative_constraints: 93800, agent episode reward: [-39.427137318375756], time: 55.026
steps: 12598238, episodes: 306000, mean episode reward: -39.281988354397164, num_cumulative_constraints: 93982, agent episode reward: [-39.281988354397164], time: 60.313
steps: 12638781, episodes: 307000, mean episode reward: -39.0450270020731, num_cumulative_constraints: 94106, agent episode reward: [-39.0450270020731], time: 58.854
steps: 12679360, episodes: 308000, mean episode reward: -39.17136665273891, num_cumulative_constraints: 94303, agent episode reward: [-39.17136665273891], time: 59.839
steps: 12719815, episodes: 309000, mean episode reward: -39.101363487638984, num_cumulative_constraints: 94523, agent episode reward: [-39.101363487638984], time: 54.433
steps: 12760624, episodes: 310000, mean episode reward: -39.371631022433014, num_cumulative_constraints: 94684, agent episode reward: [-39.371631022433014], time: 55.704
steps: 12801270, episodes: 311000, mean episode reward: -39.10934915273175, num_cumulative_constraints: 94836, agent episode reward: [-39.10934915273175], time: 58.349
steps: 12842143, episodes: 312000, mean episode reward: -39.36273687833458, num_cumulative_constraints: 95009, agent episode reward: [-39.36273687833458], time: 64.769
steps: 12882713, episodes: 313000, mean episode reward: -39.103383615608514, num_cumulative_constraints: 95155, agent episode reward: [-39.103383615608514], time: 55.066
steps: 12923291, episodes: 314000, mean episode reward: -39.204932951418684, num_cumulative_constraints: 95332, agent episode reward: [-39.204932951418684], time: 57.011
steps: 12963764, episodes: 315000, mean episode reward: -39.12470383941059, num_cumulative_constraints: 95506, agent episode reward: [-39.12470383941059], time: 56.946
steps: 13004324, episodes: 316000, mean episode reward: -39.22274941105797, num_cumulative_constraints: 95691, agent episode reward: [-39.22274941105797], time: 62.756
steps: 13044897, episodes: 317000, mean episode reward: -39.134528839047505, num_cumulative_constraints: 95852, agent episode reward: [-39.134528839047505], time: 61.771
steps: 13085216, episodes: 318000, mean episode reward: -38.90961988805017, num_cumulative_constraints: 96025, agent episode reward: [-38.90961988805017], time: 67.131
steps: 13125650, episodes: 319000, mean episode reward: -39.2837985142339, num_cumulative_constraints: 96272, agent episode reward: [-39.2837985142339], time: 68.167
steps: 13166025, episodes: 320000, mean episode reward: -38.9730617247635, num_cumulative_constraints: 96450, agent episode reward: [-38.9730617247635], time: 69.056
steps: 13206515, episodes: 321000, mean episode reward: -38.90385238117791, num_cumulative_constraints: 96564, agent episode reward: [-38.90385238117791], time: 53.17
steps: 13247095, episodes: 322000, mean episode reward: -39.33861799873342, num_cumulative_constraints: 96761, agent episode reward: [-39.33861799873342], time: 59.996
steps: 13287578, episodes: 323000, mean episode reward: -39.103551545457115, num_cumulative_constraints: 96932, agent episode reward: [-39.103551545457115], time: 55.545
steps: 13328040, episodes: 324000, mean episode reward: -39.217577738645524, num_cumulative_constraints: 97154, agent episode reward: [-39.217577738645524], time: 57.171
steps: 13368579, episodes: 325000, mean episode reward: -39.23485266190231, num_cumulative_constraints: 97348, agent episode reward: [-39.23485266190231], time: 61.822
steps: 13408987, episodes: 326000, mean episode reward: -39.2358435044272, num_cumulative_constraints: 97585, agent episode reward: [-39.2358435044272], time: 56.262
steps: 13449520, episodes: 327000, mean episode reward: -39.240390773608816, num_cumulative_constraints: 97796, agent episode reward: [-39.240390773608816], time: 60.134
steps: 13490251, episodes: 328000, mean episode reward: -39.52099609774436, num_cumulative_constraints: 98066, agent episode reward: [-39.52099609774436], time: 66.798
steps: 13531022, episodes: 329000, mean episode reward: -39.35502078448378, num_cumulative_constraints: 98268, agent episode reward: [-39.35502078448378], time: 62.659
steps: 13571728, episodes: 330000, mean episode reward: -39.18757320902047, num_cumulative_constraints: 98414, agent episode reward: [-39.18757320902047], time: 58.572
steps: 13612286, episodes: 331000, mean episode reward: -39.07523454872231, num_cumulative_constraints: 98577, agent episode reward: [-39.07523454872231], time: 62.407
steps: 13652944, episodes: 332000, mean episode reward: -39.18320440919878, num_cumulative_constraints: 98762, agent episode reward: [-39.18320440919878], time: 55.571
steps: 13693489, episodes: 333000, mean episode reward: -39.07696854334161, num_cumulative_constraints: 98934, agent episode reward: [-39.07696854334161], time: 69.363
steps: 13734010, episodes: 334000, mean episode reward: -38.94924942820651, num_cumulative_constraints: 99064, agent episode reward: [-38.94924942820651], time: 63.577
steps: 13774630, episodes: 335000, mean episode reward: -38.91533724459085, num_cumulative_constraints: 99194, agent episode reward: [-38.91533724459085], time: 54.946
steps: 13815325, episodes: 336000, mean episode reward: -39.26958638761606, num_cumulative_constraints: 99390, agent episode reward: [-39.26958638761606], time: 57.744
steps: 13856011, episodes: 337000, mean episode reward: -39.3368225458776, num_cumulative_constraints: 99598, agent episode reward: [-39.3368225458776], time: 63.026
steps: 13896782, episodes: 338000, mean episode reward: -39.173756900895896, num_cumulative_constraints: 99750, agent episode reward: [-39.173756900895896], time: 58.7
steps: 13937324, episodes: 339000, mean episode reward: -38.93601063396063, num_cumulative_constraints: 99873, agent episode reward: [-38.93601063396063], time: 53.89
steps: 13978090, episodes: 340000, mean episode reward: -39.29349711259146, num_cumulative_constraints: 100050, agent episode reward: [-39.29349711259146], time: 64.288
steps: 14018700, episodes: 341000, mean episode reward: -39.071772867413415, num_cumulative_constraints: 100231, agent episode reward: [-39.071772867413415], time: 59.585
steps: 14059655, episodes: 342000, mean episode reward: -39.48765189374607, num_cumulative_constraints: 100419, agent episode reward: [-39.48765189374607], time: 68.589
steps: 14100783, episodes: 343000, mean episode reward: -39.54859488605207, num_cumulative_constraints: 100600, agent episode reward: [-39.54859488605207], time: 68.322
steps: 14142294, episodes: 344000, mean episode reward: -39.53287654064863, num_cumulative_constraints: 100791, agent episode reward: [-39.53287654064863], time: 60.98
steps: 14184268, episodes: 345000, mean episode reward: -40.089287797881305, num_cumulative_constraints: 100994, agent episode reward: [-40.089287797881305], time: 60.39
steps: 14226573, episodes: 346000, mean episode reward: -40.234902401790855, num_cumulative_constraints: 101204, agent episode reward: [-40.234902401790855], time: 58.422
steps: 14269796, episodes: 347000, mean episode reward: -40.56587791831156, num_cumulative_constraints: 101390, agent episode reward: [-40.56587791831156], time: 56.05
steps: 14314267, episodes: 348000, mean episode reward: -41.15416445414247, num_cumulative_constraints: 101581, agent episode reward: [-41.15416445414247], time: 66.604
steps: 14358447, episodes: 349000, mean episode reward: -41.29156494964071, num_cumulative_constraints: 101883, agent episode reward: [-41.29156494964071], time: 63.492
steps: 14401778, episodes: 350000, mean episode reward: -40.62576060552831, num_cumulative_constraints: 102133, agent episode reward: [-40.62576060552831], time: 66.64
steps: 14443331, episodes: 351000, mean episode reward: -39.738750530677386, num_cumulative_constraints: 102383, agent episode reward: [-39.738750530677386], time: 63.005
steps: 14484903, episodes: 352000, mean episode reward: -40.08391202410838, num_cumulative_constraints: 102708, agent episode reward: [-40.08391202410838], time: 64.545
steps: 14526057, episodes: 353000, mean episode reward: -39.54833858896066, num_cumulative_constraints: 102906, agent episode reward: [-39.54833858896066], time: 59.48
steps: 14566913, episodes: 354000, mean episode reward: -39.41083180093084, num_cumulative_constraints: 103104, agent episode reward: [-39.41083180093084], time: 59.495
steps: 14607697, episodes: 355000, mean episode reward: -39.24167483004722, num_cumulative_constraints: 103294, agent episode reward: [-39.24167483004722], time: 64.751
steps: 14648789, episodes: 356000, mean episode reward: -39.33666929398978, num_cumulative_constraints: 103420, agent episode reward: [-39.33666929398978], time: 62.891
steps: 14689394, episodes: 357000, mean episode reward: -39.15441476194216, num_cumulative_constraints: 103588, agent episode reward: [-39.15441476194216], time: 65.322
steps: 14730361, episodes: 358000, mean episode reward: -39.308365846183705, num_cumulative_constraints: 103785, agent episode reward: [-39.308365846183705], time: 67.59
steps: 14771175, episodes: 359000, mean episode reward: -39.43908256536573, num_cumulative_constraints: 103999, agent episode reward: [-39.43908256536573], time: 62.39
steps: 14812422, episodes: 360000, mean episode reward: -39.55748681350237, num_cumulative_constraints: 104194, agent episode reward: [-39.55748681350237], time: 71.173
steps: 14853272, episodes: 361000, mean episode reward: -39.40423291089542, num_cumulative_constraints: 104381, agent episode reward: [-39.40423291089542], time: 65.57
steps: 14893889, episodes: 362000, mean episode reward: -39.13096055346007, num_cumulative_constraints: 104539, agent episode reward: [-39.13096055346007], time: 65.681
steps: 14934620, episodes: 363000, mean episode reward: -39.41797500288235, num_cumulative_constraints: 104765, agent episode reward: [-39.41797500288235], time: 57.319
steps: 14975388, episodes: 364000, mean episode reward: -39.30337251987666, num_cumulative_constraints: 104927, agent episode reward: [-39.30337251987666], time: 66.331
steps: 15015878, episodes: 365000, mean episode reward: -38.83919545327933, num_cumulative_constraints: 105055, agent episode reward: [-38.83919545327933], time: 71.431
steps: 15056490, episodes: 366000, mean episode reward: -39.0573768312339, num_cumulative_constraints: 105214, agent episode reward: [-39.0573768312339], time: 59.666
steps: 15097214, episodes: 367000, mean episode reward: -39.270143416436845, num_cumulative_constraints: 105406, agent episode reward: [-39.270143416436845], time: 61.261
steps: 15138154, episodes: 368000, mean episode reward: -39.40851046106716, num_cumulative_constraints: 105570, agent episode reward: [-39.40851046106716], time: 51.86
steps: 15179000, episodes: 369000, mean episode reward: -39.51665417355644, num_cumulative_constraints: 105806, agent episode reward: [-39.51665417355644], time: 66.745
steps: 15219719, episodes: 370000, mean episode reward: -39.17249152450334, num_cumulative_constraints: 105944, agent episode reward: [-39.17249152450334], time: 64.102
steps: 15260545, episodes: 371000, mean episode reward: -39.08391214946653, num_cumulative_constraints: 106037, agent episode reward: [-39.08391214946653], time: 75.527
steps: 15301401, episodes: 372000, mean episode reward: -39.41987745601097, num_cumulative_constraints: 106236, agent episode reward: [-39.41987745601097], time: 79.547
steps: 15342015, episodes: 373000, mean episode reward: -39.07976157281144, num_cumulative_constraints: 106384, agent episode reward: [-39.07976157281144], time: 74.812
steps: 15382772, episodes: 374000, mean episode reward: -39.216760604610656, num_cumulative_constraints: 106560, agent episode reward: [-39.216760604610656], time: 71.775
steps: 15423472, episodes: 375000, mean episode reward: -39.14644633041996, num_cumulative_constraints: 106718, agent episode reward: [-39.14644633041996], time: 75.32
steps: 15464277, episodes: 376000, mean episode reward: -39.32376377955573, num_cumulative_constraints: 106887, agent episode reward: [-39.32376377955573], time: 60.281
steps: 15504989, episodes: 377000, mean episode reward: -39.25910026122608, num_cumulative_constraints: 107068, agent episode reward: [-39.25910026122608], time: 68.063
steps: 15545720, episodes: 378000, mean episode reward: -39.50489913499519, num_cumulative_constraints: 107319, agent episode reward: [-39.50489913499519], time: 58.103
steps: 15586726, episodes: 379000, mean episode reward: -39.67500985491104, num_cumulative_constraints: 107584, agent episode reward: [-39.67500985491104], time: 55.026
steps: 15627821, episodes: 380000, mean episode reward: -39.69673490098462, num_cumulative_constraints: 107843, agent episode reward: [-39.69673490098462], time: 53.64
steps: 15669017, episodes: 381000, mean episode reward: -39.8683921850224, num_cumulative_constraints: 108153, agent episode reward: [-39.8683921850224], time: 58.658
steps: 15710150, episodes: 382000, mean episode reward: -39.60609998376838, num_cumulative_constraints: 108351, agent episode reward: [-39.60609998376838], time: 73.226
steps: 15751460, episodes: 383000, mean episode reward: -39.749682520055735, num_cumulative_constraints: 108569, agent episode reward: [-39.749682520055735], time: 77.82
steps: 15792567, episodes: 384000, mean episode reward: -39.71287747911601, num_cumulative_constraints: 108822, agent episode reward: [-39.71287747911601], time: 68.553
steps: 15833817, episodes: 385000, mean episode reward: -39.46949145314353, num_cumulative_constraints: 108980, agent episode reward: [-39.46949145314353], time: 67.32
steps: 15874955, episodes: 386000, mean episode reward: -39.185931653115695, num_cumulative_constraints: 109122, agent episode reward: [-39.185931653115695], time: 70.578
steps: 15916238, episodes: 387000, mean episode reward: -39.40711051908259, num_cumulative_constraints: 109224, agent episode reward: [-39.40711051908259], time: 63.689
steps: 15957563, episodes: 388000, mean episode reward: -39.64780275811092, num_cumulative_constraints: 109391, agent episode reward: [-39.64780275811092], time: 66.41
steps: 15998862, episodes: 389000, mean episode reward: -39.547096509019994, num_cumulative_constraints: 109535, agent episode reward: [-39.547096509019994], time: 62.41
steps: 16040151, episodes: 390000, mean episode reward: -39.49699134522224, num_cumulative_constraints: 109675, agent episode reward: [-39.49699134522224], time: 62.643
steps: 16081669, episodes: 391000, mean episode reward: -39.59043599596807, num_cumulative_constraints: 109826, agent episode reward: [-39.59043599596807], time: 56.593
steps: 16122922, episodes: 392000, mean episode reward: -39.45052056795326, num_cumulative_constraints: 109987, agent episode reward: [-39.45052056795326], time: 66.442
steps: 16163986, episodes: 393000, mean episode reward: -39.27441268739454, num_cumulative_constraints: 110089, agent episode reward: [-39.27441268739454], time: 60.046
steps: 16204980, episodes: 394000, mean episode reward: -39.2063922389887, num_cumulative_constraints: 110191, agent episode reward: [-39.2063922389887], time: 53.163
steps: 16245967, episodes: 395000, mean episode reward: -39.24674790709368, num_cumulative_constraints: 110315, agent episode reward: [-39.24674790709368], time: 55.886
steps: 16286692, episodes: 396000, mean episode reward: -39.05406538530359, num_cumulative_constraints: 110436, agent episode reward: [-39.05406538530359], time: 65.904
steps: 16327407, episodes: 397000, mean episode reward: -39.35238311624483, num_cumulative_constraints: 110623, agent episode reward: [-39.35238311624483], time: 61.609
steps: 16368081, episodes: 398000, mean episode reward: -39.00554255326465, num_cumulative_constraints: 110748, agent episode reward: [-39.00554255326465], time: 63.86
steps: 16408989, episodes: 399000, mean episode reward: -39.57159645160662, num_cumulative_constraints: 110945, agent episode reward: [-39.57159645160662], time: 63.305
steps: 16449572, episodes: 400000, mean episode reward: -39.07948408864172, num_cumulative_constraints: 111090, agent episode reward: [-39.07948408864172], time: 49.291
steps: 16490034, episodes: 401000, mean episode reward: -38.87908732701572, num_cumulative_constraints: 111228, agent episode reward: [-38.87908732701572], time: 56.234
steps: 16530601, episodes: 402000, mean episode reward: -39.18706528560525, num_cumulative_constraints: 111370, agent episode reward: [-39.18706528560525], time: 56.664
steps: 16571123, episodes: 403000, mean episode reward: -39.02094265454265, num_cumulative_constraints: 111527, agent episode reward: [-39.02094265454265], time: 63.108
steps: 16611667, episodes: 404000, mean episode reward: -39.0612025801596, num_cumulative_constraints: 111664, agent episode reward: [-39.0612025801596], time: 58.512
steps: 16652137, episodes: 405000, mean episode reward: -39.05475177908538, num_cumulative_constraints: 111820, agent episode reward: [-39.05475177908538], time: 64.55
steps: 16692521, episodes: 406000, mean episode reward: -39.11505857252566, num_cumulative_constraints: 112012, agent episode reward: [-39.11505857252566], time: 62.359
steps: 16732907, episodes: 407000, mean episode reward: -38.969522493897095, num_cumulative_constraints: 112151, agent episode reward: [-38.969522493897095], time: 57.475
steps: 16773442, episodes: 408000, mean episode reward: -39.2195367356931, num_cumulative_constraints: 112324, agent episode reward: [-39.2195367356931], time: 66.348
steps: 16813891, episodes: 409000, mean episode reward: -39.11355710788835, num_cumulative_constraints: 112494, agent episode reward: [-39.11355710788835], time: 68.585
steps: 16854214, episodes: 410000, mean episode reward: -38.719678892475294, num_cumulative_constraints: 112604, agent episode reward: [-38.719678892475294], time: 54.92
steps: 16894693, episodes: 411000, mean episode reward: -39.161245420116266, num_cumulative_constraints: 112775, agent episode reward: [-39.161245420116266], time: 54.469
steps: 16935137, episodes: 412000, mean episode reward: -39.39726994151797, num_cumulative_constraints: 113048, agent episode reward: [-39.39726994151797], time: 62.855
steps: 16975576, episodes: 413000, mean episode reward: -39.16025572048457, num_cumulative_constraints: 113217, agent episode reward: [-39.16025572048457], time: 59.825
steps: 17016131, episodes: 414000, mean episode reward: -39.30171217957728, num_cumulative_constraints: 113443, agent episode reward: [-39.30171217957728], time: 67.447
steps: 17056638, episodes: 415000, mean episode reward: -39.026391843655176, num_cumulative_constraints: 113586, agent episode reward: [-39.026391843655176], time: 59.116
steps: 17097192, episodes: 416000, mean episode reward: -39.07615625898355, num_cumulative_constraints: 113740, agent episode reward: [-39.07615625898355], time: 49.634
steps: 17137690, episodes: 417000, mean episode reward: -39.08094620357803, num_cumulative_constraints: 113898, agent episode reward: [-39.08094620357803], time: 54.148
steps: 17178227, episodes: 418000, mean episode reward: -39.190174430224914, num_cumulative_constraints: 114097, agent episode reward: [-39.190174430224914], time: 66.028
steps: 17218826, episodes: 419000, mean episode reward: -39.09155008871107, num_cumulative_constraints: 114281, agent episode reward: [-39.09155008871107], time: 62.13
steps: 17259484, episodes: 420000, mean episode reward: -39.34310232417728, num_cumulative_constraints: 114478, agent episode reward: [-39.34310232417728], time: 54.846
steps: 17299950, episodes: 421000, mean episode reward: -39.01508798348953, num_cumulative_constraints: 114656, agent episode reward: [-39.01508798348953], time: 60.822
steps: 17340624, episodes: 422000, mean episode reward: -39.27023361822511, num_cumulative_constraints: 114836, agent episode reward: [-39.27023361822511], time: 61.727
steps: 17381121, episodes: 423000, mean episode reward: -39.34656095383169, num_cumulative_constraints: 115096, agent episode reward: [-39.34656095383169], time: 65.511
steps: 17421551, episodes: 424000, mean episode reward: -39.131988455043334, num_cumulative_constraints: 115284, agent episode reward: [-39.131988455043334], time: 53.032
steps: 17461994, episodes: 425000, mean episode reward: -39.02882471446823, num_cumulative_constraints: 115464, agent episode reward: [-39.02882471446823], time: 60.879
steps: 17502319, episodes: 426000, mean episode reward: -38.95317913390648, num_cumulative_constraints: 115652, agent episode reward: [-38.95317913390648], time: 61.902
steps: 17542744, episodes: 427000, mean episode reward: -39.12469983411358, num_cumulative_constraints: 115818, agent episode reward: [-39.12469983411358], time: 61.576
steps: 17583241, episodes: 428000, mean episode reward: -39.4594595608935, num_cumulative_constraints: 116095, agent episode reward: [-39.4594595608935], time: 58.011
steps: 17623684, episodes: 429000, mean episode reward: -39.18479775357162, num_cumulative_constraints: 116305, agent episode reward: [-39.18479775357162], time: 55.335
steps: 17664258, episodes: 430000, mean episode reward: -39.14377787804524, num_cumulative_constraints: 116466, agent episode reward: [-39.14377787804524], time: 55.247
steps: 17704715, episodes: 431000, mean episode reward: -39.13285936666493, num_cumulative_constraints: 116640, agent episode reward: [-39.13285936666493], time: 56.065
steps: 17745263, episodes: 432000, mean episode reward: -39.44475401334977, num_cumulative_constraints: 116871, agent episode reward: [-39.44475401334977], time: 52.531
steps: 17785757, episodes: 433000, mean episode reward: -39.21680283707495, num_cumulative_constraints: 117057, agent episode reward: [-39.21680283707495], time: 51.348
steps: 17826111, episodes: 434000, mean episode reward: -39.11985522228601, num_cumulative_constraints: 117262, agent episode reward: [-39.11985522228601], time: 53.643
steps: 17866578, episodes: 435000, mean episode reward: -39.25145062601162, num_cumulative_constraints: 117478, agent episode reward: [-39.25145062601162], time: 61.534
steps: 17907010, episodes: 436000, mean episode reward: -39.112443314554234, num_cumulative_constraints: 117645, agent episode reward: [-39.112443314554234], time: 64.284
steps: 17947585, episodes: 437000, mean episode reward: -39.2126334674704, num_cumulative_constraints: 117801, agent episode reward: [-39.2126334674704], time: 59.623
steps: 17987901, episodes: 438000, mean episode reward: -38.79345587600373, num_cumulative_constraints: 117930, agent episode reward: [-38.79345587600373], time: 60.264
steps: 18028370, episodes: 439000, mean episode reward: -39.08655350303982, num_cumulative_constraints: 118077, agent episode reward: [-39.08655350303982], time: 60.506
steps: 18068835, episodes: 440000, mean episode reward: -38.94358582047677, num_cumulative_constraints: 118186, agent episode reward: [-38.94358582047677], time: 49.116
steps: 18109213, episodes: 441000, mean episode reward: -38.98541397976514, num_cumulative_constraints: 118333, agent episode reward: [-38.98541397976514], time: 52.193
steps: 18149860, episodes: 442000, mean episode reward: -39.42881955622472, num_cumulative_constraints: 118541, agent episode reward: [-39.42881955622472], time: 56.418
steps: 18190373, episodes: 443000, mean episode reward: -39.22865856043519, num_cumulative_constraints: 118752, agent episode reward: [-39.22865856043519], time: 57.659
steps: 18231041, episodes: 444000, mean episode reward: -39.1034873397264, num_cumulative_constraints: 118901, agent episode reward: [-39.1034873397264], time: 58.993
steps: 18271710, episodes: 445000, mean episode reward: -39.10722914375401, num_cumulative_constraints: 119053, agent episode reward: [-39.10722914375401], time: 56.838
steps: 18312537, episodes: 446000, mean episode reward: -39.50233189024389, num_cumulative_constraints: 119264, agent episode reward: [-39.50233189024389], time: 55.544
steps: 18353283, episodes: 447000, mean episode reward: -39.33740415395781, num_cumulative_constraints: 119468, agent episode reward: [-39.33740415395781], time: 58.997
steps: 18394041, episodes: 448000, mean episode reward: -39.35722179807836, num_cumulative_constraints: 119626, agent episode reward: [-39.35722179807836], time: 54.803
steps: 18434649, episodes: 449000, mean episode reward: -39.43512923312803, num_cumulative_constraints: 119853, agent episode reward: [-39.43512923312803], time: 53.075
steps: 18475236, episodes: 450000, mean episode reward: -39.29312866354107, num_cumulative_constraints: 120023, agent episode reward: [-39.29312866354107], time: 60.574
steps: 18515716, episodes: 451000, mean episode reward: -39.2706047971631, num_cumulative_constraints: 120219, agent episode reward: [-39.2706047971631], time: 57.333
steps: 18556247, episodes: 452000, mean episode reward: -39.30681195376908, num_cumulative_constraints: 120406, agent episode reward: [-39.30681195376908], time: 52.318
steps: 18596628, episodes: 453000, mean episode reward: -39.1302768233405, num_cumulative_constraints: 120600, agent episode reward: [-39.1302768233405], time: 55.52
steps: 18637102, episodes: 454000, mean episode reward: -39.3652464058578, num_cumulative_constraints: 120818, agent episode reward: [-39.3652464058578], time: 55.565
steps: 18677511, episodes: 455000, mean episode reward: -39.11734614662912, num_cumulative_constraints: 120963, agent episode reward: [-39.11734614662912], time: 56.55
steps: 18717984, episodes: 456000, mean episode reward: -39.169567863331395, num_cumulative_constraints: 121116, agent episode reward: [-39.169567863331395], time: 59.574
steps: 18758395, episodes: 457000, mean episode reward: -39.1056900084575, num_cumulative_constraints: 121294, agent episode reward: [-39.1056900084575], time: 54.321
steps: 18798897, episodes: 458000, mean episode reward: -39.185174799743784, num_cumulative_constraints: 121483, agent episode reward: [-39.185174799743784], time: 58.215
steps: 18839455, episodes: 459000, mean episode reward: -39.18822370439132, num_cumulative_constraints: 121663, agent episode reward: [-39.18822370439132], time: 63.096
steps: 18880070, episodes: 460000, mean episode reward: -39.45540742788333, num_cumulative_constraints: 121905, agent episode reward: [-39.45540742788333], time: 63.002
steps: 18920514, episodes: 461000, mean episode reward: -39.14671603646659, num_cumulative_constraints: 122090, agent episode reward: [-39.14671603646659], time: 55.95
steps: 18960973, episodes: 462000, mean episode reward: -39.12824090696331, num_cumulative_constraints: 122269, agent episode reward: [-39.12824090696331], time: 50.006
steps: 19001424, episodes: 463000, mean episode reward: -39.03318636895996, num_cumulative_constraints: 122428, agent episode reward: [-39.03318636895996], time: 64.055
steps: 19042075, episodes: 464000, mean episode reward: -39.239046851824064, num_cumulative_constraints: 122595, agent episode reward: [-39.239046851824064], time: 60.291
steps: 19082513, episodes: 465000, mean episode reward: -39.12841887543417, num_cumulative_constraints: 122800, agent episode reward: [-39.12841887543417], time: 61.005
steps: 19123285, episodes: 466000, mean episode reward: -39.256730361048405, num_cumulative_constraints: 122960, agent episode reward: [-39.256730361048405], time: 54.261
steps: 19163761, episodes: 467000, mean episode reward: -38.972168941249514, num_cumulative_constraints: 123107, agent episode reward: [-38.972168941249514], time: 59.088
steps: 19204928, episodes: 468000, mean episode reward: -39.17533330332347, num_cumulative_constraints: 123240, agent episode reward: [-39.17533330332347], time: 59.705
steps: 19246905, episodes: 469000, mean episode reward: -39.76366095695751, num_cumulative_constraints: 123408, agent episode reward: [-39.76366095695751], time: 54.926
steps: 19288657, episodes: 470000, mean episode reward: -39.64074394910732, num_cumulative_constraints: 123578, agent episode reward: [-39.64074394910732], time: 58.276
steps: 19330403, episodes: 471000, mean episode reward: -39.81044216299948, num_cumulative_constraints: 123777, agent episode reward: [-39.81044216299948], time: 56.254
steps: 19372908, episodes: 472000, mean episode reward: -40.18347037856092, num_cumulative_constraints: 123964, agent episode reward: [-40.18347037856092], time: 54.849
steps: 19414944, episodes: 473000, mean episode reward: -39.89038493342545, num_cumulative_constraints: 124133, agent episode reward: [-39.89038493342545], time: 58.149
steps: 19456519, episodes: 474000, mean episode reward: -39.73364558855162, num_cumulative_constraints: 124326, agent episode reward: [-39.73364558855162], time: 66.066
steps: 19497512, episodes: 475000, mean episode reward: -39.46309404807589, num_cumulative_constraints: 124542, agent episode reward: [-39.46309404807589], time: 57.215
steps: 19538463, episodes: 476000, mean episode reward: -39.365617699878406, num_cumulative_constraints: 124718, agent episode reward: [-39.365617699878406], time: 54.401
steps: 19579394, episodes: 477000, mean episode reward: -39.47776315781509, num_cumulative_constraints: 124930, agent episode reward: [-39.47776315781509], time: 57.673
steps: 19620309, episodes: 478000, mean episode reward: -39.348352577277296, num_cumulative_constraints: 125113, agent episode reward: [-39.348352577277296], time: 52.921
steps: 19661137, episodes: 479000, mean episode reward: -39.52336127428611, num_cumulative_constraints: 125348, agent episode reward: [-39.52336127428611], time: 66.918
steps: 19702007, episodes: 480000, mean episode reward: -39.51527648861333, num_cumulative_constraints: 125561, agent episode reward: [-39.51527648861333], time: 60.49
steps: 19742708, episodes: 481000, mean episode reward: -39.40151547813538, num_cumulative_constraints: 125803, agent episode reward: [-39.40151547813538], time: 55.588
steps: 19783630, episodes: 482000, mean episode reward: -39.62165776436629, num_cumulative_constraints: 126018, agent episode reward: [-39.62165776436629], time: 51.322
steps: 19824301, episodes: 483000, mean episode reward: -39.236646848820094, num_cumulative_constraints: 126199, agent episode reward: [-39.236646848820094], time: 53.537
steps: 19864945, episodes: 484000, mean episode reward: -39.21079828097151, num_cumulative_constraints: 126357, agent episode reward: [-39.21079828097151], time: 56.389
steps: 19905441, episodes: 485000, mean episode reward: -39.0909176609023, num_cumulative_constraints: 126549, agent episode reward: [-39.0909176609023], time: 62.439
steps: 19945954, episodes: 486000, mean episode reward: -39.199725485080876, num_cumulative_constraints: 126761, agent episode reward: [-39.199725485080876], time: 52.935
steps: 19986464, episodes: 487000, mean episode reward: -39.125995435925326, num_cumulative_constraints: 126928, agent episode reward: [-39.125995435925326], time: 57.086
steps: 20026953, episodes: 488000, mean episode reward: -38.963702369943846, num_cumulative_constraints: 127077, agent episode reward: [-38.963702369943846], time: 63.136
steps: 20067702, episodes: 489000, mean episode reward: -39.496927825647646, num_cumulative_constraints: 127314, agent episode reward: [-39.496927825647646], time: 67.568
steps: 20108516, episodes: 490000, mean episode reward: -39.37533162488544, num_cumulative_constraints: 127512, agent episode reward: [-39.37533162488544], time: 63.997
steps: 20149193, episodes: 491000, mean episode reward: -39.1361952264485, num_cumulative_constraints: 127652, agent episode reward: [-39.1361952264485], time: 59.909
steps: 20189767, episodes: 492000, mean episode reward: -39.19769439006038, num_cumulative_constraints: 127840, agent episode reward: [-39.19769439006038], time: 50.709
steps: 20230553, episodes: 493000, mean episode reward: -39.38245405630393, num_cumulative_constraints: 128015, agent episode reward: [-39.38245405630393], time: 55.609
steps: 20271556, episodes: 494000, mean episode reward: -39.691340143275696, num_cumulative_constraints: 128266, agent episode reward: [-39.691340143275696], time: 62.357
steps: 20312499, episodes: 495000, mean episode reward: -39.3766003099107, num_cumulative_constraints: 128422, agent episode reward: [-39.3766003099107], time: 55.283
steps: 20353526, episodes: 496000, mean episode reward: -39.554510076170644, num_cumulative_constraints: 128607, agent episode reward: [-39.554510076170644], time: 60.956
steps: 20394663, episodes: 497000, mean episode reward: -39.597659631187206, num_cumulative_constraints: 128782, agent episode reward: [-39.597659631187206], time: 64.436
steps: 20435664, episodes: 498000, mean episode reward: -39.52599244644641, num_cumulative_constraints: 128985, agent episode reward: [-39.52599244644641], time: 54.846
steps: 20476725, episodes: 499000, mean episode reward: -39.562763944076515, num_cumulative_constraints: 129166, agent episode reward: [-39.562763944076515], time: 60.091
steps: 20517690, episodes: 500000, mean episode reward: -39.567691782157915, num_cumulative_constraints: 129387, agent episode reward: [-39.567691782157915], time: 68.865
steps: 20558755, episodes: 501000, mean episode reward: -39.827507763343576, num_cumulative_constraints: 129653, agent episode reward: [-39.827507763343576], time: 64.844
steps: 20599437, episodes: 502000, mean episode reward: -39.16808519248279, num_cumulative_constraints: 129840, agent episode reward: [-39.16808519248279], time: 63.489
steps: 20640158, episodes: 503000, mean episode reward: -39.32999076269058, num_cumulative_constraints: 130030, agent episode reward: [-39.32999076269058], time: 59.013
steps: 20680999, episodes: 504000, mean episode reward: -39.6533205151033, num_cumulative_constraints: 130276, agent episode reward: [-39.6533205151033], time: 59.634
steps: 20721596, episodes: 505000, mean episode reward: -39.22393593773161, num_cumulative_constraints: 130457, agent episode reward: [-39.22393593773161], time: 61.446
steps: 20762252, episodes: 506000, mean episode reward: -39.31402461993066, num_cumulative_constraints: 130634, agent episode reward: [-39.31402461993066], time: 55.719
steps: 20803120, episodes: 507000, mean episode reward: -39.56754701592747, num_cumulative_constraints: 130837, agent episode reward: [-39.56754701592747], time: 55.417
steps: 20843813, episodes: 508000, mean episode reward: -39.28058066105729, num_cumulative_constraints: 131021, agent episode reward: [-39.28058066105729], time: 67.475
steps: 20884574, episodes: 509000, mean episode reward: -39.42592966442108, num_cumulative_constraints: 131230, agent episode reward: [-39.42592966442108], time: 66.446
steps: 20925645, episodes: 510000, mean episode reward: -39.61556912217527, num_cumulative_constraints: 131447, agent episode reward: [-39.61556912217527], time: 58.461
steps: 20966536, episodes: 511000, mean episode reward: -39.31908497684373, num_cumulative_constraints: 131593, agent episode reward: [-39.31908497684373], time: 58.846
steps: 21007866, episodes: 512000, mean episode reward: -39.78613746659948, num_cumulative_constraints: 131792, agent episode reward: [-39.78613746659948], time: 64.704
steps: 21048768, episodes: 513000, mean episode reward: -39.52739972296379, num_cumulative_constraints: 132002, agent episode reward: [-39.52739972296379], time: 61.906
steps: 21089493, episodes: 514000, mean episode reward: -39.42105964396287, num_cumulative_constraints: 132204, agent episode reward: [-39.42105964396287], time: 69.966
steps: 21130229, episodes: 515000, mean episode reward: -39.349882223120005, num_cumulative_constraints: 132395, agent episode reward: [-39.349882223120005], time: 64.975
steps: 21170870, episodes: 516000, mean episode reward: -39.434593684621625, num_cumulative_constraints: 132628, agent episode reward: [-39.434593684621625], time: 62.36
steps: 21211782, episodes: 517000, mean episode reward: -39.43740900044336, num_cumulative_constraints: 132783, agent episode reward: [-39.43740900044336], time: 59.572
steps: 21252386, episodes: 518000, mean episode reward: -39.31524782522732, num_cumulative_constraints: 132996, agent episode reward: [-39.31524782522732], time: 55.305
steps: 21293098, episodes: 519000, mean episode reward: -39.36541990340571, num_cumulative_constraints: 133190, agent episode reward: [-39.36541990340571], time: 59.22
steps: 21333844, episodes: 520000, mean episode reward: -39.34950251113005, num_cumulative_constraints: 133391, agent episode reward: [-39.34950251113005], time: 53.426
steps: 21374476, episodes: 521000, mean episode reward: -39.25683894287454, num_cumulative_constraints: 133574, agent episode reward: [-39.25683894287454], time: 51.009
steps: 21415244, episodes: 522000, mean episode reward: -39.23934143231176, num_cumulative_constraints: 133732, agent episode reward: [-39.23934143231176], time: 58.928
steps: 21456109, episodes: 523000, mean episode reward: -39.39971627464907, num_cumulative_constraints: 133864, agent episode reward: [-39.39971627464907], time: 69.584
steps: 21496953, episodes: 524000, mean episode reward: -39.43624689510406, num_cumulative_constraints: 134067, agent episode reward: [-39.43624689510406], time: 67.691
steps: 21537699, episodes: 525000, mean episode reward: -39.49832194801338, num_cumulative_constraints: 134313, agent episode reward: [-39.49832194801338], time: 63.191
steps: 21578387, episodes: 526000, mean episode reward: -39.414137554854904, num_cumulative_constraints: 134532, agent episode reward: [-39.414137554854904], time: 63.235
steps: 21618974, episodes: 527000, mean episode reward: -39.379944882687276, num_cumulative_constraints: 134807, agent episode reward: [-39.379944882687276], time: 61.402
steps: 21659588, episodes: 528000, mean episode reward: -39.536062734966606, num_cumulative_constraints: 135099, agent episode reward: [-39.536062734966606], time: 53.014
steps: 21700300, episodes: 529000, mean episode reward: -40.20433868982655, num_cumulative_constraints: 135590, agent episode reward: [-40.20433868982655], time: 56.14
steps: 21740972, episodes: 530000, mean episode reward: -40.18865092056747, num_cumulative_constraints: 136083, agent episode reward: [-40.18865092056747], time: 59.272
steps: 21781556, episodes: 531000, mean episode reward: -40.46598300438644, num_cumulative_constraints: 136714, agent episode reward: [-40.46598300438644], time: 58.32
steps: 21822188, episodes: 532000, mean episode reward: -40.757405131008255, num_cumulative_constraints: 137417, agent episode reward: [-40.757405131008255], time: 59.231
steps: 21862917, episodes: 533000, mean episode reward: -40.72986231783891, num_cumulative_constraints: 138082, agent episode reward: [-40.72986231783891], time: 56.667
steps: 21903486, episodes: 534000, mean episode reward: -41.174012584411464, num_cumulative_constraints: 138912, agent episode reward: [-41.174012584411464], time: 59.0
steps: 21944075, episodes: 535000, mean episode reward: -41.12698631683139, num_cumulative_constraints: 139729, agent episode reward: [-41.12698631683139], time: 56.261
steps: 21984692, episodes: 536000, mean episode reward: -40.958554348537845, num_cumulative_constraints: 140503, agent episode reward: [-40.958554348537845], time: 61.026
steps: 22025267, episodes: 537000, mean episode reward: -40.54765832673281, num_cumulative_constraints: 141156, agent episode reward: [-40.54765832673281], time: 60.188
steps: 22065881, episodes: 538000, mean episode reward: -39.96385967295596, num_cumulative_constraints: 141592, agent episode reward: [-39.96385967295596], time: 61.269
steps: 22106526, episodes: 539000, mean episode reward: -39.495480115661216, num_cumulative_constraints: 141875, agent episode reward: [-39.495480115661216], time: 57.544
steps: 22147171, episodes: 540000, mean episode reward: -39.39085072183191, num_cumulative_constraints: 142117, agent episode reward: [-39.39085072183191], time: 55.886
steps: 22187786, episodes: 541000, mean episode reward: -39.36730589273568, num_cumulative_constraints: 142365, agent episode reward: [-39.36730589273568], time: 53.845
steps: 22228362, episodes: 542000, mean episode reward: -39.416145009805625, num_cumulative_constraints: 142630, agent episode reward: [-39.416145009805625], time: 54.209
steps: 22269076, episodes: 543000, mean episode reward: -39.42021048576942, num_cumulative_constraints: 142855, agent episode reward: [-39.42021048576942], time: 64.137
steps: 22309891, episodes: 544000, mean episode reward: -39.60786321190762, num_cumulative_constraints: 143089, agent episode reward: [-39.60786321190762], time: 60.848
steps: 22350610, episodes: 545000, mean episode reward: -39.353215796001344, num_cumulative_constraints: 143305, agent episode reward: [-39.353215796001344], time: 57.767
steps: 22391436, episodes: 546000, mean episode reward: -39.39711852872942, num_cumulative_constraints: 143485, agent episode reward: [-39.39711852872942], time: 57.091
steps: 22432272, episodes: 547000, mean episode reward: -39.30764667098296, num_cumulative_constraints: 143625, agent episode reward: [-39.30764667098296], time: 57.678
steps: 22473059, episodes: 548000, mean episode reward: -39.35594026741304, num_cumulative_constraints: 143831, agent episode reward: [-39.35594026741304], time: 61.275
steps: 22513796, episodes: 549000, mean episode reward: -39.309928108727156, num_cumulative_constraints: 144007, agent episode reward: [-39.309928108727156], time: 60.602
steps: 22554489, episodes: 550000, mean episode reward: -39.28176856887707, num_cumulative_constraints: 144188, agent episode reward: [-39.28176856887707], time: 59.611
steps: 22595232, episodes: 551000, mean episode reward: -39.24843248565267, num_cumulative_constraints: 144352, agent episode reward: [-39.24843248565267], time: 56.037
steps: 22636042, episodes: 552000, mean episode reward: -39.22264899186733, num_cumulative_constraints: 144493, agent episode reward: [-39.22264899186733], time: 67.155
steps: 22676761, episodes: 553000, mean episode reward: -39.262271541800146, num_cumulative_constraints: 144672, agent episode reward: [-39.262271541800146], time: 61.352
steps: 22717562, episodes: 554000, mean episode reward: -39.29629460439176, num_cumulative_constraints: 144835, agent episode reward: [-39.29629460439176], time: 51.41
steps: 22758413, episodes: 555000, mean episode reward: -39.64779883980362, num_cumulative_constraints: 145067, agent episode reward: [-39.64779883980362], time: 52.146
steps: 22799415, episodes: 556000, mean episode reward: -39.66603862429357, num_cumulative_constraints: 145279, agent episode reward: [-39.66603862429357], time: 53.583
steps: 22840216, episodes: 557000, mean episode reward: -39.46612340644489, num_cumulative_constraints: 145480, agent episode reward: [-39.46612340644489], time: 53.574
steps: 22880984, episodes: 558000, mean episode reward: -39.278066765064835, num_cumulative_constraints: 145638, agent episode reward: [-39.278066765064835], time: 55.024
steps: 22921926, episodes: 559000, mean episode reward: -39.686130995828485, num_cumulative_constraints: 145841, agent episode reward: [-39.686130995828485], time: 48.539
steps: 22962689, episodes: 560000, mean episode reward: -39.513058190831764, num_cumulative_constraints: 146080, agent episode reward: [-39.513058190831764], time: 55.93
steps: 23003290, episodes: 561000, mean episode reward: -39.330067539582075, num_cumulative_constraints: 146306, agent episode reward: [-39.330067539582075], time: 52.309
steps: 23044052, episodes: 562000, mean episode reward: -39.39533641856613, num_cumulative_constraints: 146500, agent episode reward: [-39.39533641856613], time: 50.547
steps: 23084962, episodes: 563000, mean episode reward: -39.659276887234974, num_cumulative_constraints: 146717, agent episode reward: [-39.659276887234974], time: 55.316
steps: 23125853, episodes: 564000, mean episode reward: -39.53730233792498, num_cumulative_constraints: 146916, agent episode reward: [-39.53730233792498], time: 66.572
steps: 23166724, episodes: 565000, mean episode reward: -39.59751887608278, num_cumulative_constraints: 147146, agent episode reward: [-39.59751887608278], time: 72.518
steps: 23207711, episodes: 566000, mean episode reward: -39.60037693021047, num_cumulative_constraints: 147355, agent episode reward: [-39.60037693021047], time: 69.377
steps: 23248540, episodes: 567000, mean episode reward: -39.448047453814944, num_cumulative_constraints: 147556, agent episode reward: [-39.448047453814944], time: 69.558
steps: 23289633, episodes: 568000, mean episode reward: -39.86712858725918, num_cumulative_constraints: 147817, agent episode reward: [-39.86712858725918], time: 63.337
steps: 23330443, episodes: 569000, mean episode reward: -39.502383211301975, num_cumulative_constraints: 148024, agent episode reward: [-39.502383211301975], time: 65.98
steps: 23371264, episodes: 570000, mean episode reward: -39.72310963245308, num_cumulative_constraints: 148311, agent episode reward: [-39.72310963245308], time: 60.15
steps: 23412163, episodes: 571000, mean episode reward: -39.40079250395425, num_cumulative_constraints: 148485, agent episode reward: [-39.40079250395425], time: 66.535
steps: 23453193, episodes: 572000, mean episode reward: -39.76773205738141, num_cumulative_constraints: 148771, agent episode reward: [-39.76773205738141], time: 60.972
steps: 23494210, episodes: 573000, mean episode reward: -39.607459931060234, num_cumulative_constraints: 148966, agent episode reward: [-39.607459931060234], time: 62.892
steps: 23535280, episodes: 574000, mean episode reward: -39.583556418426866, num_cumulative_constraints: 149170, agent episode reward: [-39.583556418426866], time: 63.213
steps: 23576329, episodes: 575000, mean episode reward: -39.632710878605856, num_cumulative_constraints: 149379, agent episode reward: [-39.632710878605856], time: 56.79
steps: 23617567, episodes: 576000, mean episode reward: -39.82514036374345, num_cumulative_constraints: 149582, agent episode reward: [-39.82514036374345], time: 58.484
steps: 23658510, episodes: 577000, mean episode reward: -39.52399461001679, num_cumulative_constraints: 149758, agent episode reward: [-39.52399461001679], time: 58.089
steps: 23699408, episodes: 578000, mean episode reward: -39.430332080328796, num_cumulative_constraints: 149974, agent episode reward: [-39.430332080328796], time: 63.101
steps: 23740799, episodes: 579000, mean episode reward: -39.847121562917906, num_cumulative_constraints: 150173, agent episode reward: [-39.847121562917906], time: 59.589
steps: 23782263, episodes: 580000, mean episode reward: -39.75069200092047, num_cumulative_constraints: 150365, agent episode reward: [-39.75069200092047], time: 55.991
steps: 23823404, episodes: 581000, mean episode reward: -39.58236785912375, num_cumulative_constraints: 150526, agent episode reward: [-39.58236785912375], time: 58.367
steps: 23864743, episodes: 582000, mean episode reward: -39.89058255795579, num_cumulative_constraints: 150713, agent episode reward: [-39.89058255795579], time: 52.998
steps: 23905892, episodes: 583000, mean episode reward: -39.54030741354075, num_cumulative_constraints: 150891, agent episode reward: [-39.54030741354075], time: 55.027
steps: 23947032, episodes: 584000, mean episode reward: -39.6502771226817, num_cumulative_constraints: 151067, agent episode reward: [-39.6502771226817], time: 52.173
steps: 23988023, episodes: 585000, mean episode reward: -39.57208907024571, num_cumulative_constraints: 151269, agent episode reward: [-39.57208907024571], time: 60.229
steps: 24029109, episodes: 586000, mean episode reward: -39.41298394301067, num_cumulative_constraints: 151410, agent episode reward: [-39.41298394301067], time: 59.537
steps: 24070172, episodes: 587000, mean episode reward: -39.536839234708616, num_cumulative_constraints: 151574, agent episode reward: [-39.536839234708616], time: 54.863
steps: 24111139, episodes: 588000, mean episode reward: -39.38773409570429, num_cumulative_constraints: 151713, agent episode reward: [-39.38773409570429], time: 61.429
steps: 24152232, episodes: 589000, mean episode reward: -39.6111573124568, num_cumulative_constraints: 151909, agent episode reward: [-39.6111573124568], time: 66.912
steps: 24193290, episodes: 590000, mean episode reward: -39.511662484965676, num_cumulative_constraints: 152068, agent episode reward: [-39.511662484965676], time: 65.194
steps: 24234383, episodes: 591000, mean episode reward: -39.66829865392769, num_cumulative_constraints: 152264, agent episode reward: [-39.66829865392769], time: 70.426
steps: 24275314, episodes: 592000, mean episode reward: -39.35070873539198, num_cumulative_constraints: 152419, agent episode reward: [-39.35070873539198], time: 62.417
steps: 24316441, episodes: 593000, mean episode reward: -39.440248009915436, num_cumulative_constraints: 152595, agent episode reward: [-39.440248009915436], time: 60.493
steps: 24357437, episodes: 594000, mean episode reward: -39.63623261718856, num_cumulative_constraints: 152818, agent episode reward: [-39.63623261718856], time: 52.092
steps: 24398396, episodes: 595000, mean episode reward: -39.48150324755543, num_cumulative_constraints: 152991, agent episode reward: [-39.48150324755543], time: 58.658
steps: 24439236, episodes: 596000, mean episode reward: -39.36845593384044, num_cumulative_constraints: 153171, agent episode reward: [-39.36845593384044], time: 55.421
steps: 24480174, episodes: 597000, mean episode reward: -39.480619485766326, num_cumulative_constraints: 153337, agent episode reward: [-39.480619485766326], time: 53.102
steps: 24521179, episodes: 598000, mean episode reward: -39.59152352510615, num_cumulative_constraints: 153526, agent episode reward: [-39.59152352510615], time: 58.805
steps: 24562439, episodes: 599000, mean episode reward: -39.72535300781076, num_cumulative_constraints: 153715, agent episode reward: [-39.72535300781076], time: 62.291
steps: 24603558, episodes: 600000, mean episode reward: -39.686879711714894, num_cumulative_constraints: 153933, agent episode reward: [-39.686879711714894], time: 57.915


safe_layer
'
                if xx[0] > 1.0:
                    xx[0] = 1.0
                if xx[0] < -1.0:
                    xx[0] = -1.0

                if xx == action_omega:
                    return action

                action[3] = + xx[0]/2
                action[4] = - xx[0]/2
' with 0.05 gap
steps: 41366, episodes: 1000, mean episode reward: -39.52227936323721, num_cumulative_constraints: 58, agent episode reward: [-39.52227936323721], time: 84.642
steps: 84247, episodes: 2000, mean episode reward: -44.50774908243718, num_cumulative_constraints: 1303, agent episode reward: [-44.50774908243718], time: 106.115
steps: 127067, episodes: 3000, mean episode reward: -51.74671902678731, num_cumulative_constraints: 5004, agent episode reward: [-51.74671902678731], time: 124.302


with safety_layer
                if xx[0] > 2.0:
                    xx[0] = 2.0
                if xx[0] < -2.0:
                    xx[0] = -2.0

                if xx == action_omega:
                    return action

                delta_action = xx[0]/2 - action_omega
                action[3] = action[3] + delta_action/2
                action[4] = action[4] - delta_action/2


Starting iterations...
steps: 41048, episodes: 1000, mean episode reward: -39.24725583844495, num_cumulative_constraints: 101, agent episode reward: [-39.24725583844495], time: 95.307
steps: 82221, episodes: 2000, mean episode reward: -39.60002562139826, num_cumulative_constraints: 237, agent episode reward: [-39.60002562139826], time: 101.398
steps: 123333, episodes: 3000, mean episode reward: -39.44373632325706, num_cumulative_constraints: 312, agent episode reward: [-39.44373632325706], time: 86.969
steps: 164574, episodes: 4000, mean episode reward: -39.6265026597871, num_cumulative_constraints: 416, agent episode reward: [-39.6265026597871], time: 86.731
steps: 206376, episodes: 5000, mean episode reward: -40.01651775458276, num_cumulative_constraints: 549, agent episode reward: [-40.01651775458276], time: 92.175
steps: 247820, episodes: 6000, mean episode reward: -39.55692505502718, num_cumulative_constraints: 611, agent episode reward: [-39.55692505502718], time: 111.318
steps: 289170, episodes: 7000, mean episode reward: -39.69158465575525, num_cumulative_constraints: 740, agent episode reward: [-39.69158465575525], time: 96.252
steps: 330505, episodes: 8000, mean episode reward: -39.750610617362014, num_cumulative_constraints: 895, agent episode reward: [-39.750610617362014], time: 96.947
steps: 371880, episodes: 9000, mean episode reward: -39.82566833676209, num_cumulative_constraints: 996, agent episode reward: [-39.82566833676209], time: 94.751
steps: 413264, episodes: 10000, mean episode reward: -39.792204693354186, num_cumulative_constraints: 1076, agent episode reward: [-39.792204693354186], time: 92.913
steps: 454679, episodes: 11000, mean episode reward: -39.76971270325485, num_cumulative_constraints: 1156, agent episode reward: [-39.76971270325485], time: 90.067
steps: 496188, episodes: 12000, mean episode reward: -39.63611309320409, num_cumulative_constraints: 1232, agent episode reward: [-39.63611309320409], time: 92.733
steps: 538235, episodes: 13000, mean episode reward: -40.09202255283883, num_cumulative_constraints: 1369, agent episode reward: [-40.09202255283883], time: 98.681
steps: 580131, episodes: 14000, mean episode reward: -40.097989732614366, num_cumulative_constraints: 1515, agent episode reward: [-40.097989732614366], time: 108.143
steps: 621709, episodes: 15000, mean episode reward: -39.775297602597796, num_cumulative_constraints: 1598, agent episode reward: [-39.775297602597796], time: 93.084
steps: 663152, episodes: 16000, mean episode reward: -39.7119357526903, num_cumulative_constraints: 1684, agent episode reward: [-39.7119357526903], time: 100.839
steps: 704533, episodes: 17000, mean episode reward: -39.724952614725275, num_cumulative_constraints: 1762, agent episode reward: [-39.724952614725275], time: 97.405
steps: 745777, episodes: 18000, mean episode reward: -39.6159286243277, num_cumulative_constraints: 1855, agent episode reward: [-39.6159286243277], time: 91.509
steps: 787116, episodes: 19000, mean episode reward: -39.6581834032855, num_cumulative_constraints: 1935, agent episode reward: [-39.6581834032855], time: 97.771
steps: 828361, episodes: 20000, mean episode reward: -39.73553615713047, num_cumulative_constraints: 2041, agent episode reward: [-39.73553615713047], time: 99.786
steps: 869521, episodes: 21000, mean episode reward: -39.564826795626196, num_cumulative_constraints: 2135, agent episode reward: [-39.564826795626196], time: 97.655
steps: 910711, episodes: 22000, mean episode reward: -39.61076063002154, num_cumulative_constraints: 2211, agent episode reward: [-39.61076063002154], time: 93.515
steps: 951899, episodes: 23000, mean episode reward: -39.602207796292994, num_cumulative_constraints: 2309, agent episode reward: [-39.602207796292994], time: 94.422
steps: 993306, episodes: 24000, mean episode reward: -39.81562804096812, num_cumulative_constraints: 2429, agent episode reward: [-39.81562804096812], time: 91.799
steps: 1034600, episodes: 25000, mean episode reward: -39.77964913738961, num_cumulative_constraints: 2555, agent episode reward: [-39.77964913738961], time: 99.136
steps: 1076110, episodes: 26000, mean episode reward: -39.90031982049202, num_cumulative_constraints: 2661, agent episode reward: [-39.90031982049202], time: 91.536
steps: 1117689, episodes: 27000, mean episode reward: -40.02080629817808, num_cumulative_constraints: 2783, agent episode reward: [-40.02080629817808], time: 94.998
steps: 1159276, episodes: 28000, mean episode reward: -39.91583525418181, num_cumulative_constraints: 2906, agent episode reward: [-39.91583525418181], time: 99.92
steps: 1200813, episodes: 29000, mean episode reward: -40.134175643096285, num_cumulative_constraints: 3040, agent episode reward: [-40.134175643096285], time: 94.052
steps: 1242395, episodes: 30000, mean episode reward: -39.954537489308784, num_cumulative_constraints: 3156, agent episode reward: [-39.954537489308784], time: 89.669
steps: 1283943, episodes: 31000, mean episode reward: -39.941412223009, num_cumulative_constraints: 3254, agent episode reward: [-39.941412223009], time: 106.799
steps: 1325656, episodes: 32000, mean episode reward: -40.29708983838385, num_cumulative_constraints: 3363, agent episode reward: [-40.29708983838385], time: 122.504
steps: 1367287, episodes: 33000, mean episode reward: -40.10057326343624, num_cumulative_constraints: 3453, agent episode reward: [-40.10057326343624], time: 93.608
steps: 1408768, episodes: 34000, mean episode reward: -39.921048355800934, num_cumulative_constraints: 3575, agent episode reward: [-39.921048355800934], time: 91.231
steps: 1450465, episodes: 35000, mean episode reward: -40.344491388260614, num_cumulative_constraints: 3761, agent episode reward: [-40.344491388260614], time: 100.632
steps: 1492210, episodes: 36000, mean episode reward: -40.27992160574793, num_cumulative_constraints: 3922, agent episode reward: [-40.27992160574793], time: 96.934
steps: 1533801, episodes: 37000, mean episode reward: -40.04013479517891, num_cumulative_constraints: 4014, agent episode reward: [-40.04013479517891], time: 106.468
steps: 1575564, episodes: 38000, mean episode reward: -40.127027598808624, num_cumulative_constraints: 4161, agent episode reward: [-40.127027598808624], time: 96.729
steps: 1617232, episodes: 39000, mean episode reward: -39.88024005001363, num_cumulative_constraints: 4276, agent episode reward: [-39.88024005001363], time: 97.502
steps: 1658931, episodes: 40000, mean episode reward: -39.96984975099015, num_cumulative_constraints: 4390, agent episode reward: [-39.96984975099015], time: 94.833
steps: 1700347, episodes: 41000, mean episode reward: -39.8827539318138, num_cumulative_constraints: 4517, agent episode reward: [-39.8827539318138], time: 97.897
steps: 1741884, episodes: 42000, mean episode reward: -39.99634071322931, num_cumulative_constraints: 4654, agent episode reward: [-39.99634071322931], time: 146.0
steps: 1783127, episodes: 43000, mean episode reward: -39.53942170622186, num_cumulative_constraints: 4732, agent episode reward: [-39.53942170622186], time: 102.835
steps: 1824619, episodes: 44000, mean episode reward: -39.74922909734882, num_cumulative_constraints: 4831, agent episode reward: [-39.74922909734882], time: 93.721
steps: 1866283, episodes: 45000, mean episode reward: -39.71062301367183, num_cumulative_constraints: 4907, agent episode reward: [-39.71062301367183], time: 98.792
steps: 1907996, episodes: 46000, mean episode reward: -39.82797110847231, num_cumulative_constraints: 5041, agent episode reward: [-39.82797110847231], time: 91.133
steps: 1949591, episodes: 47000, mean episode reward: -39.91259205885068, num_cumulative_constraints: 5168, agent episode reward: [-39.91259205885068], time: 94.032
steps: 1990775, episodes: 48000, mean episode reward: -39.70440802456741, num_cumulative_constraints: 5286, agent episode reward: [-39.70440802456741], time: 105.085
steps: 2031774, episodes: 49000, mean episode reward: -39.54932902490966, num_cumulative_constraints: 5409, agent episode reward: [-39.54932902490966], time: 90.995
steps: 2072870, episodes: 50000, mean episode reward: -39.54152750865677, num_cumulative_constraints: 5479, agent episode reward: [-39.54152750865677], time: 180.086
steps: 2113861, episodes: 51000, mean episode reward: -39.55254369195495, num_cumulative_constraints: 5605, agent episode reward: [-39.55254369195495], time: 83.662
steps: 2155029, episodes: 52000, mean episode reward: -39.652253131277085, num_cumulative_constraints: 5698, agent episode reward: [-39.652253131277085], time: 104.503
steps: 2196389, episodes: 53000, mean episode reward: -39.79821606818613, num_cumulative_constraints: 5799, agent episode reward: [-39.79821606818613], time: 86.819
steps: 2237663, episodes: 54000, mean episode reward: -39.8389041517458, num_cumulative_constraints: 5909, agent episode reward: [-39.8389041517458], time: 90.591
steps: 2278964, episodes: 55000, mean episode reward: -39.875639305182716, num_cumulative_constraints: 6007, agent episode reward: [-39.875639305182716], time: 87.699
steps: 2320021, episodes: 56000, mean episode reward: -39.567463358078875, num_cumulative_constraints: 6110, agent episode reward: [-39.567463358078875], time: 90.203
steps: 2361131, episodes: 57000, mean episode reward: -39.69156879351403, num_cumulative_constraints: 6220, agent episode reward: [-39.69156879351403], time: 156.61
steps: 2402366, episodes: 58000, mean episode reward: -39.73694043310807, num_cumulative_constraints: 6295, agent episode reward: [-39.73694043310807], time: 88.58
steps: 2443787, episodes: 59000, mean episode reward: -39.968034064340564, num_cumulative_constraints: 6431, agent episode reward: [-39.968034064340564], time: 89.301
steps: 2484967, episodes: 60000, mean episode reward: -39.705658080755114, num_cumulative_constraints: 6542, agent episode reward: [-39.705658080755114], time: 84.033
steps: 2526540, episodes: 61000, mean episode reward: -39.86784622696483, num_cumulative_constraints: 6639, agent episode reward: [-39.86784622696483], time: 97.705
steps: 2568099, episodes: 62000, mean episode reward: -40.01925767230024, num_cumulative_constraints: 6778, agent episode reward: [-40.01925767230024], time: 90.726
steps: 2609865, episodes: 63000, mean episode reward: -39.96920172536406, num_cumulative_constraints: 6915, agent episode reward: [-39.96920172536406], time: 95.084
steps: 2651731, episodes: 64000, mean episode reward: -40.13522183280877, num_cumulative_constraints: 7012, agent episode reward: [-40.13522183280877], time: 90.396
steps: 2693018, episodes: 65000, mean episode reward: -39.80967158772648, num_cumulative_constraints: 7125, agent episode reward: [-39.80967158772648], time: 97.325
steps: 2734394, episodes: 66000, mean episode reward: -39.870471561710424, num_cumulative_constraints: 7236, agent episode reward: [-39.870471561710424], time: 92.464
steps: 2775713, episodes: 67000, mean episode reward: -39.76001382408981, num_cumulative_constraints: 7349, agent episode reward: [-39.76001382408981], time: 97.501
steps: 2816931, episodes: 68000, mean episode reward: -39.78660795715709, num_cumulative_constraints: 7471, agent episode reward: [-39.78660795715709], time: 109.766
steps: 2858059, episodes: 69000, mean episode reward: -39.75095848215811, num_cumulative_constraints: 7579, agent episode reward: [-39.75095848215811], time: 112.543
steps: 2899126, episodes: 70000, mean episode reward: -39.64889986066255, num_cumulative_constraints: 7722, agent episode reward: [-39.64889986066255], time: 145.958
steps: 2940114, episodes: 71000, mean episode reward: -39.56140950654729, num_cumulative_constraints: 7831, agent episode reward: [-39.56140950654729], time: 98.657
steps: 2981309, episodes: 72000, mean episode reward: -39.903693881583486, num_cumulative_constraints: 7982, agent episode reward: [-39.903693881583486], time: 100.552
steps: 3022324, episodes: 73000, mean episode reward: -39.49800892420298, num_cumulative_constraints: 8090, agent episode reward: [-39.49800892420298], time: 92.679
steps: 3063346, episodes: 74000, mean episode reward: -39.48375331022768, num_cumulative_constraints: 8206, agent episode reward: [-39.48375331022768], time: 91.584


with safety_layer
tarting iterations...
steps: 41780, episodes: 1000, mean episode reward: -39.976962305463616, num_cumulative_constraints: 66, agent episode reward: [-39.976962305463616], time: 86.981
steps: 83734, episodes: 2000, mean episode reward: -40.16044512997768, num_cumulative_constraints: 146, agent episode reward: [-40.16044512997768], time: 82.311
steps: 125586, episodes: 3000, mean episode reward: -40.28530162884895, num_cumulative_constraints: 180, agent episode reward: [-40.28530162884895], time: 88.169
steps: 167347, episodes: 4000, mean episode reward: -40.08463860226241, num_cumulative_constraints: 226, agent episode reward: [-40.08463860226241], time: 84.405
steps: 209250, episodes: 5000, mean episode reward: -40.37794587466993, num_cumulative_constraints: 328, agent episode reward: [-40.37794587466993], time: 87.77
steps: 251203, episodes: 6000, mean episode reward: -40.26223521739344, num_cumulative_constraints: 405, agent episode reward: [-40.26223521739344], time: 90.391
steps: 293523, episodes: 7000, mean episode reward: -40.659033457658666, num_cumulative_constraints: 489, agent episode reward: [-40.659033457658666], time: 91.37
steps: 335818, episodes: 8000, mean episode reward: -40.62525643898477, num_cumulative_constraints: 573, agent episode reward: [-40.62525643898477], time: 93.365
steps: 378288, episodes: 9000, mean episode reward: -40.78103296622437, num_cumulative_constraints: 670, agent episode reward: [-40.78103296622437], time: 87.815
steps: 420451, episodes: 10000, mean episode reward: -40.42491721294062, num_cumulative_constraints: 788, agent episode reward: [-40.42491721294062], time: 89.872
steps: 462689, episodes: 11000, mean episode reward: -40.68023907357019, num_cumulative_constraints: 921, agent episode reward: [-40.68023907357019], time: 91.486
steps: 505113, episodes: 12000, mean episode reward: -40.51825533924415, num_cumulative_constraints: 987, agent episode reward: [-40.51825533924415], time: 91.094
steps: 547031, episodes: 13000, mean episode reward: -40.1687005124632, num_cumulative_constraints: 1074, agent episode reward: [-40.1687005124632], time: 92.124
steps: 589046, episodes: 14000, mean episode reward: -40.34322627066334, num_cumulative_constraints: 1181, agent episode reward: [-40.34322627066334], time: 89.83
steps: 631277, episodes: 15000, mean episode reward: -40.36269402343141, num_cumulative_constraints: 1277, agent episode reward: [-40.36269402343141], time: 87.066
steps: 673498, episodes: 16000, mean episode reward: -40.56463540005566, num_cumulative_constraints: 1361, agent episode reward: [-40.56463540005566], time: 86.75
steps: 715402, episodes: 17000, mean episode reward: -40.3295454117572, num_cumulative_constraints: 1477, agent episode reward: [-40.3295454117572], time: 119.676
steps: 757069, episodes: 18000, mean episode reward: -39.91819967573385, num_cumulative_constraints: 1559, agent episode reward: [-39.91819967573385], time: 92.799
steps: 798608, episodes: 19000, mean episode reward: -39.84269685319085, num_cumulative_constraints: 1629, agent episode reward: [-39.84269685319085], time: 86.008
steps: 840175, episodes: 20000, mean episode reward: -40.02312595602616, num_cumulative_constraints: 1716, agent episode reward: [-40.02312595602616], time: 83.1
steps: 881546, episodes: 21000, mean episode reward: -39.77934085058004, num_cumulative_constraints: 1817, agent episode reward: [-39.77934085058004], time: 79.672
steps: 922900, episodes: 22000, mean episode reward: -39.79488831325203, num_cumulative_constraints: 1867, agent episode reward: [-39.79488831325203], time: 80.425
steps: 964395, episodes: 23000, mean episode reward: -39.973692742399, num_cumulative_constraints: 1937, agent episode reward: [-39.973692742399], time: 83.559
steps: 1005739, episodes: 24000, mean episode reward: -39.75190547103886, num_cumulative_constraints: 2015, agent episode reward: [-39.75190547103886], time: 85.581
steps: 1047164, episodes: 25000, mean episode reward: -39.71053023854076, num_cumulative_constraints: 2069, agent episode reward: [-39.71053023854076], time: 85.289
steps: 1088638, episodes: 26000, mean episode reward: -39.87183534819286, num_cumulative_constraints: 2128, agent episode reward: [-39.87183534819286], time: 87.017
steps: 1130008, episodes: 27000, mean episode reward: -39.61486556686998, num_cumulative_constraints: 2179, agent episode reward: [-39.61486556686998], time: 85.603
steps: 1172001, episodes: 28000, mean episode reward: -39.90201443036539, num_cumulative_constraints: 2231, agent episode reward: [-39.90201443036539], time: 86.419
steps: 1214121, episodes: 29000, mean episode reward: -40.11299051095523, num_cumulative_constraints: 2295, agent episode reward: [-40.11299051095523], time: 90.913
steps: 1255635, episodes: 30000, mean episode reward: -39.81211998737983, num_cumulative_constraints: 2365, agent episode reward: [-39.81211998737983], time: 89.495
steps: 1296824, episodes: 31000, mean episode reward: -39.44555677988533, num_cumulative_constraints: 2405, agent episode reward: [-39.44555677988533], time: 86.038
steps: 1338101, episodes: 32000, mean episode reward: -39.60292681173692, num_cumulative_constraints: 2455, agent episode reward: [-39.60292681173692], time: 90.926
steps: 1379332, episodes: 33000, mean episode reward: -39.62918872629405, num_cumulative_constraints: 2532, agent episode reward: [-39.62918872629405], time: 88.983
steps: 1420942, episodes: 34000, mean episode reward: -39.87192832890648, num_cumulative_constraints: 2575, agent episode reward: [-39.87192832890648], time: 92.562
steps: 1462582, episodes: 35000, mean episode reward: -39.875684544032254, num_cumulative_constraints: 2623, agent episode reward: [-39.875684544032254], time: 90.414
steps: 1504385, episodes: 36000, mean episode reward: -39.94796304088011, num_cumulative_constraints: 2675, agent episode reward: [-39.94796304088011], time: 94.502
steps: 1546403, episodes: 37000, mean episode reward: -40.15547183466819, num_cumulative_constraints: 2777, agent episode reward: [-40.15547183466819], time: 95.159
steps: 1588283, episodes: 38000, mean episode reward: -39.82767936911483, num_cumulative_constraints: 2825, agent episode reward: [-39.82767936911483], time: 97.433
steps: 1630192, episodes: 39000, mean episode reward: -39.8008174935813, num_cumulative_constraints: 2892, agent episode reward: [-39.8008174935813], time: 96.481
steps: 1672664, episodes: 40000, mean episode reward: -40.09100468869603, num_cumulative_constraints: 2929, agent episode reward: [-40.09100468869603], time: 94.185
steps: 1715655, episodes: 41000, mean episode reward: -40.43502295065233, num_cumulative_constraints: 2998, agent episode reward: [-40.43502295065233], time: 93.36
steps: 1758095, episodes: 42000, mean episode reward: -40.40562845493309, num_cumulative_constraints: 3091, agent episode reward: [-40.40562845493309], time: 91.965
steps: 1799989, episodes: 43000, mean episode reward: -40.146417580366716, num_cumulative_constraints: 3205, agent episode reward: [-40.146417580366716], time: 88.045
steps: 1841491, episodes: 44000, mean episode reward: -40.055990720597926, num_cumulative_constraints: 3315, agent episode reward: [-40.055990720597926], time: 85.845
steps: 1882929, episodes: 45000, mean episode reward: -39.90687556606374, num_cumulative_constraints: 3436, agent episode reward: [-39.90687556606374], time: 88.08
steps: 1924246, episodes: 46000, mean episode reward: -39.71082629763153, num_cumulative_constraints: 3501, agent episode reward: [-39.71082629763153], time: 84.369
steps: 1965727, episodes: 47000, mean episode reward: -39.972386527157184, num_cumulative_constraints: 3621, agent episode reward: [-39.972386527157184], time: 88.533
steps: 2007136, episodes: 48000, mean episode reward: -39.927524985096944, num_cumulative_constraints: 3694, agent episode reward: [-39.927524985096944], time: 87.822
steps: 2048628, episodes: 49000, mean episode reward: -40.122502777781946, num_cumulative_constraints: 3805, agent episode reward: [-40.122502777781946], time: 87.749
steps: 2090190, episodes: 50000, mean episode reward: -40.107442698184784, num_cumulative_constraints: 3929, agent episode reward: [-40.107442698184784], time: 89.478
steps: 2131578, episodes: 51000, mean episode reward: -39.75764379040078, num_cumulative_constraints: 4011, agent episode reward: [-39.75764379040078], time: 86.929
steps: 2173137, episodes: 52000, mean episode reward: -40.01100665269894, num_cumulative_constraints: 4109, agent episode reward: [-40.01100665269894], time: 89.733
steps: 2214915, episodes: 53000, mean episode reward: -40.23603787219582, num_cumulative_constraints: 4203, agent episode reward: [-40.23603787219582], time: 87.754
steps: 2256917, episodes: 54000, mean episode reward: -40.28373541101467, num_cumulative_constraints: 4318, agent episode reward: [-40.28373541101467], time: 87.065
steps: 2299239, episodes: 55000, mean episode reward: -40.479241358015685, num_cumulative_constraints: 4412, agent episode reward: [-40.479241358015685], time: 90.388
steps: 2341119, episodes: 56000, mean episode reward: -40.08709423085077, num_cumulative_constraints: 4521, agent episode reward: [-40.08709423085077], time: 89.131
steps: 2382668, episodes: 57000, mean episode reward: -39.958328310105436, num_cumulative_constraints: 4604, agent episode reward: [-39.958328310105436], time: 86.649
steps: 2424506, episodes: 58000, mean episode reward: -40.29593535784172, num_cumulative_constraints: 4709, agent episode reward: [-40.29593535784172], time: 93.157
steps: 2466014, episodes: 59000, mean episode reward: -39.99531231890259, num_cumulative_constraints: 4822, agent episode reward: [-39.99531231890259], time: 90.082
steps: 2507817, episodes: 60000, mean episode reward: -40.1227966707515, num_cumulative_constraints: 4908, agent episode reward: [-40.1227966707515], time: 89.49
steps: 2550201, episodes: 61000, mean episode reward: -40.61757150107436, num_cumulative_constraints: 5025, agent episode reward: [-40.61757150107436], time: 93.875
steps: 2591791, episodes: 62000, mean episode reward: -40.24590882660864, num_cumulative_constraints: 5146, agent episode reward: [-40.24590882660864], time: 93.974
steps: 2633037, episodes: 63000, mean episode reward: -39.73798889190998, num_cumulative_constraints: 5235, agent episode reward: [-39.73798889190998], time: 96.874
steps: 2674689, episodes: 64000, mean episode reward: -40.32908788256125, num_cumulative_constraints: 5416, agent episode reward: [-40.32908788256125], time: 99.503
steps: 2716266, episodes: 65000, mean episode reward: -39.99997737041126, num_cumulative_constraints: 5508, agent episode reward: [-39.99997737041126], time: 97.892
steps: 2757670, episodes: 66000, mean episode reward: -39.897599932874186, num_cumulative_constraints: 5617, agent episode reward: [-39.897599932874186], time: 94.53
steps: 2799358, episodes: 67000, mean episode reward: -40.27549916962224, num_cumulative_constraints: 5783, agent episode reward: [-40.27549916962224], time: 100.38
steps: 2840689, episodes: 68000, mean episode reward: -40.04031827272551, num_cumulative_constraints: 5976, agent episode reward: [-40.04031827272551], time: 97.454
steps: 2881978, episodes: 69000, mean episode reward: -39.84489054950552, num_cumulative_constraints: 6109, agent episode reward: [-39.84489054950552], time: 97.375
steps: 2923161, episodes: 70000, mean episode reward: -40.02165842721325, num_cumulative_constraints: 6294, agent episode reward: [-40.02165842721325], time: 98.433
steps: 2964270, episodes: 71000, mean episode reward: -39.81061396101788, num_cumulative_constraints: 6457, agent episode reward: [-39.81061396101788], time: 96.962
steps: 3005678, episodes: 72000, mean episode reward: -39.99106546776902, num_cumulative_constraints: 6596, agent episode reward: [-39.99106546776902], time: 96.699
steps: 3046682, episodes: 73000, mean episode reward: -39.42101234462786, num_cumulative_constraints: 6715, agent episode reward: [-39.42101234462786], time: 97.059
steps: 3088026, episodes: 74000, mean episode reward: -39.948962059992596, num_cumulative_constraints: 6851, agent episode reward: [-39.948962059992596], time: 94.572
steps: 3129223, episodes: 75000, mean episode reward: -39.88912689121246, num_cumulative_constraints: 7014, agent episode reward: [-39.88912689121246], time: 98.751
steps: 3170562, episodes: 76000, mean episode reward: -39.76198499261288, num_cumulative_constraints: 7144, agent episode reward: [-39.76198499261288], time: 93.758
steps: 3212354, episodes: 77000, mean episode reward: -40.34210427162942, num_cumulative_constraints: 7362, agent episode reward: [-40.34210427162942], time: 99.782
steps: 3254839, episodes: 78000, mean episode reward: -40.66189988422726, num_cumulative_constraints: 7591, agent episode reward: [-40.66189988422726], time: 97.704
steps: 3298625, episodes: 79000, mean episode reward: -41.07355616034971, num_cumulative_constraints: 7812, agent episode reward: [-41.07355616034971], time: 100.335
steps: 3344034, episodes: 80000, mean episode reward: -41.959217636407466, num_cumulative_constraints: 8087, agent episode reward: [-41.959217636407466], time: 106.41
steps: 3389029, episodes: 81000, mean episode reward: -41.53539557211683, num_cumulative_constraints: 8357, agent episode reward: [-41.53539557211683], time: 101.592
steps: 3431970, episodes: 82000, mean episode reward: -40.68246019368094, num_cumulative_constraints: 8554, agent episode reward: [-40.68246019368094], time: 96.116
steps: 3474622, episodes: 83000, mean episode reward: -40.25216459643811, num_cumulative_constraints: 8655, agent episode reward: [-40.25216459643811], time: 94.112
steps: 3520186, episodes: 84000, mean episode reward: -41.75955860118264, num_cumulative_constraints: 8807, agent episode reward: [-41.75955860118264], time: 98.983
steps: 3562593, episodes: 85000, mean episode reward: -40.4723827165947, num_cumulative_constraints: 8972, agent episode reward: [-40.4723827165947], time: 97.48
steps: 3603834, episodes: 86000, mean episode reward: -39.92999737834653, num_cumulative_constraints: 9147, agent episode reward: [-39.92999737834653], time: 93.07
steps: 3645120, episodes: 87000, mean episode reward: -39.79677113121468, num_cumulative_constraints: 9252, agent episode reward: [-39.79677113121468], time: 92.622
steps: 3686624, episodes: 88000, mean episode reward: -39.94815828170607, num_cumulative_constraints: 9413, agent episode reward: [-39.94815828170607], time: 99.175
steps: 3728093, episodes: 89000, mean episode reward: -40.04903727437297, num_cumulative_constraints: 9614, agent episode reward: [-40.04903727437297], time: 96.067
steps: 3770074, episodes: 90000, mean episode reward: -40.43978289266514, num_cumulative_constraints: 9819, agent episode reward: [-40.43978289266514], time: 100.738
steps: 3811956, episodes: 91000, mean episode reward: -40.41179817613312, num_cumulative_constraints: 10064, agent episode reward: [-40.41179817613312], time: 96.383
steps: 3853298, episodes: 92000, mean episode reward: -39.80035810787312, num_cumulative_constraints: 10218, agent episode reward: [-39.80035810787312], time: 99.322
steps: 3894987, episodes: 93000, mean episode reward: -40.192436310685004, num_cumulative_constraints: 10459, agent episode reward: [-40.192436310685004], time: 98.449
steps: 3936738, episodes: 94000, mean episode reward: -40.32910716752778, num_cumulative_constraints: 10719, agent episode reward: [-40.32910716752778], time: 100.422
steps: 3978437, episodes: 95000, mean episode reward: -40.179892003761566, num_cumulative_constraints: 10923, agent episode reward: [-40.179892003761566], time: 98.06
steps: 4019846, episodes: 96000, mean episode reward: -40.06831350879362, num_cumulative_constraints: 11120, agent episode reward: [-40.06831350879362], time: 96.061
steps: 4061377, episodes: 97000, mean episode reward: -40.20343932473038, num_cumulative_constraints: 11339, agent episode reward: [-40.20343932473038], time: 100.854
steps: 4102763, episodes: 98000, mean episode reward: -39.94336027379255, num_cumulative_constraints: 11493, agent episode reward: [-39.94336027379255], time: 97.201
steps: 4144327, episodes: 99000, mean episode reward: -40.364221847973326, num_cumulative_constraints: 11720, agent episode reward: [-40.364221847973326], time: 101.486
steps: 4185943, episodes: 100000, mean episode reward: -40.48105173432064, num_cumulative_constraints: 11998, agent episode reward: [-40.48105173432064], time: 101.958
steps: 4227587, episodes: 101000, mean episode reward: -40.08524697964349, num_cumulative_constraints: 12125, agent episode reward: [-40.08524697964349], time: 99.972
steps: 4269237, episodes: 102000, mean episode reward: -40.24488350608166, num_cumulative_constraints: 12333, agent episode reward: [-40.24488350608166], time: 100.561
steps: 4311080, episodes: 103000, mean episode reward: -40.361829064512214, num_cumulative_constraints: 12587, agent episode reward: [-40.361829064512214], time: 100.645
steps: 4353058, episodes: 104000, mean episode reward: -40.817570742635525, num_cumulative_constraints: 12901, agent episode reward: [-40.817570742635525], time: 101.132
steps: 4394582, episodes: 105000, mean episode reward: -40.74899575148351, num_cumulative_constraints: 13289, agent episode reward: [-40.74899575148351], time: 103.015
steps: 4435731, episodes: 106000, mean episode reward: -40.91439064137343, num_cumulative_constraints: 13808, agent episode reward: [-40.91439064137343], time: 102.059
steps: 4477030, episodes: 107000, mean episode reward: -41.16850743260697, num_cumulative_constraints: 14387, agent episode reward: [-41.16850743260697], time: 102.299
steps: 4518457, episodes: 108000, mean episode reward: -41.89306982357538, num_cumulative_constraints: 15195, agent episode reward: [-41.89306982357538], time: 101.739
steps: 4559812, episodes: 109000, mean episode reward: -41.50225663113594, num_cumulative_constraints: 15914, agent episode reward: [-41.50225663113594], time: 99.205
steps: 4601120, episodes: 110000, mean episode reward: -41.27447940896066, num_cumulative_constraints: 16540, agent episode reward: [-41.27447940896066], time: 97.323
steps: 4642551, episodes: 111000, mean episode reward: -41.275686904329596, num_cumulative_constraints: 17136, agent episode reward: [-41.275686904329596], time: 95.262
steps: 4684532, episodes: 112000, mean episode reward: -41.67809249711811, num_cumulative_constraints: 17788, agent episode reward: [-41.67809249711811], time: 97.159
steps: 4726406, episodes: 113000, mean episode reward: -42.002772772315204, num_cumulative_constraints: 18503, agent episode reward: [-42.002772772315204], time: 95.134
steps: 4768576, episodes: 114000, mean episode reward: -42.02551780248449, num_cumulative_constraints: 19204, agent episode reward: [-42.02551780248449], time: 96.508
steps: 4810805, episodes: 115000, mean episode reward: -41.67310415827173, num_cumulative_constraints: 19779, agent episode reward: [-41.67310415827173], time: 93.305
steps: 4853113, episodes: 116000, mean episode reward: -41.9306139973473, num_cumulative_constraints: 20378, agent episode reward: [-41.9306139973473], time: 94.278
steps: 4896026, episodes: 117000, mean episode reward: -42.202604013526724, num_cumulative_constraints: 20963, agent episode reward: [-42.202604013526724], time: 91.297
steps: 4939436, episodes: 118000, mean episode reward: -42.186075222791786, num_cumulative_constraints: 21505, agent episode reward: [-42.186075222791786], time: 91.658
steps: 4983232, episodes: 119000, mean episode reward: -42.843029233001246, num_cumulative_constraints: 22217, agent episode reward: [-42.843029233001246], time: 93.593
steps: 5026436, episodes: 120000, mean episode reward: -42.59266347666591, num_cumulative_constraints: 22914, agent episode reward: [-42.59266347666591], time: 93.048
steps: 5069182, episodes: 121000, mean episode reward: -43.00437928839166, num_cumulative_constraints: 23834, agent episode reward: [-43.00437928839166], time: 91.284
steps: 5111430, episodes: 122000, mean episode reward: -43.325202804109054, num_cumulative_constraints: 24946, agent episode reward: [-43.325202804109054], time: 95.994
steps: 5153725, episodes: 123000, mean episode reward: -44.694852836034514, num_cumulative_constraints: 26499, agent episode reward: [-44.694852836034514], time: 94.031
steps: 5196612, episodes: 124000, mean episode reward: -45.529309385548956, num_cumulative_constraints: 28236, agent episode reward: [-45.529309385548956], time: 99.429
steps: 5239584, episodes: 125000, mean episode reward: -46.74556583626696, num_cumulative_constraints: 30290, agent episode reward: [-46.74556583626696], time: 101.392
steps: 5282308, episodes: 126000, mean episode reward: -46.37538084824147, num_cumulative_constraints: 32285, agent episode reward: [-46.37538084824147], time: 102.983
steps: 5325165, episodes: 127000, mean episode reward: -46.33053476974855, num_cumulative_constraints: 34230, agent episode reward: [-46.33053476974855], time: 100.101
steps: 5367814, episodes: 128000, mean episode reward: -45.3432289331354, num_cumulative_constraints: 35945, agent episode reward: [-45.3432289331354], time: 100.789
steps: 5411056, episodes: 129000, mean episode reward: -47.42576804639019, num_cumulative_constraints: 38158, agent episode reward: [-47.42576804639019], time: 106.243
steps: 5453817, episodes: 130000, mean episode reward: -47.33306425489659, num_cumulative_constraints: 40434, agent episode reward: [-47.33306425489659], time: 106.623
steps: 5496796, episodes: 131000, mean episode reward: -46.24097397991001, num_cumulative_constraints: 42332, agent episode reward: [-46.24097397991001], time: 103.697
steps: 5539296, episodes: 132000, mean episode reward: -45.686814274528096, num_cumulative_constraints: 44150, agent episode reward: [-45.686814274528096], time: 96.752
steps: 5582300, episodes: 133000, mean episode reward: -46.36714710921305, num_cumulative_constraints: 46068, agent episode reward: [-46.36714710921305], time: 101.253
steps: 5625162, episodes: 134000, mean episode reward: -46.620086081480984, num_cumulative_constraints: 48128, agent episode reward: [-46.620086081480984], time: 98.615
steps: 5668072, episodes: 135000, mean episode reward: -46.833495033537886, num_cumulative_constraints: 50169, agent episode reward: [-46.833495033537886], time: 100.355
steps: 5711042, episodes: 136000, mean episode reward: -45.083422734776036, num_cumulative_constraints: 51651, agent episode reward: [-45.083422734776036], time: 91.58
steps: 5753533, episodes: 137000, mean episode reward: -44.750342594140065, num_cumulative_constraints: 53136, agent episode reward: [-44.750342594140065], time: 92.155
steps: 5795978, episodes: 138000, mean episode reward: -44.566839927558085, num_cumulative_constraints: 54517, agent episode reward: [-44.566839927558085], time: 90.075
steps: 5837979, episodes: 139000, mean episode reward: -44.38438212140328, num_cumulative_constraints: 55908, agent episode reward: [-44.38438212140328], time: 88.898
steps: 5880221, episodes: 140000, mean episode reward: -44.247908500583634, num_cumulative_constraints: 57263, agent episode reward: [-44.247908500583634], time: 89.084
steps: 5922484, episodes: 141000, mean episode reward: -45.1757314012701, num_cumulative_constraints: 58919, agent episode reward: [-45.1757314012701], time: 91.119
steps: 5965149, episodes: 142000, mean episode reward: -45.543461752694164, num_cumulative_constraints: 60616, agent episode reward: [-45.543461752694164], time: 91.022
steps: 6007757, episodes: 143000, mean episode reward: -44.78849821816358, num_cumulative_constraints: 62051, agent episode reward: [-44.78849821816358], time: 91.424
steps: 6050502, episodes: 144000, mean episode reward: -43.61351572563212, num_cumulative_constraints: 63165, agent episode reward: [-43.61351572563212], time: 83.73
steps: 6092691, episodes: 145000, mean episode reward: -42.806181443253664, num_cumulative_constraints: 64106, agent episode reward: [-42.806181443253664], time: 81.505
steps: 6134555, episodes: 146000, mean episode reward: -42.198736596027445, num_cumulative_constraints: 64858, agent episode reward: [-42.198736596027445], time: 77.413
steps: 6176838, episodes: 147000, mean episode reward: -42.11237796541567, num_cumulative_constraints: 65504, agent episode reward: [-42.11237796541567], time: 78.942
steps: 6218990, episodes: 148000, mean episode reward: -41.87941334264993, num_cumulative_constraints: 66099, agent episode reward: [-41.87941334264993], time: 75.823
steps: 6261882, episodes: 149000, mean episode reward: -42.80132282153923, num_cumulative_constraints: 66862, agent episode reward: [-42.80132282153923], time: 78.746
steps: 6304090, episodes: 150000, mean episode reward: -41.675955665913584, num_cumulative_constraints: 67414, agent episode reward: [-41.675955665913584], time: 76.573
steps: 6346778, episodes: 151000, mean episode reward: -41.951862913407126, num_cumulative_constraints: 67924, agent episode reward: [-41.951862913407126], time: 76.375
steps: 6389642, episodes: 152000, mean episode reward: -41.947221163362514, num_cumulative_constraints: 68417, agent episode reward: [-41.947221163362514], time: 79.711
steps: 6432340, episodes: 153000, mean episode reward: -41.07262715719761, num_cumulative_constraints: 68721, agent episode reward: [-41.07262715719761], time: 76.165
steps: 6474965, episodes: 154000, mean episode reward: -41.08226941965394, num_cumulative_constraints: 69035, agent episode reward: [-41.08226941965394], time: 73.403
steps: 6517159, episodes: 155000, mean episode reward: -40.971199916720316, num_cumulative_constraints: 69390, agent episode reward: [-40.971199916720316], time: 76.69
steps: 6558993, episodes: 156000, mean episode reward: -40.90928662086232, num_cumulative_constraints: 69731, agent episode reward: [-40.90928662086232], time: 74.236
steps: 6600569, episodes: 157000, mean episode reward: -40.72853244389431, num_cumulative_constraints: 70082, agent episode reward: [-40.72853244389431], time: 74.354
steps: 6642085, episodes: 158000, mean episode reward: -40.431612388162904, num_cumulative_constraints: 70338, agent episode reward: [-40.431612388162904], time: 72.479
steps: 6683688, episodes: 159000, mean episode reward: -40.921061190428894, num_cumulative_constraints: 70663, agent episode reward: [-40.921061190428894], time: 78.111
steps: 6725126, episodes: 160000, mean episode reward: -40.51357659959514, num_cumulative_constraints: 70906, agent episode reward: [-40.51357659959514], time: 76.919
steps: 6766513, episodes: 161000, mean episode reward: -40.259526889642295, num_cumulative_constraints: 71133, agent episode reward: [-40.259526889642295], time: 80.31
steps: 6807929, episodes: 162000, mean episode reward: -40.16152171218004, num_cumulative_constraints: 71311, agent episode reward: [-40.16152171218004], time: 77.826
steps: 6849237, episodes: 163000, mean episode reward: -40.19607276641342, num_cumulative_constraints: 71549, agent episode reward: [-40.19607276641342], time: 79.699
steps: 6890684, episodes: 164000, mean episode reward: -40.36013351525247, num_cumulative_constraints: 71775, agent episode reward: [-40.36013351525247], time: 83.191
steps: 6932558, episodes: 165000, mean episode reward: -40.59282987260943, num_cumulative_constraints: 72006, agent episode reward: [-40.59282987260943], time: 83.486
steps: 6974318, episodes: 166000, mean episode reward: -40.77029300833646, num_cumulative_constraints: 72250, agent episode reward: [-40.77029300833646], time: 82.021
steps: 7015899, episodes: 167000, mean episode reward: -40.664215244232835, num_cumulative_constraints: 72525, agent episode reward: [-40.664215244232835], time: 80.665
steps: 7057399, episodes: 168000, mean episode reward: -40.587384261975835, num_cumulative_constraints: 72794, agent episode reward: [-40.587384261975835], time: 81.331
steps: 7098961, episodes: 169000, mean episode reward: -40.65866628565524, num_cumulative_constraints: 73064, agent episode reward: [-40.65866628565524], time: 83.227
steps: 7140917, episodes: 170000, mean episode reward: -41.62976670968897, num_cumulative_constraints: 73475, agent episode reward: [-41.62976670968897], time: 83.712
steps: 7182563, episodes: 171000, mean episode reward: -40.955070475458065, num_cumulative_constraints: 73859, agent episode reward: [-40.955070475458065], time: 82.506
steps: 7224139, episodes: 172000, mean episode reward: -41.03344112857597, num_cumulative_constraints: 74200, agent episode reward: [-41.03344112857597], time: 78.479
steps: 7266065, episodes: 173000, mean episode reward: -41.374474568178265, num_cumulative_constraints: 74646, agent episode reward: [-41.374474568178265], time: 82.386
steps: 7307767, episodes: 174000, mean episode reward: -41.168214327152405, num_cumulative_constraints: 75049, agent episode reward: [-41.168214327152405], time: 81.192
steps: 7349334, episodes: 175000, mean episode reward: -41.00564040834726, num_cumulative_constraints: 75476, agent episode reward: [-41.00564040834726], time: 80.225
steps: 7391490, episodes: 176000, mean episode reward: -41.285550578210795, num_cumulative_constraints: 75895, agent episode reward: [-41.285550578210795], time: 81.071
steps: 7433607, episodes: 177000, mean episode reward: -41.33752767435394, num_cumulative_constraints: 76254, agent episode reward: [-41.33752767435394], time: 81.253
steps: 7475573, episodes: 178000, mean episode reward: -40.93328039882492, num_cumulative_constraints: 76549, agent episode reward: [-40.93328039882492], time: 81.702
steps: 7517533, episodes: 179000, mean episode reward: -41.03000915191042, num_cumulative_constraints: 76935, agent episode reward: [-41.03000915191042], time: 84.833
steps: 7559870, episodes: 180000, mean episode reward: -40.95174379941886, num_cumulative_constraints: 77265, agent episode reward: [-40.95174379941886], time: 86.016
steps: 7601910, episodes: 181000, mean episode reward: -40.73968265253179, num_cumulative_constraints: 77542, agent episode reward: [-40.73968265253179], time: 83.214
steps: 7644018, episodes: 182000, mean episode reward: -41.292154858287475, num_cumulative_constraints: 77948, agent episode reward: [-41.292154858287475], time: 85.884
steps: 7686092, episodes: 183000, mean episode reward: -41.95001964537306, num_cumulative_constraints: 78430, agent episode reward: [-41.95001964537306], time: 91.005
steps: 7728188, episodes: 184000, mean episode reward: -41.68538611153828, num_cumulative_constraints: 78886, agent episode reward: [-41.68538611153828], time: 86.918
steps: 7770472, episodes: 185000, mean episode reward: -42.324629961043435, num_cumulative_constraints: 79436, agent episode reward: [-42.324629961043435], time: 86.826
steps: 7813215, episodes: 186000, mean episode reward: -42.14708919656207, num_cumulative_constraints: 79958, agent episode reward: [-42.14708919656207], time: 91.038
steps: 7856238, episodes: 187000, mean episode reward: -42.296673196188436, num_cumulative_constraints: 80592, agent episode reward: [-42.296673196188436], time: 89.149
steps: 7899387, episodes: 188000, mean episode reward: -42.53916056928265, num_cumulative_constraints: 81244, agent episode reward: [-42.53916056928265], time: 87.124
steps: 7941904, episodes: 189000, mean episode reward: -42.62062215884751, num_cumulative_constraints: 81936, agent episode reward: [-42.62062215884751], time: 84.754
steps: 7983944, episodes: 190000, mean episode reward: -42.116426031849976, num_cumulative_constraints: 82596, agent episode reward: [-42.116426031849976], time: 88.516
steps: 8025972, episodes: 191000, mean episode reward: -41.83019955918261, num_cumulative_constraints: 83146, agent episode reward: [-41.83019955918261], time: 86.422
steps: 8067645, episodes: 192000, mean episode reward: -41.39579183812684, num_cumulative_constraints: 83626, agent episode reward: [-41.39579183812684], time: 85.339
steps: 8109175, episodes: 193000, mean episode reward: -40.94683995952352, num_cumulative_constraints: 84037, agent episode reward: [-40.94683995952352], time: 80.737
steps: 8150661, episodes: 194000, mean episode reward: -41.322021608378535, num_cumulative_constraints: 84575, agent episode reward: [-41.322021608378535], time: 84.047
steps: 8192234, episodes: 195000, mean episode reward: -41.51097596746655, num_cumulative_constraints: 85112, agent episode reward: [-41.51097596746655], time: 84.956
steps: 8233926, episodes: 196000, mean episode reward: -41.38786262602865, num_cumulative_constraints: 85616, agent episode reward: [-41.38786262602865], time: 83.969
steps: 8275836, episodes: 197000, mean episode reward: -41.398227440553434, num_cumulative_constraints: 86068, agent episode reward: [-41.398227440553434], time: 81.151
steps: 8317239, episodes: 198000, mean episode reward: -41.129135625740496, num_cumulative_constraints: 86556, agent episode reward: [-41.129135625740496], time: 82.351



1s 0.24x
without safety_layer
steps: 40657, episodes: 1000, mean episode reward: -39.19822689697923, num_cumulative_constraints: 204, agent episode reward: [-39.19822689697923], time: 59.645
steps: 81710, episodes: 2000, mean episode reward: -39.618573187731975, num_cumulative_constraints: 477, agent episode reward: [-39.618573187731975], time: 66.788
steps: 122335, episodes: 3000, mean episode reward: -38.91539249235821, num_cumulative_constraints: 623, agent episode reward: [-38.91539249235821], time: 82.37
steps: 162949, episodes: 4000, mean episode reward: -38.91300791205745, num_cumulative_constraints: 788, agent episode reward: [-38.91300791205745], time: 107.596
steps: 203705, episodes: 5000, mean episode reward: -38.93903020203057, num_cumulative_constraints: 925, agent episode reward: [-38.93903020203057], time: 65.053
steps: 244284, episodes: 6000, mean episode reward: -38.6608178345891, num_cumulative_constraints: 1007, agent episode reward: [-38.6608178345891], time: 64.732
steps: 284996, episodes: 7000, mean episode reward: -38.72049772456949, num_cumulative_constraints: 1099, agent episode reward: [-38.72049772456949], time: 66.364
steps: 325731, episodes: 8000, mean episode reward: -38.654015055155035, num_cumulative_constraints: 1164, agent episode reward: [-38.654015055155035], time: 70.125
steps: 366730, episodes: 9000, mean episode reward: -38.78485684276948, num_cumulative_constraints: 1232, agent episode reward: [-38.78485684276948], time: 68.598
steps: 407378, episodes: 10000, mean episode reward: -38.95042881754078, num_cumulative_constraints: 1371, agent episode reward: [-38.95042881754078], time: 67.847
steps: 448025, episodes: 11000, mean episode reward: -38.8337373875465, num_cumulative_constraints: 1497, agent episode reward: [-38.8337373875465], time: 67.396
steps: 488647, episodes: 12000, mean episode reward: -38.83267404948499, num_cumulative_constraints: 1634, agent episode reward: [-38.83267404948499], time: 63.197


without safety-layer once again 0.24rad/s
steps: 40422, episodes: 1000, mean episode reward: -38.70881830648586, num_cumulative_constraints: 182, agent episode reward: [-38.70881830648586], time: 58.868
steps: 81208, episodes: 2000, mean episode reward: -38.9330882540616, num_cumulative_constraints: 343, agent episode reward: [-38.9330882540616], time: 67.504
steps: 121927, episodes: 3000, mean episode reward: -38.642301991574875, num_cumulative_constraints: 431, agent episode reward: [-38.642301991574875], time: 73.973
steps: 162462, episodes: 4000, mean episode reward: -38.6821133260747, num_cumulative_constraints: 562, agent episode reward: [-38.6821133260747], time: 75.191
steps: 202953, episodes: 5000, mean episode reward: -38.6543595188974, num_cumulative_constraints: 676, agent episode reward: [-38.6543595188974], time: 74.958
steps: 243587, episodes: 6000, mean episode reward: -38.8377974940594, num_cumulative_constraints: 785, agent episode reward: [-38.8377974940594], time: 72.034
steps: 284201, episodes: 7000, mean episode reward: -38.701193348878874, num_cumulative_constraints: 884, agent episode reward: [-38.701193348878874], time: 71.441
steps: 324784, episodes: 8000, mean episode reward: -38.798807083000405, num_cumulative_constraints: 1041, agent episode reward: [-38.798807083000405], time: 73.991
steps: 365424, episodes: 9000, mean episode reward: -38.719648763786736, num_cumulative_constraints: 1144, agent episode reward: [-38.719648763786736], time: 74.723
steps: 406029, episodes: 10000, mean episode reward: -38.66777241773311, num_cumulative_constraints: 1253, agent episode reward: [-38.66777241773311], time: 74.056
steps: 446667, episodes: 11000, mean episode reward: -38.672555089834596, num_cumulative_constraints: 1376, agent episode reward: [-38.672555089834596], time: 67.93
steps: 487381, episodes: 12000, mean episode reward: -38.74677401217276, num_cumulative_constraints: 1494, agent episode reward: [-38.74677401217276], time: 62.104
steps: 528174, episodes: 13000, mean episode reward: -38.67719125831192, num_cumulative_constraints: 1607, agent episode reward: [-38.67719125831192], time: 61.716
steps: 568704, episodes: 14000, mean episode reward: -38.70539554668966, num_cumulative_constraints: 1784, agent episode reward: [-38.70539554668966], time: 62.642
steps: 609264, episodes: 15000, mean episode reward: -38.58158362822174, num_cumulative_constraints: 1883, agent episode reward: [-38.58158362822174], time: 72.778
steps: 649807, episodes: 16000, mean episode reward: -38.71553418575452, num_cumulative_constraints: 2015, agent episode reward: [-38.71553418575452], time: 75.004
steps: 690200, episodes: 17000, mean episode reward: -38.392977629710835, num_cumulative_constraints: 2104, agent episode reward: [-38.392977629710835], time: 75.715
steps: 730682, episodes: 18000, mean episode reward: -38.556191738072116, num_cumulative_constraints: 2191, agent episode reward: [-38.556191738072116], time: 76.611
steps: 771099, episodes: 19000, mean episode reward: -38.47110236170648, num_cumulative_constraints: 2256, agent episode reward: [-38.47110236170648], time: 68.74
steps: 811572, episodes: 20000, mean episode reward: -38.513931618147524, num_cumulative_constraints: 2347, agent episode reward: [-38.513931618147524], time: 73.623
steps: 852278, episodes: 21000, mean episode reward: -38.976282605275, num_cumulative_constraints: 2493, agent episode reward: [-38.976282605275], time: 67.094
steps: 892774, episodes: 22000, mean episode reward: -38.47487223944692, num_cumulative_constraints: 2566, agent episode reward: [-38.47487223944692], time: 59.594
steps: 933164, episodes: 23000, mean episode reward: -38.34296228097874, num_cumulative_constraints: 2616, agent episode reward: [-38.34296228097874], time: 68.58
steps: 973537, episodes: 24000, mean episode reward: -38.62382121847739, num_cumulative_constraints: 2728, agent episode reward: [-38.62382121847739], time: 56.693
steps: 1013779, episodes: 25000, mean episode reward: -38.319448010273554, num_cumulative_constraints: 2805, agent episode reward: [-38.319448010273554], time: 62.906
steps: 1054278, episodes: 26000, mean episode reward: -38.75694008792838, num_cumulative_constraints: 2940, agent episode reward: [-38.75694008792838], time: 81.949
steps: 1094749, episodes: 27000, mean episode reward: -38.779596401010494, num_cumulative_constraints: 3089, agent episode reward: [-38.779596401010494], time: 73.626
steps: 1135186, episodes: 28000, mean episode reward: -38.68422293904985, num_cumulative_constraints: 3221, agent episode reward: [-38.68422293904985], time: 59.349
steps: 1175548, episodes: 29000, mean episode reward: -38.71771489335141, num_cumulative_constraints: 3364, agent episode reward: [-38.71771489335141], time: 61.12
steps: 1215970, episodes: 30000, mean episode reward: -38.47632314151271, num_cumulative_constraints: 3444, agent episode reward: [-38.47632314151271], time: 75.06
steps: 1256393, episodes: 31000, mean episode reward: -38.51953737117165, num_cumulative_constraints: 3524, agent episode reward: [-38.51953737117165], time: 73.026
steps: 1296770, episodes: 32000, mean episode reward: -38.552762627042995, num_cumulative_constraints: 3603, agent episode reward: [-38.552762627042995], time: 75.42
steps: 1337025, episodes: 33000, mean episode reward: -38.51335785091245, num_cumulative_constraints: 3715, agent episode reward: [-38.51335785091245], time: 74.45
steps: 1377371, episodes: 34000, mean episode reward: -38.775965758160446, num_cumulative_constraints: 3862, agent episode reward: [-38.775965758160446], time: 75.728
steps: 1417576, episodes: 35000, mean episode reward: -38.392669181262654, num_cumulative_constraints: 3955, agent episode reward: [-38.392669181262654], time: 76.459
steps: 1457825, episodes: 36000, mean episode reward: -38.48716961649772, num_cumulative_constraints: 4058, agent episode reward: [-38.48716961649772], time: 74.979
steps: 1498136, episodes: 37000, mean episode reward: -38.63782553089544, num_cumulative_constraints: 4166, agent episode reward: [-38.63782553089544], time: 71.46
steps: 1538396, episodes: 38000, mean episode reward: -38.50758569943655, num_cumulative_constraints: 4262, agent episode reward: [-38.50758569943655], time: 71.717
steps: 1578639, episodes: 39000, mean episode reward: -38.64198490421682, num_cumulative_constraints: 4390, agent episode reward: [-38.64198490421682], time: 64.33
steps: 1618938, episodes: 40000, mean episode reward: -38.57724485347046, num_cumulative_constraints: 4506, agent episode reward: [-38.57724485347046], time: 62.879
steps: 1659256, episodes: 41000, mean episode reward: -38.52882147976403, num_cumulative_constraints: 4617, agent episode reward: [-38.52882147976403], time: 62.723
steps: 1699536, episodes: 42000, mean episode reward: -38.56820721572636, num_cumulative_constraints: 4716, agent episode reward: [-38.56820721572636], time: 62.529
steps: 1739749, episodes: 43000, mean episode reward: -38.539987898577294, num_cumulative_constraints: 4834, agent episode reward: [-38.539987898577294], time: 63.371
steps: 1780113, episodes: 44000, mean episode reward: -38.65133173931602, num_cumulative_constraints: 4938, agent episode reward: [-38.65133173931602], time: 62.87
steps: 1820312, episodes: 45000, mean episode reward: -38.31367063213085, num_cumulative_constraints: 5010, agent episode reward: [-38.31367063213085], time: 66.73
steps: 1860559, episodes: 46000, mean episode reward: -38.45992162896406, num_cumulative_constraints: 5106, agent episode reward: [-38.45992162896406], time: 77.547
steps: 1900803, episodes: 47000, mean episode reward: -38.434548116032694, num_cumulative_constraints: 5204, agent episode reward: [-38.434548116032694], time: 74.81
steps: 1941096, episodes: 48000, mean episode reward: -38.49147902990061, num_cumulative_constraints: 5285, agent episode reward: [-38.49147902990061], time: 63.596
steps: 1981377, episodes: 49000, mean episode reward: -38.46700630312899, num_cumulative_constraints: 5354, agent episode reward: [-38.46700630312899], time: 62.987
steps: 2021644, episodes: 50000, mean episode reward: -38.399925690342904, num_cumulative_constraints: 5408, agent episode reward: [-38.399925690342904], time: 62.848
steps: 2061819, episodes: 51000, mean episode reward: -38.503326763363816, num_cumulative_constraints: 5528, agent episode reward: [-38.503326763363816], time: 69.558
steps: 2102058, episodes: 52000, mean episode reward: -38.45786159437972, num_cumulative_constraints: 5625, agent episode reward: [-38.45786159437972], time: 64.518
steps: 2142316, episodes: 53000, mean episode reward: -38.55153311797183, num_cumulative_constraints: 5731, agent episode reward: [-38.55153311797183], time: 66.99
steps: 2182510, episodes: 54000, mean episode reward: -38.57459515815332, num_cumulative_constraints: 5895, agent episode reward: [-38.57459515815332], time: 70.961
steps: 2222777, episodes: 55000, mean episode reward: -38.55885349359273, num_cumulative_constraints: 6002, agent episode reward: [-38.55885349359273], time: 74.397
steps: 2263063, episodes: 56000, mean episode reward: -38.4368820866321, num_cumulative_constraints: 6087, agent episode reward: [-38.4368820866321], time: 73.171
steps: 2303407, episodes: 57000, mean episode reward: -38.66746650188739, num_cumulative_constraints: 6224, agent episode reward: [-38.66746650188739], time: 74.241
steps: 2343699, episodes: 58000, mean episode reward: -38.5789854149868, num_cumulative_constraints: 6323, agent episode reward: [-38.5789854149868], time: 75.392
steps: 2383904, episodes: 59000, mean episode reward: -38.538249122131134, num_cumulative_constraints: 6446, agent episode reward: [-38.538249122131134], time: 69.909
steps: 2424096, episodes: 60000, mean episode reward: -38.40544861002711, num_cumulative_constraints: 6554, agent episode reward: [-38.40544861002711], time: 75.303
steps: 2464489, episodes: 61000, mean episode reward: -38.59641617779452, num_cumulative_constraints: 6664, agent episode reward: [-38.59641617779452], time: 75.811
steps: 2504912, episodes: 62000, mean episode reward: -38.44137985303538, num_cumulative_constraints: 6754, agent episode reward: [-38.44137985303538], time: 72.796
steps: 2545274, episodes: 63000, mean episode reward: -38.64497044182213, num_cumulative_constraints: 6909, agent episode reward: [-38.64497044182213], time: 70.522
steps: 2585704, episodes: 64000, mean episode reward: -38.60560680248903, num_cumulative_constraints: 7027, agent episode reward: [-38.60560680248903], time: 64.651
steps: 2626049, episodes: 65000, mean episode reward: -38.534847410319344, num_cumulative_constraints: 7122, agent episode reward: [-38.534847410319344], time: 75.38
steps: 2666570, episodes: 66000, mean episode reward: -38.71672693113776, num_cumulative_constraints: 7222, agent episode reward: [-38.71672693113776], time: 71.818
steps: 2706868, episodes: 67000, mean episode reward: -38.35316963060863, num_cumulative_constraints: 7292, agent episode reward: [-38.35316963060863], time: 77.108
steps: 2747088, episodes: 68000, mean episode reward: -38.38215793669765, num_cumulative_constraints: 7372, agent episode reward: [-38.38215793669765], time: 63.152
steps: 2787665, episodes: 69000, mean episode reward: -38.53255950176369, num_cumulative_constraints: 7450, agent episode reward: [-38.53255950176369], time: 77.035
steps: 2828369, episodes: 70000, mean episode reward: -38.762135209804605, num_cumulative_constraints: 7614, agent episode reward: [-38.762135209804605], time: 72.484
steps: 2869210, episodes: 71000, mean episode reward: -38.917770097522094, num_cumulative_constraints: 7768, agent episode reward: [-38.917770097522094], time: 82.524
steps: 2909719, episodes: 72000, mean episode reward: -38.47399819460415, num_cumulative_constraints: 7855, agent episode reward: [-38.47399819460415], time: 70.43
steps: 2950260, episodes: 73000, mean episode reward: -38.8559971519568, num_cumulative_constraints: 8033, agent episode reward: [-38.8559971519568], time: 64.5
steps: 2990663, episodes: 74000, mean episode reward: -38.64789747467755, num_cumulative_constraints: 8139, agent episode reward: [-38.64789747467755], time: 70.085
steps: 3030928, episodes: 75000, mean episode reward: -38.51242273259562, num_cumulative_constraints: 8248, agent episode reward: [-38.51242273259562], time: 35.349
steps: 3071251, episodes: 76000, mean episode reward: -38.527293398768634, num_cumulative_constraints: 8363, agent episode reward: [-38.527293398768634], time: 34.414
steps: 3111579, episodes: 77000, mean episode reward: -38.325638694935336, num_cumulative_constraints: 8422, agent episode reward: [-38.325638694935336], time: 35.679
steps: 3151988, episodes: 78000, mean episode reward: -38.463760997444545, num_cumulative_constraints: 8495, agent episode reward: [-38.463760997444545], time: 34.24
steps: 3192220, episodes: 79000, mean episode reward: -38.37347816342286, num_cumulative_constraints: 8604, agent episode reward: [-38.37347816342286], time: 33.491
steps: 3232580, episodes: 80000, mean episode reward: -38.4362011987626, num_cumulative_constraints: 8686, agent episode reward: [-38.4362011987626], time: 36.123
steps: 3272906, episodes: 81000, mean episode reward: -38.41766078222948, num_cumulative_constraints: 8760, agent episode reward: [-38.41766078222948], time: 34.884
steps: 3313279, episodes: 82000, mean episode reward: -38.44844715123387, num_cumulative_constraints: 8820, agent episode reward: [-38.44844715123387], time: 52.832
steps: 3353626, episodes: 83000, mean episode reward: -38.53938116727985, num_cumulative_constraints: 8921, agent episode reward: [-38.53938116727985], time: 37.933
steps: 3393986, episodes: 84000, mean episode reward: -38.292653638985314, num_cumulative_constraints: 8968, agent episode reward: [-38.292653638985314], time: 44.255
steps: 3434315, episodes: 85000, mean episode reward: -38.44925425037666, num_cumulative_constraints: 9080, agent episode reward: [-38.44925425037666], time: 39.499


with safety
steps: 40360, episodes: 1000, mean episode reward: -38.47713334367935, num_cumulative_constraints: 45, agent episode reward: [-38.47713334367935], time: 157.293
steps: 80699, episodes: 2000, mean episode reward: -38.20982423577791, num_cumulative_constraints: 74, agent episode reward: [-38.20982423577791], time: 161.794
steps: 121201, episodes: 3000, mean episode reward: -38.39989175522424, num_cumulative_constraints: 93, agent episode reward: [-38.39989175522424], time: 170.648
steps: 161642, episodes: 4000, mean episode reward: -38.42161522588355, num_cumulative_constraints: 127, agent episode reward: [-38.42161522588355], time: 191.168
steps: 201987, episodes: 5000, mean episode reward: -38.387108685460234, num_cumulative_constraints: 160, agent episode reward: [-38.387108685460234], time: 180.426
steps: 242497, episodes: 6000, mean episode reward: -38.630370343014846, num_cumulative_constraints: 214, agent episode reward: [-38.630370343014846], time: 253.129
steps: 283069, episodes: 7000, mean episode reward: -38.60326728610335, num_cumulative_constraints: 251, agent episode reward: [-38.60326728610335], time: 305.281


omege_diff < 0.3 without safety_layer
steps: 41058, episodes: 1000, mean episode reward: -48.10054988997059, num_cumulative_constraints: 242, agent episode reward: [-48.10054988997059], time: 36.074
steps: 82662, episodes: 2000, mean episode reward: -48.674404973858735, num_cumulative_constraints: 543, agent episode reward: [-48.674404973858735], time: 39.365
steps: 124790, episodes: 3000, mean episode reward: -48.66378849255694, num_cumulative_constraints: 861, agent episode reward: [-48.66378849255694], time: 46.092
steps: 166954, episodes: 4000, mean episode reward: -48.603599262121975, num_cumulative_constraints: 1180, agent episode reward: [-48.603599262121975], time: 49.186
steps: 208849, episodes: 5000, mean episode reward: -48.07247931459012, num_cumulative_constraints: 1458, agent episode reward: [-48.07247931459012], time: 45.049
steps: 250537, episodes: 6000, mean episode reward: -47.930508933846106, num_cumulative_constraints: 1732, agent episode reward: [-47.930508933846106], time: 47.4
steps: 292438, episodes: 7000, mean episode reward: -48.102714364551694, num_cumulative_constraints: 2127, agent episode reward: [-48.102714364551694], time: 45.499
steps: 334446, episodes: 8000, mean episode reward: -48.11638670545773, num_cumulative_constraints: 2521, agent episode reward: [-48.11638670545773], time: 45.213
steps: 377404, episodes: 9000, mean episode reward: -48.97183949450954, num_cumulative_constraints: 2961, agent episode reward: [-48.97183949450954], time: 43.897
steps: 419852, episodes: 10000, mean episode reward: -48.42125790346498, num_cumulative_constraints: 3335, agent episode reward: [-48.42125790346498], time: 46.777
steps: 462799, episodes: 11000, mean episode reward: -48.232223612447356, num_cumulative_constraints: 3624, agent episode reward: [-48.232223612447356], time: 48.118
steps: 504530, episodes: 12000, mean episode reward: -47.92112579859894, num_cumulative_constraints: 3988, agent episode reward: [-47.92112579859894], time: 46.005
steps: 545970, episodes: 13000, mean episode reward: -47.897944872330044, num_cumulative_constraints: 4324, agent episode reward: [-47.897944872330044], time: 44.364
steps: 587270, episodes: 14000, mean episode reward: -47.46896660510864, num_cumulative_constraints: 4661, agent episode reward: [-47.46896660510864], time: 43.727
steps: 628838, episodes: 15000, mean episode reward: -48.320047824380474, num_cumulative_constraints: 5053, agent episode reward: [-48.320047824380474], time: 44.388
steps: 670348, episodes: 16000, mean episode reward: -47.889863384928894, num_cumulative_constraints: 5375, agent episode reward: [-47.889863384928894], time: 43.26
steps: 711866, episodes: 17000, mean episode reward: -48.012692663648615, num_cumulative_constraints: 5709, agent episode reward: [-48.012692663648615], time: 43.902
steps: 753568, episodes: 18000, mean episode reward: -47.96802769817831, num_cumulative_constraints: 6037, agent episode reward: [-47.96802769817831], time: 44.808
steps: 795099, episodes: 19000, mean episode reward: -47.43706738999908, num_cumulative_constraints: 6321, agent episode reward: [-47.43706738999908], time: 42.496
steps: 836460, episodes: 20000, mean episode reward: -47.43334568811377, num_cumulative_constraints: 6663, agent episode reward: [-47.43334568811377], time: 44.199
steps: 878049, episodes: 21000, mean episode reward: -47.52483002918988, num_cumulative_constraints: 6978, agent episode reward: [-47.52483002918988], time: 45.09
steps: 919609, episodes: 22000, mean episode reward: -48.15566231119983, num_cumulative_constraints: 7375, agent episode reward: [-48.15566231119983], time: 42.997
steps: 961013, episodes: 23000, mean episode reward: -47.47423776825056, num_cumulative_constraints: 7712, agent episode reward: [-47.47423776825056], time: 46.165
steps: 1002505, episodes: 24000, mean episode reward: -47.628876661105416, num_cumulative_constraints: 8060, agent episode reward: [-47.628876661105416], time: 47.235
steps: 1044039, episodes: 25000, mean episode reward: -47.51292919335529, num_cumulative_constraints: 8346, agent episode reward: [-47.51292919335529], time: 44.085
steps: 1085441, episodes: 26000, mean episode reward: -47.54274428318713, num_cumulative_constraints: 8700, agent episode reward: [-47.54274428318713], time: 46.02
steps: 1126855, episodes: 27000, mean episode reward: -47.297652021657065, num_cumulative_constraints: 8994, agent episode reward: [-47.297652021657065], time: 48.754
steps: 1168275, episodes: 28000, mean episode reward: -47.44880754212056, num_cumulative_constraints: 9297, agent episode reward: [-47.44880754212056], time: 45.262
steps: 1209954, episodes: 29000, mean episode reward: -47.71701522675813, num_cumulative_constraints: 9593, agent episode reward: [-47.71701522675813], time: 48.893
steps: 1251309, episodes: 30000, mean episode reward: -47.40569308052427, num_cumulative_constraints: 9906, agent episode reward: [-47.40569308052427], time: 48.106
steps: 1292708, episodes: 31000, mean episode reward: -47.47882728239718, num_cumulative_constraints: 10283, agent episode reward: [-47.47882728239718], time: 47.317
steps: 1334460, episodes: 32000, mean episode reward: -47.520729979758194, num_cumulative_constraints: 10561, agent episode reward: [-47.520729979758194], time: 50.073
steps: 1375884, episodes: 33000, mean episode reward: -47.351798343883, num_cumulative_constraints: 10927, agent episode reward: [-47.351798343883], time: 49.584
steps: 1417469, episodes: 34000, mean episode reward: -47.68405756195372, num_cumulative_constraints: 11267, agent episode reward: [-47.68405756195372], time: 46.942
steps: 1458751, episodes: 35000, mean episode reward: -47.20270697236732, num_cumulative_constraints: 11589, agent episode reward: [-47.20270697236732], time: 48.709
steps: 1500235, episodes: 36000, mean episode reward: -47.710551212674254, num_cumulative_constraints: 11975, agent episode reward: [-47.710551212674254], time: 45.947
steps: 1541904, episodes: 37000, mean episode reward: -47.53578493546273, num_cumulative_constraints: 12399, agent episode reward: [-47.53578493546273], time: 46.586
steps: 1583611, episodes: 38000, mean episode reward: -47.73229738540253, num_cumulative_constraints: 12763, agent episode reward: [-47.73229738540253], time: 48.85
steps: 1625091, episodes: 39000, mean episode reward: -47.327626796945694, num_cumulative_constraints: 13084, agent episode reward: [-47.327626796945694], time: 52.496
steps: 1666476, episodes: 40000, mean episode reward: -47.20205132733195, num_cumulative_constraints: 13395, agent episode reward: [-47.20205132733195], time: 49.888
steps: 1707959, episodes: 41000, mean episode reward: -47.20150559178129, num_cumulative_constraints: 13658, agent episode reward: [-47.20150559178129], time: 51.603
steps: 1749641, episodes: 42000, mean episode reward: -47.76855751576022, num_cumulative_constraints: 14005, agent episode reward: [-47.76855751576022], time: 47.49
steps: 1791058, episodes: 43000, mean episode reward: -47.83927111296514, num_cumulative_constraints: 14367, agent episode reward: [-47.83927111296514], time: 49.706
steps: 1832400, episodes: 44000, mean episode reward: -47.551599883793074, num_cumulative_constraints: 14697, agent episode reward: [-47.551599883793074], time: 48.136
steps: 1873774, episodes: 45000, mean episode reward: -47.66888656989286, num_cumulative_constraints: 15056, agent episode reward: [-47.66888656989286], time: 47.37
steps: 1915186, episodes: 46000, mean episode reward: -47.551466103648565, num_cumulative_constraints: 15363, agent episode reward: [-47.551466103648565], time: 47.605
steps: 1956561, episodes: 47000, mean episode reward: -47.070993085943115, num_cumulative_constraints: 15668, agent episode reward: [-47.070993085943115], time: 46.594
steps: 1998067, episodes: 48000, mean episode reward: -47.574478072432775, num_cumulative_constraints: 16002, agent episode reward: [-47.574478072432775], time: 47.786
steps: 2039720, episodes: 49000, mean episode reward: -47.15878979451597, num_cumulative_constraints: 16254, agent episode reward: [-47.15878979451597], time: 44.635
steps: 2081535, episodes: 50000, mean episode reward: -47.70533750053501, num_cumulative_constraints: 16563, agent episode reward: [-47.70533750053501], time: 61.907
steps: 2122962, episodes: 51000, mean episode reward: -47.42721662622668, num_cumulative_constraints: 17004, agent episode reward: [-47.42721662622668], time: 45.591
steps: 2164538, episodes: 52000, mean episode reward: -47.17465493113194, num_cumulative_constraints: 17254, agent episode reward: [-47.17465493113194], time: 47.091
steps: 2205952, episodes: 53000, mean episode reward: -47.07808797619276, num_cumulative_constraints: 17552, agent episode reward: [-47.07808797619276], time: 48.322
steps: 2247543, episodes: 54000, mean episode reward: -47.461446103949505, num_cumulative_constraints: 17868, agent episode reward: [-47.461446103949505], time: 50.617
steps: 2288875, episodes: 55000, mean episode reward: -46.78117855706898, num_cumulative_constraints: 18125, agent episode reward: [-46.78117855706898], time: 46.942
steps: 2330299, episodes: 56000, mean episode reward: -47.09108903351854, num_cumulative_constraints: 18385, agent episode reward: [-47.09108903351854], time: 51.11
steps: 2371659, episodes: 57000, mean episode reward: -47.04946812281171, num_cumulative_constraints: 18645, agent episode reward: [-47.04946812281171], time: 46.892
steps: 2413248, episodes: 58000, mean episode reward: -47.29915715115979, num_cumulative_constraints: 18904, agent episode reward: [-47.29915715115979], time: 51.672
steps: 2455138, episodes: 59000, mean episode reward: -47.34431545162487, num_cumulative_constraints: 19278, agent episode reward: [-47.34431545162487], time: 47.19
steps: 2497195, episodes: 60000, mean episode reward: -47.57320688994578, num_cumulative_constraints: 19573, agent episode reward: [-47.57320688994578], time: 48.868
steps: 2539371, episodes: 61000, mean episode reward: -47.913410551114346, num_cumulative_constraints: 19957, agent episode reward: [-47.913410551114346], time: 50.258
steps: 2581370, episodes: 62000, mean episode reward: -47.63694664423957, num_cumulative_constraints: 20285, agent episode reward: [-47.63694664423957], time: 53.245
steps: 2623551, episodes: 63000, mean episode reward: -47.656606689169145, num_cumulative_constraints: 20598, agent episode reward: [-47.656606689169145], time: 47.108
steps: 2665836, episodes: 64000, mean episode reward: -47.677811476539574, num_cumulative_constraints: 20845, agent episode reward: [-47.677811476539574], time: 57.501
steps: 2708112, episodes: 65000, mean episode reward: -47.61374604991468, num_cumulative_constraints: 21126, agent episode reward: [-47.61374604991468], time: 50.659
steps: 2750054, episodes: 66000, mean episode reward: -47.400910680436894, num_cumulative_constraints: 21427, agent episode reward: [-47.400910680436894], time: 82.734
steps: 2792146, episodes: 67000, mean episode reward: -47.400967718123226, num_cumulative_constraints: 21728, agent episode reward: [-47.400967718123226], time: 58.094
steps: 2834059, episodes: 68000, mean episode reward: -47.38359243956262, num_cumulative_constraints: 22036, agent episode reward: [-47.38359243956262], time: 48.847
steps: 2876134, episodes: 69000, mean episode reward: -47.33728702524496, num_cumulative_constraints: 22339, agent episode reward: [-47.33728702524496], time: 47.8
steps: 2918527, episodes: 70000, mean episode reward: -47.171571047895114, num_cumulative_constraints: 22641, agent episode reward: [-47.171571047895114], time: 46.547
steps: 2961371, episodes: 71000, mean episode reward: -47.60912382643349, num_cumulative_constraints: 22977, agent episode reward: [-47.60912382643349], time: 44.723
steps: 3004112, episodes: 72000, mean episode reward: -47.462265699023064, num_cumulative_constraints: 23313, agent episode reward: [-47.462265699023064], time: 45.743
steps: 3047126, episodes: 73000, mean episode reward: -47.5053098301553, num_cumulative_constraints: 23571, agent episode reward: [-47.5053098301553], time: 46.698
steps: 3091035, episodes: 74000, mean episode reward: -48.15668133580557, num_cumulative_constraints: 23867, agent episode reward: [-48.15668133580557], time: 48.428
steps: 3135615, episodes: 75000, mean episode reward: -48.90294686034907, num_cumulative_constraints: 24279, agent episode reward: [-48.90294686034907], time: 48.48
steps: 3179656, episodes: 76000, mean episode reward: -48.729879828525775, num_cumulative_constraints: 24675, agent episode reward: [-48.729879828525775], time: 48.697
steps: 3222724, episodes: 77000, mean episode reward: -47.782343730987904, num_cumulative_constraints: 25021, agent episode reward: [-47.782343730987904], time: 44.807
steps: 3265040, episodes: 78000, mean episode reward: -47.5492992542959, num_cumulative_constraints: 25399, agent episode reward: [-47.5492992542959], time: 45.771
steps: 3307419, episodes: 79000, mean episode reward: -47.80659167971801, num_cumulative_constraints: 25775, agent episode reward: [-47.80659167971801], time: 48.266
steps: 3349232, episodes: 80000, mean episode reward: -47.16770250951929, num_cumulative_constraints: 26109, agent episode reward: [-47.16770250951929], time: 45.376
steps: 3390965, episodes: 81000, mean episode reward: -47.39205906365407, num_cumulative_constraints: 26428, agent episode reward: [-47.39205906365407], time: 49.29
steps: 3432650, episodes: 82000, mean episode reward: -47.168697503816, num_cumulative_constraints: 26777, agent episode reward: [-47.168697503816], time: 46.724
steps: 3474209, episodes: 83000, mean episode reward: -47.328257290650456, num_cumulative_constraints: 27106, agent episode reward: [-47.328257290650456], time: 46.616
steps: 3515534, episodes: 84000, mean episode reward: -47.10215096312704, num_cumulative_constraints: 27499, agent episode reward: [-47.10215096312704], time: 46.419
steps: 3557136, episodes: 85000, mean episode reward: -47.6417596916499, num_cumulative_constraints: 27883, agent episode reward: [-47.6417596916499], time: 44.793
steps: 3598834, episodes: 86000, mean episode reward: -47.57833488380155, num_cumulative_constraints: 28196, agent episode reward: [-47.57833488380155], time: 44.884
steps: 3640351, episodes: 87000, mean episode reward: -47.50674630422323, num_cumulative_constraints: 28637, agent episode reward: [-47.50674630422323], time: 46.024
steps: 3681674, episodes: 88000, mean episode reward: -47.09981747212917, num_cumulative_constraints: 28940, agent episode reward: [-47.09981747212917], time: 47.392
steps: 3722924, episodes: 89000, mean episode reward: -47.0900764241806, num_cumulative_constraints: 29238, agent episode reward: [-47.0900764241806], time: 49.55
steps: 3764367, episodes: 90000, mean episode reward: -47.26057804949918, num_cumulative_constraints: 29526, agent episode reward: [-47.26057804949918], time: 48.84
steps: 3805624, episodes: 91000, mean episode reward: -47.2365916272673, num_cumulative_constraints: 29905, agent episode reward: [-47.2365916272673], time: 44.638
steps: 3846751, episodes: 92000, mean episode reward: -47.20667715252931, num_cumulative_constraints: 30277, agent episode reward: [-47.20667715252931], time: 45.545
steps: 3887877, episodes: 93000, mean episode reward: -46.55778518760581, num_cumulative_constraints: 30512, agent episode reward: [-46.55778518760581], time: 45.897
steps: 3928923, episodes: 94000, mean episode reward: -47.05666950307208, num_cumulative_constraints: 30815, agent episode reward: [-47.05666950307208], time: 47.024
steps: 3970273, episodes: 95000, mean episode reward: -47.6224645477525, num_cumulative_constraints: 31207, agent episode reward: [-47.6224645477525], time: 49.295
steps: 4011703, episodes: 96000, mean episode reward: -47.414224096680925, num_cumulative_constraints: 31534, agent episode reward: [-47.414224096680925], time: 49.096
steps: 4053099, episodes: 97000, mean episode reward: -47.15575726906077, num_cumulative_constraints: 31880, agent episode reward: [-47.15575726906077], time: 46.197
steps: 4094452, episodes: 98000, mean episode reward: -47.40261964566864, num_cumulative_constraints: 32208, agent episode reward: [-47.40261964566864], time: 45.302
steps: 4135651, episodes: 99000, mean episode reward: -47.19248200889584, num_cumulative_constraints: 32507, agent episode reward: [-47.19248200889584], time: 45.927
steps: 4176940, episodes: 100000, mean episode reward: -46.949792524050785, num_cumulative_constraints: 32792, agent episode reward: [-46.949792524050785], time: 46.085
steps: 4218155, episodes: 101000, mean episode reward: -47.2528021988697, num_cumulative_constraints: 33155, agent episode reward: [-47.2528021988697], time: 46.34
steps: 4259447, episodes: 102000, mean episode reward: -47.34705215252658, num_cumulative_constraints: 33527, agent episode reward: [-47.34705215252658], time: 48.698
steps: 4300891, episodes: 103000, mean episode reward: -47.421159149607135, num_cumulative_constraints: 33925, agent episode reward: [-47.421159149607135], time: 48.338
steps: 4342113, episodes: 104000, mean episode reward: -47.205572494213826, num_cumulative_constraints: 34266, agent episode reward: [-47.205572494213826], time: 45.424
steps: 4383481, episodes: 105000, mean episode reward: -47.61010368803591, num_cumulative_constraints: 34691, agent episode reward: [-47.61010368803591], time: 45.873
steps: 4425076, episodes: 106000, mean episode reward: -47.51666292855391, num_cumulative_constraints: 35038, agent episode reward: [-47.51666292855391], time: 46.563
steps: 4466406, episodes: 107000, mean episode reward: -47.360849786911146, num_cumulative_constraints: 35437, agent episode reward: [-47.360849786911146], time: 46.89
steps: 4507976, episodes: 108000, mean episode reward: -47.78454818165394, num_cumulative_constraints: 35876, agent episode reward: [-47.78454818165394], time: 47.296
steps: 4549405, episodes: 109000, mean episode reward: -47.31069870952531, num_cumulative_constraints: 36263, agent episode reward: [-47.31069870952531], time: 48.503
steps: 4590905, episodes: 110000, mean episode reward: -47.40280891945973, num_cumulative_constraints: 36649, agent episode reward: [-47.40280891945973], time: 49.587
steps: 4632073, episodes: 111000, mean episode reward: -47.05581570099095, num_cumulative_constraints: 36997, agent episode reward: [-47.05581570099095], time: 47.104
steps: 4673381, episodes: 112000, mean episode reward: -47.637300775299806, num_cumulative_constraints: 37431, agent episode reward: [-47.637300775299806], time: 44.918
steps: 4714702, episodes: 113000, mean episode reward: -47.26544854307518, num_cumulative_constraints: 37790, agent episode reward: [-47.26544854307518], time: 45.796
steps: 4756035, episodes: 114000, mean episode reward: -47.23944520994542, num_cumulative_constraints: 38222, agent episode reward: [-47.23944520994542], time: 46.312
steps: 4797292, episodes: 115000, mean episode reward: -47.485964572450705, num_cumulative_constraints: 38673, agent episode reward: [-47.485964572450705], time: 50.08
steps: 4838579, episodes: 116000, mean episode reward: -47.303225548702464, num_cumulative_constraints: 39094, agent episode reward: [-47.303225548702464], time: 48.041
steps: 4879648, episodes: 117000, mean episode reward: -46.91058103293568, num_cumulative_constraints: 39391, agent episode reward: [-46.91058103293568], time: 47.757
steps: 4920890, episodes: 118000, mean episode reward: -47.12495500504663, num_cumulative_constraints: 39705, agent episode reward: [-47.12495500504663], time: 46.207
steps: 4962208, episodes: 119000, mean episode reward: -46.83214561900512, num_cumulative_constraints: 40038, agent episode reward: [-46.83214561900512], time: 47.506
steps: 5003347, episodes: 120000, mean episode reward: -47.11339799794716, num_cumulative_constraints: 40383, agent episode reward: [-47.11339799794716], time: 45.534
steps: 5044527, episodes: 121000, mean episode reward: -47.11313105720842, num_cumulative_constraints: 40756, agent episode reward: [-47.11313105720842], time: 46.089
steps: 5085570, episodes: 122000, mean episode reward: -47.11180026088262, num_cumulative_constraints: 41134, agent episode reward: [-47.11180026088262], time: 45.919
steps: 5126962, episodes: 123000, mean episode reward: -47.18312490972725, num_cumulative_constraints: 41457, agent episode reward: [-47.18312490972725], time: 48.06
steps: 5167918, episodes: 124000, mean episode reward: -46.78576161342222, num_cumulative_constraints: 41864, agent episode reward: [-46.78576161342222], time: 46.305
steps: 5209062, episodes: 125000, mean episode reward: -46.64439547570052, num_cumulative_constraints: 42208, agent episode reward: [-46.64439547570052], time: 47.602
steps: 5250457, episodes: 126000, mean episode reward: -46.772352940079955, num_cumulative_constraints: 42503, agent episode reward: [-46.772352940079955], time: 48.031
steps: 5291587, episodes: 127000, mean episode reward: -46.71697347941909, num_cumulative_constraints: 42827, agent episode reward: [-46.71697347941909], time: 44.654
steps: 5332928, episodes: 128000, mean episode reward: -47.016222848486485, num_cumulative_constraints: 43173, agent episode reward: [-47.016222848486485], time: 46.197
steps: 5374452, episodes: 129000, mean episode reward: -47.16754143522504, num_cumulative_constraints: 43469, agent episode reward: [-47.16754143522504], time: 46.425
steps: 5415856, episodes: 130000, mean episode reward: -47.27711444235618, num_cumulative_constraints: 43835, agent episode reward: [-47.27711444235618], time: 47.539
steps: 5457060, episodes: 131000, mean episode reward: -46.5845165988359, num_cumulative_constraints: 44130, agent episode reward: [-46.5845165988359], time: 46.075
steps: 5498490, episodes: 132000, mean episode reward: -47.01296814893163, num_cumulative_constraints: 44410, agent episode reward: [-47.01296814893163], time: 46.145
steps: 5539752, episodes: 133000, mean episode reward: -46.96794290898035, num_cumulative_constraints: 44776, agent episode reward: [-46.96794290898035], time: 49.107
steps: 5581039, episodes: 134000, mean episode reward: -46.848496445373605, num_cumulative_constraints: 45080, agent episode reward: [-46.848496445373605], time: 45.738
steps: 5622349, episodes: 135000, mean episode reward: -46.915555270101315, num_cumulative_constraints: 45418, agent episode reward: [-46.915555270101315], time: 46.158
steps: 5663597, episodes: 136000, mean episode reward: -46.636260071907365, num_cumulative_constraints: 45759, agent episode reward: [-46.636260071907365], time: 46.512
steps: 5705217, episodes: 137000, mean episode reward: -46.8163159273163, num_cumulative_constraints: 46064, agent episode reward: [-46.8163159273163], time: 47.605
steps: 5746297, episodes: 138000, mean episode reward: -46.65683622578156, num_cumulative_constraints: 46445, agent episode reward: [-46.65683622578156], time: 46.505
steps: 5787471, episodes: 139000, mean episode reward: -46.88901402295391, num_cumulative_constraints: 46830, agent episode reward: [-46.88901402295391], time: 47.565
steps: 5828862, episodes: 140000, mean episode reward: -46.77693501262121, num_cumulative_constraints: 47218, agent episode reward: [-46.77693501262121], time: 47.674
steps: 5870200, episodes: 141000, mean episode reward: -47.352970550350534, num_cumulative_constraints: 47651, agent episode reward: [-47.352970550350534], time: 46.232
steps: 5911231, episodes: 142000, mean episode reward: -46.66682711231474, num_cumulative_constraints: 48020, agent episode reward: [-46.66682711231474], time: 46.508
steps: 5952490, episodes: 143000, mean episode reward: -46.79305531995568, num_cumulative_constraints: 48348, agent episode reward: [-46.79305531995568], time: 46.13
steps: 5993751, episodes: 144000, mean episode reward: -47.038029484485264, num_cumulative_constraints: 48763, agent episode reward: [-47.038029484485264], time: 47.145
steps: 6034970, episodes: 145000, mean episode reward: -46.865224334081944, num_cumulative_constraints: 49124, agent episode reward: [-46.865224334081944], time: 47.002
steps: 6076360, episodes: 146000, mean episode reward: -46.92801411874046, num_cumulative_constraints: 49510, agent episode reward: [-46.92801411874046], time: 46.312
steps: 6118013, episodes: 147000, mean episode reward: -46.990708464888826, num_cumulative_constraints: 49817, agent episode reward: [-46.990708464888826], time: 47.874
steps: 6159086, episodes: 148000, mean episode reward: -46.56713024376775, num_cumulative_constraints: 50130, agent episode reward: [-46.56713024376775], time: 46.884
steps: 6200325, episodes: 149000, mean episode reward: -46.44263153768642, num_cumulative_constraints: 50322, agent episode reward: [-46.44263153768642], time: 48.483
steps: 6241547, episodes: 150000, mean episode reward: -46.86904791129962, num_cumulative_constraints: 50680, agent episode reward: [-46.86904791129962], time: 47.61
steps: 6282983, episodes: 151000, mean episode reward: -46.593576337619396, num_cumulative_constraints: 50911, agent episode reward: [-46.593576337619396], time: 46.122
steps: 6324301, episodes: 152000, mean episode reward: -46.81197031029769, num_cumulative_constraints: 51211, agent episode reward: [-46.81197031029769], time: 46.443
steps: 6365663, episodes: 153000, mean episode reward: -47.10195055017938, num_cumulative_constraints: 51596, agent episode reward: [-47.10195055017938], time: 46.888
steps: 6406881, episodes: 154000, mean episode reward: -46.50598470278518, num_cumulative_constraints: 51884, agent episode reward: [-46.50598470278518], time: 47.554
steps: 6448077, episodes: 155000, mean episode reward: -46.79814331703847, num_cumulative_constraints: 52208, agent episode reward: [-46.79814331703847], time: 46.974
steps: 6489270, episodes: 156000, mean episode reward: -47.01985224806736, num_cumulative_constraints: 52555, agent episode reward: [-47.01985224806736], time: 48.298
steps: 6530095, episodes: 157000, mean episode reward: -46.57339212389924, num_cumulative_constraints: 52825, agent episode reward: [-46.57339212389924], time: 45.919
steps: 6571526, episodes: 158000, mean episode reward: -47.52216123903307, num_cumulative_constraints: 53260, agent episode reward: [-47.52216123903307], time: 47.482
steps: 6612549, episodes: 159000, mean episode reward: -46.99667861460876, num_cumulative_constraints: 53651, agent episode reward: [-46.99667861460876], time: 46.003
steps: 6653903, episodes: 160000, mean episode reward: -47.23896223317564, num_cumulative_constraints: 54020, agent episode reward: [-47.23896223317564], time: 46.514
steps: 6695161, episodes: 161000, mean episode reward: -46.91363459332152, num_cumulative_constraints: 54376, agent episode reward: [-46.91363459332152], time: 45.888
steps: 6736113, episodes: 162000, mean episode reward: -46.94254087358389, num_cumulative_constraints: 54718, agent episode reward: [-46.94254087358389], time: 46.599
steps: 6777381, episodes: 163000, mean episode reward: -47.0184547996069, num_cumulative_constraints: 55047, agent episode reward: [-47.0184547996069], time: 46.812
steps: 6818572, episodes: 164000, mean episode reward: -47.12101456248463, num_cumulative_constraints: 55422, agent episode reward: [-47.12101456248463], time: 47.32
steps: 6859624, episodes: 165000, mean episode reward: -46.93058619957945, num_cumulative_constraints: 55767, agent episode reward: [-46.93058619957945], time: 49.723
steps: 6900714, episodes: 166000, mean episode reward: -46.596246292100915, num_cumulative_constraints: 56066, agent episode reward: [-46.596246292100915], time: 47.875
steps: 6941774, episodes: 167000, mean episode reward: -47.22100517296734, num_cumulative_constraints: 56527, agent episode reward: [-47.22100517296734], time: 46.638
steps: 6983078, episodes: 168000, mean episode reward: -46.89673982629631, num_cumulative_constraints: 56888, agent episode reward: [-46.89673982629631], time: 47.027
steps: 7024512, episodes: 169000, mean episode reward: -47.42947265599048, num_cumulative_constraints: 57273, agent episode reward: [-47.42947265599048], time: 46.407
steps: 7065935, episodes: 170000, mean episode reward: -47.32561574736511, num_cumulative_constraints: 57661, agent episode reward: [-47.32561574736511], time: 46.11
steps: 7107250, episodes: 171000, mean episode reward: -46.81200251066802, num_cumulative_constraints: 58044, agent episode reward: [-46.81200251066802], time: 47.446
steps: 7148740, episodes: 172000, mean episode reward: -47.02861530217331, num_cumulative_constraints: 58353, agent episode reward: [-47.02861530217331], time: 47.935
steps: 7189942, episodes: 173000, mean episode reward: -46.944056313848705, num_cumulative_constraints: 58689, agent episode reward: [-46.944056313848705], time: 47.349
steps: 7231446, episodes: 174000, mean episode reward: -47.17306542668989, num_cumulative_constraints: 59035, agent episode reward: [-47.17306542668989], time: 46.767
steps: 7273049, episodes: 175000, mean episode reward: -47.409216604298535, num_cumulative_constraints: 59422, agent episode reward: [-47.409216604298535], time: 47.863
steps: 7314596, episodes: 176000, mean episode reward: -47.01043139174898, num_cumulative_constraints: 59735, agent episode reward: [-47.01043139174898], time: 47.74
steps: 7356101, episodes: 177000, mean episode reward: -47.70189705982821, num_cumulative_constraints: 60130, agent episode reward: [-47.70189705982821], time: 48.555
steps: 7397936, episodes: 178000, mean episode reward: -47.63392334952956, num_cumulative_constraints: 60520, agent episode reward: [-47.63392334952956], time: 46.754
steps: 7439726, episodes: 179000, mean episode reward: -48.02519704703476, num_cumulative_constraints: 60969, agent episode reward: [-48.02519704703476], time: 47.08
steps: 7481998, episodes: 180000, mean episode reward: -47.976818085032676, num_cumulative_constraints: 61401, agent episode reward: [-47.976818085032676], time: 46.478
steps: 7523858, episodes: 181000, mean episode reward: -47.52362333008453, num_cumulative_constraints: 61760, agent episode reward: [-47.52362333008453], time: 48.151
steps: 7565805, episodes: 182000, mean episode reward: -47.373818604456886, num_cumulative_constraints: 62066, agent episode reward: [-47.373818604456886], time: 47.546
steps: 7607791, episodes: 183000, mean episode reward: -47.34045038967203, num_cumulative_constraints: 62365, agent episode reward: [-47.34045038967203], time: 47.564
steps: 7649372, episodes: 184000, mean episode reward: -47.27848416660149, num_cumulative_constraints: 62721, agent episode reward: [-47.27848416660149], time: 47.297
steps: 7691955, episodes: 185000, mean episode reward: -47.82204113860215, num_cumulative_constraints: 63013, agent episode reward: [-47.82204113860215], time: 49.025
steps: 7733438, episodes: 186000, mean episode reward: -46.761711460108735, num_cumulative_constraints: 63265, agent episode reward: [-46.761711460108735], time: 48.838
steps: 7774744, episodes: 187000, mean episode reward: -46.937539564568596, num_cumulative_constraints: 63552, agent episode reward: [-46.937539564568596], time: 45.85
steps: 7816675, episodes: 188000, mean episode reward: -47.25952291979662, num_cumulative_constraints: 63829, agent episode reward: [-47.25952291979662], time: 48.843
steps: 7858325, episodes: 189000, mean episode reward: -47.80498855350953, num_cumulative_constraints: 64274, agent episode reward: [-47.80498855350953], time: 47.805
steps: 7899847, episodes: 190000, mean episode reward: -47.526097623776856, num_cumulative_constraints: 64658, agent episode reward: [-47.526097623776856], time: 47.258
steps: 7941610, episodes: 191000, mean episode reward: -47.59096153487122, num_cumulative_constraints: 64948, agent episode reward: [-47.59096153487122], time: 46.839
steps: 7983276, episodes: 192000, mean episode reward: -47.72433196237753, num_cumulative_constraints: 65305, agent episode reward: [-47.72433196237753], time: 47.276
steps: 8024959, episodes: 193000, mean episode reward: -47.529822868608214, num_cumulative_constraints: 65621, agent episode reward: [-47.529822868608214], time: 50.622
steps: 8066414, episodes: 194000, mean episode reward: -47.1926328971285, num_cumulative_constraints: 65951, agent episode reward: [-47.1926328971285], time: 47.087
steps: 8108078, episodes: 195000, mean episode reward: -47.576851281003314, num_cumulative_constraints: 66200, agent episode reward: [-47.576851281003314], time: 50.635
steps: 8150139, episodes: 196000, mean episode reward: -48.127352501164964, num_cumulative_constraints: 66520, agent episode reward: [-48.127352501164964], time: 49.05
steps: 8192001, episodes: 197000, mean episode reward: -48.211161054206094, num_cumulative_constraints: 66886, agent episode reward: [-48.211161054206094], time: 47.635
steps: 8233733, episodes: 198000, mean episode reward: -48.04126263219516, num_cumulative_constraints: 67225, agent episode reward: [-48.04126263219516], time: 45.423
steps: 8275389, episodes: 199000, mean episode reward: -48.0036100535609, num_cumulative_constraints: 67557, agent episode reward: [-48.0036100535609], time: 46.523
steps: 8316791, episodes: 200000, mean episode reward: -47.140828474114144, num_cumulative_constraints: 67890, agent episode reward: [-47.140828474114144], time: 46.585
steps: 8358173, episodes: 201000, mean episode reward: -47.034888058203606, num_cumulative_constraints: 68127, agent episode reward: [-47.034888058203606], time: 46.668
steps: 8399761, episodes: 202000, mean episode reward: -47.30936016094881, num_cumulative_constraints: 68442, agent episode reward: [-47.30936016094881], time: 46.719
steps: 8440859, episodes: 203000, mean episode reward: -47.175503178972704, num_cumulative_constraints: 68799, agent episode reward: [-47.175503178972704], time: 49.367
steps: 8482111, episodes: 204000, mean episode reward: -47.148196586482925, num_cumulative_constraints: 69104, agent episode reward: [-47.148196586482925], time: 49.597
steps: 8523412, episodes: 205000, mean episode reward: -47.16622953083897, num_cumulative_constraints: 69490, agent episode reward: [-47.16622953083897], time: 46.339
steps: 8564889, episodes: 206000, mean episode reward: -47.27962407540716, num_cumulative_constraints: 69799, agent episode reward: [-47.27962407540716], time: 47.939
steps: 8606419, episodes: 207000, mean episode reward: -47.25515075406959, num_cumulative_constraints: 70104, agent episode reward: [-47.25515075406959], time: 46.374
steps: 8647812, episodes: 208000, mean episode reward: -47.11629235652593, num_cumulative_constraints: 70425, agent episode reward: [-47.11629235652593], time: 45.491
steps: 8689606, episodes: 209000, mean episode reward: -47.19594243167208, num_cumulative_constraints: 70728, agent episode reward: [-47.19594243167208], time: 47.317
steps: 8731271, episodes: 210000, mean episode reward: -47.56324122396657, num_cumulative_constraints: 71062, agent episode reward: [-47.56324122396657], time: 48.498
steps: 8772570, episodes: 211000, mean episode reward: -47.22014858039901, num_cumulative_constraints: 71413, agent episode reward: [-47.22014858039901], time: 47.216
steps: 8814322, episodes: 212000, mean episode reward: -47.681065956214795, num_cumulative_constraints: 71736, agent episode reward: [-47.681065956214795], time: 47.641
steps: 8855760, episodes: 213000, mean episode reward: -47.228000305733744, num_cumulative_constraints: 72026, agent episode reward: [-47.228000305733744], time: 47.075
steps: 8897152, episodes: 214000, mean episode reward: -47.47010942891617, num_cumulative_constraints: 72316, agent episode reward: [-47.47010942891617], time: 48.081
steps: 8938834, episodes: 215000, mean episode reward: -47.69753627875283, num_cumulative_constraints: 72631, agent episode reward: [-47.69753627875283], time: 46.765
steps: 8980363, episodes: 216000, mean episode reward: -47.36539271084701, num_cumulative_constraints: 72965, agent episode reward: [-47.36539271084701], time: 47.215
steps: 9021731, episodes: 217000, mean episode reward: -47.47350480416123, num_cumulative_constraints: 73283, agent episode reward: [-47.47350480416123], time: 47.008
steps: 9063375, episodes: 218000, mean episode reward: -47.67384759747602, num_cumulative_constraints: 73677, agent episode reward: [-47.67384759747602], time: 47.712
steps: 9104651, episodes: 219000, mean episode reward: -47.39280879750784, num_cumulative_constraints: 74002, agent episode reward: [-47.39280879750784], time: 47.89
steps: 9145895, episodes: 220000, mean episode reward: -47.32308466706585, num_cumulative_constraints: 74370, agent episode reward: [-47.32308466706585], time: 46.626
steps: 9187319, episodes: 221000, mean episode reward: -47.3464991354721, num_cumulative_constraints: 74695, agent episode reward: [-47.3464991354721], time: 46.757
steps: 9228582, episodes: 222000, mean episode reward: -47.01074491881179, num_cumulative_constraints: 74999, agent episode reward: [-47.01074491881179], time: 47.68
steps: 9270121, episodes: 223000, mean episode reward: -47.39981131526257, num_cumulative_constraints: 75331, agent episode reward: [-47.39981131526257], time: 49.839
steps: 9311416, episodes: 224000, mean episode reward: -47.148062880635564, num_cumulative_constraints: 75658, agent episode reward: [-47.148062880635564], time: 48.01
steps: 9352819, episodes: 225000, mean episode reward: -47.342980619750286, num_cumulative_constraints: 75947, agent episode reward: [-47.342980619750286], time: 48.63
steps: 9393971, episodes: 226000, mean episode reward: -47.243570398475775, num_cumulative_constraints: 76274, agent episode reward: [-47.243570398475775], time: 46.978
steps: 9435243, episodes: 227000, mean episode reward: -46.932805310388176, num_cumulative_constraints: 76519, agent episode reward: [-46.932805310388176], time: 47.604
steps: 9476518, episodes: 228000, mean episode reward: -47.66217324243147, num_cumulative_constraints: 76892, agent episode reward: [-47.66217324243147], time: 45.988
steps: 9517627, episodes: 229000, mean episode reward: -46.909875558427935, num_cumulative_constraints: 77130, agent episode reward: [-46.909875558427935], time: 47.273
steps: 9558959, episodes: 230000, mean episode reward: -47.626190234923, num_cumulative_constraints: 77481, agent episode reward: [-47.626190234923], time: 46.005
steps: 9600418, episodes: 231000, mean episode reward: -47.23592008117589, num_cumulative_constraints: 77781, agent episode reward: [-47.23592008117589], time: 47.356
steps: 9641362, episodes: 232000, mean episode reward: -46.985647067727975, num_cumulative_constraints: 78091, agent episode reward: [-46.985647067727975], time: 46.55
steps: 9682423, episodes: 233000, mean episode reward: -47.284143933425284, num_cumulative_constraints: 78413, agent episode reward: [-47.284143933425284], time: 48.899
steps: 9723666, episodes: 234000, mean episode reward: -47.89042193556538, num_cumulative_constraints: 78778, agent episode reward: [-47.89042193556538], time: 47.112
steps: 9765092, episodes: 235000, mean episode reward: -47.87766200727945, num_cumulative_constraints: 79155, agent episode reward: [-47.87766200727945], time: 46.901
steps: 9806338, episodes: 236000, mean episode reward: -47.42131588550113, num_cumulative_constraints: 79478, agent episode reward: [-47.42131588550113], time: 48.182
steps: 9847539, episodes: 237000, mean episode reward: -47.56721251746037, num_cumulative_constraints: 79779, agent episode reward: [-47.56721251746037], time: 47.65
steps: 9889075, episodes: 238000, mean episode reward: -47.77883648341375, num_cumulative_constraints: 80119, agent episode reward: [-47.77883648341375], time: 47.238
steps: 9931143, episodes: 239000, mean episode reward: -48.55273048661553, num_cumulative_constraints: 80607, agent episode reward: [-48.55273048661553], time: 48.824
steps: 9973094, episodes: 240000, mean episode reward: -47.59687803380598, num_cumulative_constraints: 80949, agent episode reward: [-47.59687803380598], time: 48.974
steps: 10015059, episodes: 241000, mean episode reward: -48.03182583497463, num_cumulative_constraints: 81273, agent episode reward: [-48.03182583497463], time: 47.504
steps: 10056773, episodes: 242000, mean episode reward: -47.52137771671853, num_cumulative_constraints: 81576, agent episode reward: [-47.52137771671853], time: 45.726
steps: 10098894, episodes: 243000, mean episode reward: -48.33661375986298, num_cumulative_constraints: 82040, agent episode reward: [-48.33661375986298], time: 47.68
steps: 10141124, episodes: 244000, mean episode reward: -48.10350733661598, num_cumulative_constraints: 82413, agent episode reward: [-48.10350733661598], time: 46.889
steps: 10183842, episodes: 245000, mean episode reward: -48.267594442107004, num_cumulative_constraints: 82783, agent episode reward: [-48.267594442107004], time: 48.473
steps: 10225754, episodes: 246000, mean episode reward: -47.95543181425384, num_cumulative_constraints: 83200, agent episode reward: [-47.95543181425384], time: 47.073
steps: 10267855, episodes: 247000, mean episode reward: -48.002885568222275, num_cumulative_constraints: 83580, agent episode reward: [-48.002885568222275], time: 49.838
steps: 10309795, episodes: 248000, mean episode reward: -47.66936073943411, num_cumulative_constraints: 83994, agent episode reward: [-47.66936073943411], time: 49.178
steps: 10351443, episodes: 249000, mean episode reward: -47.689561630623125, num_cumulative_constraints: 84364, agent episode reward: [-47.689561630623125], time: 47.971
steps: 10392959, episodes: 250000, mean episode reward: -47.405697971155035, num_cumulative_constraints: 84694, agent episode reward: [-47.405697971155035], time: 47.754
steps: 10435049, episodes: 251000, mean episode reward: -48.071971696025415, num_cumulative_constraints: 85014, agent episode reward: [-48.071971696025415], time: 46.289
steps: 10477223, episodes: 252000, mean episode reward: -47.88904216577965, num_cumulative_constraints: 85370, agent episode reward: [-47.88904216577965], time: 47.102
steps: 10519480, episodes: 253000, mean episode reward: -48.18345206965102, num_cumulative_constraints: 85775, agent episode reward: [-48.18345206965102], time: 46.29
steps: 10561718, episodes: 254000, mean episode reward: -48.00682968216985, num_cumulative_constraints: 86191, agent episode reward: [-48.00682968216985], time: 47.775
steps: 10603710, episodes: 255000, mean episode reward: -47.9658883347425, num_cumulative_constraints: 86538, agent episode reward: [-47.9658883347425], time: 47.521
steps: 10645518, episodes: 256000, mean episode reward: -47.64259709029159, num_cumulative_constraints: 86801, agent episode reward: [-47.64259709029159], time: 46.485
steps: 10686905, episodes: 257000, mean episode reward: -47.54269404608363, num_cumulative_constraints: 87161, agent episode reward: [-47.54269404608363], time: 48.475
steps: 10728820, episodes: 258000, mean episode reward: -47.57887717586357, num_cumulative_constraints: 87499, agent episode reward: [-47.57887717586357], time: 48.955
steps: 10770632, episodes: 259000, mean episode reward: -47.79016333661535, num_cumulative_constraints: 87869, agent episode reward: [-47.79016333661535], time: 46.575
steps: 10812492, episodes: 260000, mean episode reward: -47.187724713961266, num_cumulative_constraints: 88084, agent episode reward: [-47.187724713961266], time: 47.806
steps: 10854304, episodes: 261000, mean episode reward: -47.6481268247524, num_cumulative_constraints: 88393, agent episode reward: [-47.6481268247524], time: 47.627
steps: 10896061, episodes: 262000, mean episode reward: -48.021047257119626, num_cumulative_constraints: 88803, agent episode reward: [-48.021047257119626], time: 45.794
steps: 10937989, episodes: 263000, mean episode reward: -48.061411802718226, num_cumulative_constraints: 89148, agent episode reward: [-48.061411802718226], time: 48.002
steps: 10979751, episodes: 264000, mean episode reward: -47.18910640380796, num_cumulative_constraints: 89389, agent episode reward: [-47.18910640380796], time: 46.064
steps: 11021882, episodes: 265000, mean episode reward: -48.226268819680364, num_cumulative_constraints: 89780, agent episode reward: [-48.226268819680364], time: 48.041
steps: 11064876, episodes: 266000, mean episode reward: -48.444356607268084, num_cumulative_constraints: 90115, agent episode reward: [-48.444356607268084], time: 49.186
steps: 11107639, episodes: 267000, mean episode reward: -47.85724072112783, num_cumulative_constraints: 90417, agent episode reward: [-47.85724072112783], time: 50.417
steps: 11150105, episodes: 268000, mean episode reward: -48.33699608536397, num_cumulative_constraints: 90752, agent episode reward: [-48.33699608536397], time: 50.048
steps: 11193024, episodes: 269000, mean episode reward: -48.811803582403165, num_cumulative_constraints: 91126, agent episode reward: [-48.811803582403165], time: 49.788
steps: 11235352, episodes: 270000, mean episode reward: -48.348876036960675, num_cumulative_constraints: 91473, agent episode reward: [-48.348876036960675], time: 47.646
steps: 11276418, episodes: 271000, mean episode reward: -47.402551653948976, num_cumulative_constraints: 91886, agent episode reward: [-47.402551653948976], time: 46.213
steps: 11317669, episodes: 272000, mean episode reward: -47.82296245369658, num_cumulative_constraints: 92231, agent episode reward: [-47.82296245369658], time: 48.119
steps: 11359187, episodes: 273000, mean episode reward: -47.55061160128402, num_cumulative_constraints: 92564, agent episode reward: [-47.55061160128402], time: 47.301
steps: 11400716, episodes: 274000, mean episode reward: -47.42897345270055, num_cumulative_constraints: 92868, agent episode reward: [-47.42897345270055], time: 50.06
steps: 11442074, episodes: 275000, mean episode reward: -47.81832095155667, num_cumulative_constraints: 93249, agent episode reward: [-47.81832095155667], time: 46.962
steps: 11483657, episodes: 276000, mean episode reward: -47.732550068627766, num_cumulative_constraints: 93593, agent episode reward: [-47.732550068627766], time: 49.112
steps: 11525657, episodes: 277000, mean episode reward: -48.06643091023369, num_cumulative_constraints: 93944, agent episode reward: [-48.06643091023369], time: 49.31
steps: 11567596, episodes: 278000, mean episode reward: -48.302221246529996, num_cumulative_constraints: 94362, agent episode reward: [-48.302221246529996], time: 47.689
steps: 11609409, episodes: 279000, mean episode reward: -47.70880603564223, num_cumulative_constraints: 94654, agent episode reward: [-47.70880603564223], time: 46.033
steps: 11650908, episodes: 280000, mean episode reward: -47.482943280279464, num_cumulative_constraints: 94930, agent episode reward: [-47.482943280279464], time: 46.952
steps: 11692891, episodes: 281000, mean episode reward: -47.59832512654182, num_cumulative_constraints: 95255, agent episode reward: [-47.59832512654182], time: 47.332
steps: 11734914, episodes: 282000, mean episode reward: -47.32422777108033, num_cumulative_constraints: 95494, agent episode reward: [-47.32422777108033], time: 66.452
steps: 11776886, episodes: 283000, mean episode reward: -47.875018318282926, num_cumulative_constraints: 95857, agent episode reward: [-47.875018318282926], time: 97.445
steps: 11818846, episodes: 284000, mean episode reward: -47.52478438052023, num_cumulative_constraints: 96073, agent episode reward: [-47.52478438052023], time: 52.835
steps: 11860385, episodes: 285000, mean episode reward: -47.50166701361694, num_cumulative_constraints: 96447, agent episode reward: [-47.50166701361694], time: 58.204
steps: 11901937, episodes: 286000, mean episode reward: -46.996554664082154, num_cumulative_constraints: 96708, agent episode reward: [-46.996554664082154], time: 52.608
steps: 11943469, episodes: 287000, mean episode reward: -47.1548566189685, num_cumulative_constraints: 96918, agent episode reward: [-47.1548566189685], time: 52.611
steps: 11985358, episodes: 288000, mean episode reward: -47.58522415764684, num_cumulative_constraints: 97190, agent episode reward: [-47.58522415764684], time: 57.783
steps: 12027002, episodes: 289000, mean episode reward: -47.6860062164555, num_cumulative_constraints: 97530, agent episode reward: [-47.6860062164555], time: 54.8
steps: 12068723, episodes: 290000, mean episode reward: -47.6505067295967, num_cumulative_constraints: 97889, agent episode reward: [-47.6505067295967], time: 55.871
steps: 12110512, episodes: 291000, mean episode reward: -47.17308734733195, num_cumulative_constraints: 98160, agent episode reward: [-47.17308734733195], time: 48.96
steps: 12152455, episodes: 292000, mean episode reward: -47.30191036521948, num_cumulative_constraints: 98483, agent episode reward: [-47.30191036521948], time: 48.247
steps: 12194223, episodes: 293000, mean episode reward: -46.968778566024206, num_cumulative_constraints: 98750, agent episode reward: [-46.968778566024206], time: 50.38
steps: 12236049, episodes: 294000, mean episode reward: -47.109655169849766, num_cumulative_constraints: 99052, agent episode reward: [-47.109655169849766], time: 52.741
steps: 12278327, episodes: 295000, mean episode reward: -47.45897975677647, num_cumulative_constraints: 99416, agent episode reward: [-47.45897975677647], time: 45.913
steps: 12320828, episodes: 296000, mean episode reward: -48.12497121347324, num_cumulative_constraints: 99868, agent episode reward: [-48.12497121347324], time: 48.713
steps: 12362931, episodes: 297000, mean episode reward: -47.28355879389303, num_cumulative_constraints: 100226, agent episode reward: [-47.28355879389303], time: 48.367
steps: 12404881, episodes: 298000, mean episode reward: -47.18171433482836, num_cumulative_constraints: 100570, agent episode reward: [-47.18171433482836], time: 50.398
steps: 12446642, episodes: 299000, mean episode reward: -47.192865207428575, num_cumulative_constraints: 100909, agent episode reward: [-47.192865207428575], time: 47.346
steps: 12488188, episodes: 300000, mean episode reward: -46.63914714830545, num_cumulative_constraints: 101171, agent episode reward: [-46.63914714830545], time: 48.051
steps: 12529980, episodes: 301000, mean episode reward: -46.916415949851626, num_cumulative_constraints: 101415, agent episode reward: [-46.916415949851626], time: 49.403
steps: 12571428, episodes: 302000, mean episode reward: -46.37035327123757, num_cumulative_constraints: 101624, agent episode reward: [-46.37035327123757], time: 45.206
steps: 12612913, episodes: 303000, mean episode reward: -46.96358017328842, num_cumulative_constraints: 101907, agent episode reward: [-46.96358017328842], time: 47.217
steps: 12654595, episodes: 304000, mean episode reward: -46.9037869828735, num_cumulative_constraints: 102232, agent episode reward: [-46.9037869828735], time: 48.314
steps: 12696336, episodes: 305000, mean episode reward: -46.90938184694065, num_cumulative_constraints: 102492, agent episode reward: [-46.90938184694065], time: 46.469
steps: 12738044, episodes: 306000, mean episode reward: -47.17728049203196, num_cumulative_constraints: 102810, agent episode reward: [-47.17728049203196], time: 49.66
steps: 12779977, episodes: 307000, mean episode reward: -47.2322049193373, num_cumulative_constraints: 103104, agent episode reward: [-47.2322049193373], time: 47.659
steps: 12821704, episodes: 308000, mean episode reward: -47.13726762872321, num_cumulative_constraints: 103400, agent episode reward: [-47.13726762872321], time: 45.365
steps: 12863302, episodes: 309000, mean episode reward: -46.72379961981492, num_cumulative_constraints: 103678, agent episode reward: [-46.72379961981492], time: 41.214
steps: 12905069, episodes: 310000, mean episode reward: -47.04952616788636, num_cumulative_constraints: 103937, agent episode reward: [-47.04952616788636], time: 43.929
steps: 12946838, episodes: 311000, mean episode reward: -47.18061960518268, num_cumulative_constraints: 104214, agent episode reward: [-47.18061960518268], time: 47.758
steps: 12988772, episodes: 312000, mean episode reward: -47.085839918383016, num_cumulative_constraints: 104466, agent episode reward: [-47.085839918383016], time: 48.876
steps: 13030704, episodes: 313000, mean episode reward: -47.13910799677249, num_cumulative_constraints: 104718, agent episode reward: [-47.13910799677249], time: 45.912
steps: 13072492, episodes: 314000, mean episode reward: -46.97146734515687, num_cumulative_constraints: 104996, agent episode reward: [-46.97146734515687], time: 46.691
steps: 13114554, episodes: 315000, mean episode reward: -47.14437001413891, num_cumulative_constraints: 105253, agent episode reward: [-47.14437001413891], time: 46.225
steps: 13156401, episodes: 316000, mean episode reward: -47.04110094324981, num_cumulative_constraints: 105527, agent episode reward: [-47.04110094324981], time: 47.897
steps: 13198378, episodes: 317000, mean episode reward: -47.15777694226378, num_cumulative_constraints: 105846, agent episode reward: [-47.15777694226378], time: 49.719
steps: 13240225, episodes: 318000, mean episode reward: -46.865100113653476, num_cumulative_constraints: 106092, agent episode reward: [-46.865100113653476], time: 45.128
steps: 13282359, episodes: 319000, mean episode reward: -47.17600371200463, num_cumulative_constraints: 106374, agent episode reward: [-47.17600371200463], time: 48.117
steps: 13324267, episodes: 320000, mean episode reward: -47.093063457846014, num_cumulative_constraints: 106627, agent episode reward: [-47.093063457846014], time: 49.918
steps: 13366089, episodes: 321000, mean episode reward: -46.82680781047582, num_cumulative_constraints: 106901, agent episode reward: [-46.82680781047582], time: 47.37
steps: 13408106, episodes: 322000, mean episode reward: -47.02298216733662, num_cumulative_constraints: 107198, agent episode reward: [-47.02298216733662], time: 47.066
steps: 13449937, episodes: 323000, mean episode reward: -46.720811425851416, num_cumulative_constraints: 107432, agent episode reward: [-46.720811425851416], time: 48.531
steps: 13491563, episodes: 324000, mean episode reward: -46.68967149945834, num_cumulative_constraints: 107735, agent episode reward: [-46.68967149945834], time: 40.911
steps: 13533214, episodes: 325000, mean episode reward: -46.95879645631841, num_cumulative_constraints: 108052, agent episode reward: [-46.95879645631841], time: 40.738
steps: 13574904, episodes: 326000, mean episode reward: -47.02340284372527, num_cumulative_constraints: 108355, agent episode reward: [-47.02340284372527], time: 38.449
steps: 13616604, episodes: 327000, mean episode reward: -47.09308845981813, num_cumulative_constraints: 108643, agent episode reward: [-47.09308845981813], time: 40.413
steps: 13658720, episodes: 328000, mean episode reward: -47.46283536956266, num_cumulative_constraints: 108965, agent episode reward: [-47.46283536956266], time: 39.118
steps: 13700545, episodes: 329000, mean episode reward: -46.85584200608598, num_cumulative_constraints: 109248, agent episode reward: [-46.85584200608598], time: 39.047
steps: 13742199, episodes: 330000, mean episode reward: -47.127674133506225, num_cumulative_constraints: 109588, agent episode reward: [-47.127674133506225], time: 38.758
steps: 13783705, episodes: 331000, mean episode reward: -46.75093811922641, num_cumulative_constraints: 109863, agent episode reward: [-46.75093811922641], time: 39.775
steps: 13825223, episodes: 332000, mean episode reward: -46.918068349784335, num_cumulative_constraints: 110141, agent episode reward: [-46.918068349784335], time: 39.401
steps: 13866813, episodes: 333000, mean episode reward: -47.22534334812772, num_cumulative_constraints: 110447, agent episode reward: [-47.22534334812772], time: 39.61
steps: 13908593, episodes: 334000, mean episode reward: -47.38890227177883, num_cumulative_constraints: 110751, agent episode reward: [-47.38890227177883], time: 39.857
steps: 13950087, episodes: 335000, mean episode reward: -46.75438695085491, num_cumulative_constraints: 111043, agent episode reward: [-46.75438695085491], time: 38.895
steps: 13991740, episodes: 336000, mean episode reward: -46.89102635207206, num_cumulative_constraints: 111307, agent episode reward: [-46.89102635207206], time: 38.713
steps: 14033395, episodes: 337000, mean episode reward: -47.28286737613893, num_cumulative_constraints: 111630, agent episode reward: [-47.28286737613893], time: 40.381
steps: 14074865, episodes: 338000, mean episode reward: -46.736871775869, num_cumulative_constraints: 111909, agent episode reward: [-46.736871775869], time: 38.924
steps: 14116555, episodes: 339000, mean episode reward: -46.928352165682035, num_cumulative_constraints: 112164, agent episode reward: [-46.928352165682035], time: 39.642
steps: 14158262, episodes: 340000, mean episode reward: -46.91103132528427, num_cumulative_constraints: 112432, agent episode reward: [-46.91103132528427], time: 39.067
steps: 14199416, episodes: 341000, mean episode reward: -46.301848498050326, num_cumulative_constraints: 112707, agent episode reward: [-46.301848498050326], time: 39.306
steps: 14240946, episodes: 342000, mean episode reward: -46.66737245805658, num_cumulative_constraints: 112937, agent episode reward: [-46.66737245805658], time: 39.58
steps: 14282863, episodes: 343000, mean episode reward: -47.223277083840195, num_cumulative_constraints: 113234, agent episode reward: [-47.223277083840195], time: 39.099
steps: 14324362, episodes: 344000, mean episode reward: -46.81725857873781, num_cumulative_constraints: 113491, agent episode reward: [-46.81725857873781], time: 41.052
steps: 14366090, episodes: 345000, mean episode reward: -46.87059062640467, num_cumulative_constraints: 113735, agent episode reward: [-46.87059062640467], time: 40.129
steps: 14407875, episodes: 346000, mean episode reward: -46.98750012956919, num_cumulative_constraints: 113985, agent episode reward: [-46.98750012956919], time: 40.452
steps: 14449360, episodes: 347000, mean episode reward: -46.68879008969165, num_cumulative_constraints: 114232, agent episode reward: [-46.68879008969165], time: 38.909
steps: 14491145, episodes: 348000, mean episode reward: -47.330895326859206, num_cumulative_constraints: 114519, agent episode reward: [-47.330895326859206], time: 39.047
steps: 14532819, episodes: 349000, mean episode reward: -47.02535881245046, num_cumulative_constraints: 114801, agent episode reward: [-47.02535881245046], time: 40.028
steps: 14574659, episodes: 350000, mean episode reward: -46.744150235861746, num_cumulative_constraints: 115004, agent episode reward: [-46.744150235861746], time: 39.223
steps: 14616443, episodes: 351000, mean episode reward: -46.730238370918656, num_cumulative_constraints: 115204, agent episode reward: [-46.730238370918656], time: 40.881
steps: 14658699, episodes: 352000, mean episode reward: -47.19911975045664, num_cumulative_constraints: 115447, agent episode reward: [-47.19911975045664], time: 39.324
steps: 14700969, episodes: 353000, mean episode reward: -47.022730672841696, num_cumulative_constraints: 115647, agent episode reward: [-47.022730672841696], time: 40.67
steps: 14743174, episodes: 354000, mean episode reward: -47.30701379845933, num_cumulative_constraints: 115886, agent episode reward: [-47.30701379845933], time: 41.549
steps: 14785459, episodes: 355000, mean episode reward: -47.42852411975569, num_cumulative_constraints: 116184, agent episode reward: [-47.42852411975569], time: 39.237
steps: 14827293, episodes: 356000, mean episode reward: -46.869881088058115, num_cumulative_constraints: 116431, agent episode reward: [-46.869881088058115], time: 39.717
steps: 14869582, episodes: 357000, mean episode reward: -47.076774807602, num_cumulative_constraints: 116656, agent episode reward: [-47.076774807602], time: 40.442
steps: 14911449, episodes: 358000, mean episode reward: -47.05525956859246, num_cumulative_constraints: 116905, agent episode reward: [-47.05525956859246], time: 41.69
steps: 14953733, episodes: 359000, mean episode reward: -47.346486257794986, num_cumulative_constraints: 117184, agent episode reward: [-47.346486257794986], time: 40.025
steps: 14995757, episodes: 360000, mean episode reward: -47.27350806515993, num_cumulative_constraints: 117518, agent episode reward: [-47.27350806515993], time: 40.683
steps: 15038109, episodes: 361000, mean episode reward: -47.37616271644332, num_cumulative_constraints: 117770, agent episode reward: [-47.37616271644332], time: 38.916
steps: 15080337, episodes: 362000, mean episode reward: -46.91912530963741, num_cumulative_constraints: 117967, agent episode reward: [-46.91912530963741], time: 39.652
steps: 15122361, episodes: 363000, mean episode reward: -47.09375778347257, num_cumulative_constraints: 118214, agent episode reward: [-47.09375778347257], time: 39.669
steps: 15164486, episodes: 364000, mean episode reward: -46.8544913176813, num_cumulative_constraints: 118453, agent episode reward: [-46.8544913176813], time: 39.501
steps: 15206328, episodes: 365000, mean episode reward: -47.03995825617468, num_cumulative_constraints: 118696, agent episode reward: [-47.03995825617468], time: 39.584
steps: 15248134, episodes: 366000, mean episode reward: -47.251699860542146, num_cumulative_constraints: 118941, agent episode reward: [-47.251699860542146], time: 39.012
steps: 15289718, episodes: 367000, mean episode reward: -46.880917182455896, num_cumulative_constraints: 119174, agent episode reward: [-46.880917182455896], time: 41.684
steps: 15331365, episodes: 368000, mean episode reward: -47.01207923212102, num_cumulative_constraints: 119431, agent episode reward: [-47.01207923212102], time: 38.39
steps: 15373195, episodes: 369000, mean episode reward: -47.05396461534711, num_cumulative_constraints: 119682, agent episode reward: [-47.05396461534711], time: 40.713
steps: 15415012, episodes: 370000, mean episode reward: -46.940165095010855, num_cumulative_constraints: 119924, agent episode reward: [-46.940165095010855], time: 40.121
steps: 15456679, episodes: 371000, mean episode reward: -47.248274951226115, num_cumulative_constraints: 120269, agent episode reward: [-47.248274951226115], time: 39.942
steps: 15498224, episodes: 372000, mean episode reward: -46.83938887267161, num_cumulative_constraints: 120505, agent episode reward: [-46.83938887267161], time: 38.544
steps: 15539853, episodes: 373000, mean episode reward: -46.87843670294991, num_cumulative_constraints: 120727, agent episode reward: [-46.87843670294991], time: 38.158
steps: 15581457, episodes: 374000, mean episode reward: -47.167093039357084, num_cumulative_constraints: 120977, agent episode reward: [-47.167093039357084], time: 39.496
steps: 15623034, episodes: 375000, mean episode reward: -46.83857688672718, num_cumulative_constraints: 121206, agent episode reward: [-46.83857688672718], time: 40.429
steps: 15664967, episodes: 376000, mean episode reward: -47.489495310750804, num_cumulative_constraints: 121512, agent episode reward: [-47.489495310750804], time: 39.319
steps: 15706725, episodes: 377000, mean episode reward: -47.32293597674997, num_cumulative_constraints: 121757, agent episode reward: [-47.32293597674997], time: 40.349
steps: 15748417, episodes: 378000, mean episode reward: -47.01665243016111, num_cumulative_constraints: 121977, agent episode reward: [-47.01665243016111], time: 38.813
steps: 15790091, episodes: 379000, mean episode reward: -46.94662168208869, num_cumulative_constraints: 122218, agent episode reward: [-46.94662168208869], time: 40.994
steps: 15831646, episodes: 380000, mean episode reward: -46.8674431183186, num_cumulative_constraints: 122496, agent episode reward: [-46.8674431183186], time: 38.838
steps: 15873355, episodes: 381000, mean episode reward: -47.08434576799539, num_cumulative_constraints: 122792, agent episode reward: [-47.08434576799539], time: 38.859
steps: 15915133, episodes: 382000, mean episode reward: -47.164426048209194, num_cumulative_constraints: 123080, agent episode reward: [-47.164426048209194], time: 40.358
steps: 15956948, episodes: 383000, mean episode reward: -46.587255068549936, num_cumulative_constraints: 123261, agent episode reward: [-46.587255068549936], time: 39.237
steps: 15998665, episodes: 384000, mean episode reward: -47.15072275061336, num_cumulative_constraints: 123576, agent episode reward: [-47.15072275061336], time: 39.866
steps: 16040356, episodes: 385000, mean episode reward: -46.61274114845458, num_cumulative_constraints: 123843, agent episode reward: [-46.61274114845458], time: 40.671
steps: 16082156, episodes: 386000, mean episode reward: -46.57706821419765, num_cumulative_constraints: 124086, agent episode reward: [-46.57706821419765], time: 39.527
steps: 16124080, episodes: 387000, mean episode reward: -47.17454433059289, num_cumulative_constraints: 124331, agent episode reward: [-47.17454433059289], time: 38.802
steps: 16165897, episodes: 388000, mean episode reward: -47.11395307084518, num_cumulative_constraints: 124621, agent episode reward: [-47.11395307084518], time: 38.503
steps: 16207619, episodes: 389000, mean episode reward: -46.73508975446751, num_cumulative_constraints: 124905, agent episode reward: [-46.73508975446751], time: 40.214
steps: 16249297, episodes: 390000, mean episode reward: -46.692237287377836, num_cumulative_constraints: 125135, agent episode reward: [-46.692237287377836], time: 38.456
steps: 16291036, episodes: 391000, mean episode reward: -46.716441796946185, num_cumulative_constraints: 125452, agent episode reward: [-46.716441796946185], time: 39.419
steps: 16332845, episodes: 392000, mean episode reward: -46.7355790016665, num_cumulative_constraints: 125654, agent episode reward: [-46.7355790016665], time: 41.362
steps: 16374561, episodes: 393000, mean episode reward: -46.554429314661824, num_cumulative_constraints: 125904, agent episode reward: [-46.554429314661824], time: 39.405
steps: 16416542, episodes: 394000, mean episode reward: -47.14405955105342, num_cumulative_constraints: 126188, agent episode reward: [-47.14405955105342], time: 39.795
steps: 16458507, episodes: 395000, mean episode reward: -46.88407164972588, num_cumulative_constraints: 126483, agent episode reward: [-46.88407164972588], time: 39.948
steps: 16500444, episodes: 396000, mean episode reward: -47.045564688775016, num_cumulative_constraints: 126746, agent episode reward: [-47.045564688775016], time: 39.098
steps: 16542537, episodes: 397000, mean episode reward: -46.71522588943796, num_cumulative_constraints: 127033, agent episode reward: [-46.71522588943796], time: 41.799
steps: 16584599, episodes: 398000, mean episode reward: -47.03568349976107, num_cumulative_constraints: 127347, agent episode reward: [-47.03568349976107], time: 40.844
steps: 16626433, episodes: 399000, mean episode reward: -46.83604629365368, num_cumulative_constraints: 127598, agent episode reward: [-46.83604629365368], time: 40.254
steps: 16668277, episodes: 400000, mean episode reward: -46.838997766224566, num_cumulative_constraints: 127905, agent episode reward: [-46.838997766224566], time: 39.911
steps: 16710399, episodes: 401000, mean episode reward: -46.83494334518575, num_cumulative_constraints: 128164, agent episode reward: [-46.83494334518575], time: 39.332
steps: 16752543, episodes: 402000, mean episode reward: -47.0764601393669, num_cumulative_constraints: 128466, agent episode reward: [-47.0764601393669], time: 39.76
steps: 16794567, episodes: 403000, mean episode reward: -47.00070284834083, num_cumulative_constraints: 128755, agent episode reward: [-47.00070284834083], time: 39.376
steps: 16836996, episodes: 404000, mean episode reward: -46.78579981674333, num_cumulative_constraints: 129031, agent episode reward: [-46.78579981674333], time: 38.696
steps: 16879414, episodes: 405000, mean episode reward: -46.923866439671, num_cumulative_constraints: 129284, agent episode reward: [-46.923866439671], time: 39.589
steps: 16921633, episodes: 406000, mean episode reward: -47.15431298964143, num_cumulative_constraints: 129543, agent episode reward: [-47.15431298964143], time: 41.559
steps: 16963673, episodes: 407000, mean episode reward: -46.91559899592187, num_cumulative_constraints: 129810, agent episode reward: [-46.91559899592187], time: 40.193
steps: 17005596, episodes: 408000, mean episode reward: -46.65426976769466, num_cumulative_constraints: 130070, agent episode reward: [-46.65426976769466], time: 40.138
steps: 17047473, episodes: 409000, mean episode reward: -46.84188568440838, num_cumulative_constraints: 130325, agent episode reward: [-46.84188568440838], time: 39.757
steps: 17089368, episodes: 410000, mean episode reward: -46.799708346732366, num_cumulative_constraints: 130633, agent episode reward: [-46.799708346732366], time: 39.183
steps: 17131040, episodes: 411000, mean episode reward: -46.59236681022727, num_cumulative_constraints: 130921, agent episode reward: [-46.59236681022727], time: 39.185
steps: 17172940, episodes: 412000, mean episode reward: -46.723991137737414, num_cumulative_constraints: 131170, agent episode reward: [-46.723991137737414], time: 39.463
steps: 17215155, episodes: 413000, mean episode reward: -47.17410350010512, num_cumulative_constraints: 131469, agent episode reward: [-47.17410350010512], time: 42.273
steps: 17256914, episodes: 414000, mean episode reward: -46.773184231794865, num_cumulative_constraints: 131725, agent episode reward: [-46.773184231794865], time: 38.587
steps: 17298675, episodes: 415000, mean episode reward: -46.89561461330914, num_cumulative_constraints: 132089, agent episode reward: [-46.89561461330914], time: 39.65
steps: 17340562, episodes: 416000, mean episode reward: -46.92245141838269, num_cumulative_constraints: 132376, agent episode reward: [-46.92245141838269], time: 39.248
steps: 17382727, episodes: 417000, mean episode reward: -47.07525493601296, num_cumulative_constraints: 132631, agent episode reward: [-47.07525493601296], time: 40.176
steps: 17424468, episodes: 418000, mean episode reward: -46.80608951649304, num_cumulative_constraints: 132862, agent episode reward: [-46.80608951649304], time: 39.612
steps: 17466393, episodes: 419000, mean episode reward: -47.113149394261775, num_cumulative_constraints: 133206, agent episode reward: [-47.113149394261775], time: 39.308
steps: 17508265, episodes: 420000, mean episode reward: -46.779545818405964, num_cumulative_constraints: 133508, agent episode reward: [-46.779545818405964], time: 41.374
steps: 17550186, episodes: 421000, mean episode reward: -46.70764827965973, num_cumulative_constraints: 133763, agent episode reward: [-46.70764827965973], time: 40.173
steps: 17592008, episodes: 422000, mean episode reward: -46.82899219133719, num_cumulative_constraints: 134061, agent episode reward: [-46.82899219133719], time: 39.582
steps: 17634296, episodes: 423000, mean episode reward: -46.83085583351514, num_cumulative_constraints: 134344, agent episode reward: [-46.83085583351514], time: 38.991
steps: 17676368, episodes: 424000, mean episode reward: -46.79638165831766, num_cumulative_constraints: 134664, agent episode reward: [-46.79638165831766], time: 40.497
steps: 17718366, episodes: 425000, mean episode reward: -46.892825154844395, num_cumulative_constraints: 134946, agent episode reward: [-46.892825154844395], time: 40.167
steps: 17760221, episodes: 426000, mean episode reward: -46.52713274329333, num_cumulative_constraints: 135174, agent episode reward: [-46.52713274329333], time: 40.695
steps: 17802198, episodes: 427000, mean episode reward: -46.56398958639974, num_cumulative_constraints: 135428, agent episode reward: [-46.56398958639974], time: 40.846
steps: 17844264, episodes: 428000, mean episode reward: -46.74184371283031, num_cumulative_constraints: 135735, agent episode reward: [-46.74184371283031], time: 39.46
steps: 17886295, episodes: 429000, mean episode reward: -46.97781317425487, num_cumulative_constraints: 136036, agent episode reward: [-46.97781317425487], time: 39.872
steps: 17928716, episodes: 430000, mean episode reward: -47.03772938783532, num_cumulative_constraints: 136287, agent episode reward: [-47.03772938783532], time: 39.734
steps: 17971058, episodes: 431000, mean episode reward: -47.21233924562851, num_cumulative_constraints: 136631, agent episode reward: [-47.21233924562851], time: 40.556
steps: 18013357, episodes: 432000, mean episode reward: -46.96914552591021, num_cumulative_constraints: 136859, agent episode reward: [-46.96914552591021], time: 39.115
steps: 18055451, episodes: 433000, mean episode reward: -46.72675339268895, num_cumulative_constraints: 137130, agent episode reward: [-46.72675339268895], time: 40.378
steps: 18097425, episodes: 434000, mean episode reward: -46.57504062861088, num_cumulative_constraints: 137362, agent episode reward: [-46.57504062861088], time: 40.166
steps: 18139416, episodes: 435000, mean episode reward: -46.70859577746286, num_cumulative_constraints: 137635, agent episode reward: [-46.70859577746286], time: 42.145
steps: 18181479, episodes: 436000, mean episode reward: -46.61805959250635, num_cumulative_constraints: 137880, agent episode reward: [-46.61805959250635], time: 39.423
steps: 18223588, episodes: 437000, mean episode reward: -47.13962643004999, num_cumulative_constraints: 138195, agent episode reward: [-47.13962643004999], time: 39.351
steps: 18265536, episodes: 438000, mean episode reward: -46.71424191864296, num_cumulative_constraints: 138457, agent episode reward: [-46.71424191864296], time: 40.354
steps: 18308006, episodes: 439000, mean episode reward: -47.02842131202343, num_cumulative_constraints: 138721, agent episode reward: [-47.02842131202343], time: 40.741
steps: 18349967, episodes: 440000, mean episode reward: -46.6533054428448, num_cumulative_constraints: 138984, agent episode reward: [-46.6533054428448], time: 39.044
steps: 18391861, episodes: 441000, mean episode reward: -46.52176856987229, num_cumulative_constraints: 139239, agent episode reward: [-46.52176856987229], time: 40.765
steps: 18433946, episodes: 442000, mean episode reward: -47.002489490460405, num_cumulative_constraints: 139486, agent episode reward: [-47.002489490460405], time: 39.802
steps: 18476170, episodes: 443000, mean episode reward: -46.873617899332515, num_cumulative_constraints: 139737, agent episode reward: [-46.873617899332515], time: 44.308
steps: 18518301, episodes: 444000, mean episode reward: -47.03177040063645, num_cumulative_constraints: 140095, agent episode reward: [-47.03177040063645], time: 49.946
steps: 18560472, episodes: 445000, mean episode reward: -46.719583759749916, num_cumulative_constraints: 140323, agent episode reward: [-46.719583759749916], time: 38.426
steps: 18602452, episodes: 446000, mean episode reward: -46.939620608235344, num_cumulative_constraints: 140639, agent episode reward: [-46.939620608235344], time: 40.567
steps: 18644525, episodes: 447000, mean episode reward: -47.020049696417104, num_cumulative_constraints: 140918, agent episode reward: [-47.020049696417104], time: 40.082
steps: 18686733, episodes: 448000, mean episode reward: -47.256417694618726, num_cumulative_constraints: 141265, agent episode reward: [-47.256417694618726], time: 40.855
steps: 18728911, episodes: 449000, mean episode reward: -46.92011781486353, num_cumulative_constraints: 141561, agent episode reward: [-46.92011781486353], time: 39.511
steps: 18770921, episodes: 450000, mean episode reward: -46.45449901710951, num_cumulative_constraints: 141791, agent episode reward: [-46.45449901710951], time: 38.694
steps: 18813139, episodes: 451000, mean episode reward: -46.85736582690135, num_cumulative_constraints: 142025, agent episode reward: [-46.85736582690135], time: 38.826
steps: 18855184, episodes: 452000, mean episode reward: -46.59891492526424, num_cumulative_constraints: 142262, agent episode reward: [-46.59891492526424], time: 40.595
steps: 18897260, episodes: 453000, mean episode reward: -46.87361889008206, num_cumulative_constraints: 142562, agent episode reward: [-46.87361889008206], time: 39.362
steps: 18939551, episodes: 454000, mean episode reward: -47.216957662184036, num_cumulative_constraints: 142863, agent episode reward: [-47.216957662184036], time: 39.596
steps: 18981931, episodes: 455000, mean episode reward: -46.993294747258666, num_cumulative_constraints: 143140, agent episode reward: [-46.993294747258666], time: 40.387
steps: 19024610, episodes: 456000, mean episode reward: -47.10846044380846, num_cumulative_constraints: 143347, agent episode reward: [-47.10846044380846], time: 39.932
steps: 19067130, episodes: 457000, mean episode reward: -46.799915090440045, num_cumulative_constraints: 143568, agent episode reward: [-46.799915090440045], time: 40.355
steps: 19109169, episodes: 458000, mean episode reward: -46.855999412089645, num_cumulative_constraints: 143829, agent episode reward: [-46.855999412089645], time: 40.121
steps: 19151634, episodes: 459000, mean episode reward: -47.215718984514254, num_cumulative_constraints: 144101, agent episode reward: [-47.215718984514254], time: 40.226
steps: 19194115, episodes: 460000, mean episode reward: -46.92295586991246, num_cumulative_constraints: 144355, agent episode reward: [-46.92295586991246], time: 40.34
steps: 19236641, episodes: 461000, mean episode reward: -47.25667652177677, num_cumulative_constraints: 144661, agent episode reward: [-47.25667652177677], time: 40.396
steps: 19278885, episodes: 462000, mean episode reward: -46.67442674741261, num_cumulative_constraints: 144917, agent episode reward: [-46.67442674741261], time: 39.689
steps: 19321362, episodes: 463000, mean episode reward: -46.90012577254213, num_cumulative_constraints: 145153, agent episode reward: [-46.90012577254213], time: 39.731
steps: 19363561, episodes: 464000, mean episode reward: -46.839136028374064, num_cumulative_constraints: 145439, agent episode reward: [-46.839136028374064], time: 39.809
steps: 19405261, episodes: 465000, mean episode reward: -46.84055968241279, num_cumulative_constraints: 145731, agent episode reward: [-46.84055968241279], time: 38.797
steps: 19446809, episodes: 466000, mean episode reward: -46.554798849312924, num_cumulative_constraints: 145962, agent episode reward: [-46.554798849312924], time: 37.852
steps: 19488545, episodes: 467000, mean episode reward: -46.63452416835522, num_cumulative_constraints: 146209, agent episode reward: [-46.63452416835522], time: 40.137
steps: 19530242, episodes: 468000, mean episode reward: -46.60265683146572, num_cumulative_constraints: 146434, agent episode reward: [-46.60265683146572], time: 39.228
steps: 19572027, episodes: 469000, mean episode reward: -47.026625614193044, num_cumulative_constraints: 146663, agent episode reward: [-47.026625614193044], time: 39.615
steps: 19613592, episodes: 470000, mean episode reward: -46.46869736368743, num_cumulative_constraints: 146891, agent episode reward: [-46.46869736368743], time: 40.2
steps: 19655596, episodes: 471000, mean episode reward: -46.5021458791573, num_cumulative_constraints: 147140, agent episode reward: [-46.5021458791573], time: 38.628
steps: 19697566, episodes: 472000, mean episode reward: -46.70922664082366, num_cumulative_constraints: 147380, agent episode reward: [-46.70922664082366], time: 40.262
steps: 19739293, episodes: 473000, mean episode reward: -46.72090506688741, num_cumulative_constraints: 147650, agent episode reward: [-46.72090506688741], time: 39.06
steps: 19781261, episodes: 474000, mean episode reward: -47.139529976617695, num_cumulative_constraints: 147882, agent episode reward: [-47.139529976617695], time: 39.71
steps: 19823196, episodes: 475000, mean episode reward: -46.79187749827565, num_cumulative_constraints: 148118, agent episode reward: [-46.79187749827565], time: 38.949
steps: 19865199, episodes: 476000, mean episode reward: -46.90318412651602, num_cumulative_constraints: 148365, agent episode reward: [-46.90318412651602], time: 38.963
steps: 19907070, episodes: 477000, mean episode reward: -47.09865120425872, num_cumulative_constraints: 148615, agent episode reward: [-47.09865120425872], time: 39.805
steps: 19948991, episodes: 478000, mean episode reward: -46.97481295309376, num_cumulative_constraints: 148900, agent episode reward: [-46.97481295309376], time: 40.112
steps: 19990939, episodes: 479000, mean episode reward: -46.87742378793527, num_cumulative_constraints: 149115, agent episode reward: [-46.87742378793527], time: 39.626
steps: 20032973, episodes: 480000, mean episode reward: -47.341192535393525, num_cumulative_constraints: 149440, agent episode reward: [-47.341192535393525], time: 40.686
steps: 20075007, episodes: 481000, mean episode reward: -47.052301332406586, num_cumulative_constraints: 149722, agent episode reward: [-47.052301332406586], time: 39.768
steps: 20117002, episodes: 482000, mean episode reward: -46.670473131744494, num_cumulative_constraints: 149985, agent episode reward: [-46.670473131744494], time: 39.275
steps: 20159042, episodes: 483000, mean episode reward: -47.076994037077014, num_cumulative_constraints: 150223, agent episode reward: [-47.076994037077014], time: 40.301
steps: 20201295, episodes: 484000, mean episode reward: -46.94333664416675, num_cumulative_constraints: 150457, agent episode reward: [-46.94333664416675], time: 39.594
steps: 20243495, episodes: 485000, mean episode reward: -46.90949964803974, num_cumulative_constraints: 150699, agent episode reward: [-46.90949964803974], time: 41.414
steps: 20285815, episodes: 486000, mean episode reward: -47.24467868039265, num_cumulative_constraints: 150994, agent episode reward: [-47.24467868039265], time: 41.464
steps: 20328260, episodes: 487000, mean episode reward: -47.53259053538699, num_cumulative_constraints: 151409, agent episode reward: [-47.53259053538699], time: 39.308
steps: 20370731, episodes: 488000, mean episode reward: -47.289079384874626, num_cumulative_constraints: 151735, agent episode reward: [-47.289079384874626], time: 39.883
steps: 20413241, episodes: 489000, mean episode reward: -47.48188705900313, num_cumulative_constraints: 152019, agent episode reward: [-47.48188705900313], time: 39.478
steps: 20455335, episodes: 490000, mean episode reward: -47.12026886527912, num_cumulative_constraints: 152268, agent episode reward: [-47.12026886527912], time: 40.653
steps: 20497518, episodes: 491000, mean episode reward: -47.324281624647355, num_cumulative_constraints: 152638, agent episode reward: [-47.324281624647355], time: 39.325
steps: 20539439, episodes: 492000, mean episode reward: -47.027003954050514, num_cumulative_constraints: 152913, agent episode reward: [-47.027003954050514], time: 39.089
steps: 20581848, episodes: 493000, mean episode reward: -47.34857188715006, num_cumulative_constraints: 153199, agent episode reward: [-47.34857188715006], time: 40.204
steps: 20624183, episodes: 494000, mean episode reward: -47.089730650327475, num_cumulative_constraints: 153459, agent episode reward: [-47.089730650327475], time: 39.127
steps: 20665969, episodes: 495000, mean episode reward: -46.99593169017774, num_cumulative_constraints: 153801, agent episode reward: [-46.99593169017774], time: 38.364
steps: 20707752, episodes: 496000, mean episode reward: -46.82566501035604, num_cumulative_constraints: 154100, agent episode reward: [-46.82566501035604], time: 40.294
steps: 20750030, episodes: 497000, mean episode reward: -47.4135513684917, num_cumulative_constraints: 154475, agent episode reward: [-47.4135513684917], time: 39.534
steps: 20792373, episodes: 498000, mean episode reward: -47.784221340739656, num_cumulative_constraints: 154796, agent episode reward: [-47.784221340739656], time: 40.514
steps: 20834543, episodes: 499000, mean episode reward: -47.01114187669547, num_cumulative_constraints: 155047, agent episode reward: [-47.01114187669547], time: 38.862
steps: 20876386, episodes: 500000, mean episode reward: -47.3249432525838, num_cumulative_constraints: 155371, agent episode reward: [-47.3249432525838], time: 40.198
steps: 20918204, episodes: 501000, mean episode reward: -47.10005660835783, num_cumulative_constraints: 155656, agent episode reward: [-47.10005660835783], time: 37.882
steps: 20959769, episodes: 502000, mean episode reward: -47.10994210382206, num_cumulative_constraints: 155988, agent episode reward: [-47.10994210382206], time: 40.586
steps: 21001501, episodes: 503000, mean episode reward: -46.84380382122091, num_cumulative_constraints: 156258, agent episode reward: [-46.84380382122091], time: 40.055
steps: 21043038, episodes: 504000, mean episode reward: -46.982703460295404, num_cumulative_constraints: 156578, agent episode reward: [-46.982703460295404], time: 38.61
steps: 21084652, episodes: 505000, mean episode reward: -47.185675780270344, num_cumulative_constraints: 156869, agent episode reward: [-47.185675780270344], time: 38.505
steps: 21125950, episodes: 506000, mean episode reward: -46.883501957962636, num_cumulative_constraints: 157204, agent episode reward: [-46.883501957962636], time: 40.408
steps: 21167627, episodes: 507000, mean episode reward: -46.988824208508234, num_cumulative_constraints: 157584, agent episode reward: [-46.988824208508234], time: 39.616
steps: 21209112, episodes: 508000, mean episode reward: -47.166824628706124, num_cumulative_constraints: 157975, agent episode reward: [-47.166824628706124], time: 38.929
steps: 21250876, episodes: 509000, mean episode reward: -47.07589359442088, num_cumulative_constraints: 158256, agent episode reward: [-47.07589359442088], time: 38.835
steps: 21292842, episodes: 510000, mean episode reward: -47.54090506859841, num_cumulative_constraints: 158615, agent episode reward: [-47.54090506859841], time: 39.258
steps: 21334477, episodes: 511000, mean episode reward: -47.13919450522745, num_cumulative_constraints: 158892, agent episode reward: [-47.13919450522745], time: 38.732
steps: 21376006, episodes: 512000, mean episode reward: -46.998997302553875, num_cumulative_constraints: 159241, agent episode reward: [-46.998997302553875], time: 39.655
steps: 21418109, episodes: 513000, mean episode reward: -47.37172042616889, num_cumulative_constraints: 159557, agent episode reward: [-47.37172042616889], time: 41.218
steps: 21459988, episodes: 514000, mean episode reward: -47.33360272539412, num_cumulative_constraints: 159855, agent episode reward: [-47.33360272539412], time: 38.796
steps: 21502066, episodes: 515000, mean episode reward: -46.896567588265015, num_cumulative_constraints: 160049, agent episode reward: [-46.896567588265015], time: 38.742
steps: 21544161, episodes: 516000, mean episode reward: -47.14314224695075, num_cumulative_constraints: 160306, agent episode reward: [-47.14314224695075], time: 39.067
steps: 21586150, episodes: 517000, mean episode reward: -47.110753109653245, num_cumulative_constraints: 160581, agent episode reward: [-47.110753109653245], time: 38.356
steps: 21628008, episodes: 518000, mean episode reward: -47.08105259647247, num_cumulative_constraints: 160848, agent episode reward: [-47.08105259647247], time: 40.567
steps: 21670074, episodes: 519000, mean episode reward: -46.85167053544024, num_cumulative_constraints: 161109, agent episode reward: [-46.85167053544024], time: 38.749
steps: 21712026, episodes: 520000, mean episode reward: -46.89445119225935, num_cumulative_constraints: 161463, agent episode reward: [-46.89445119225935], time: 39.573
steps: 21753770, episodes: 521000, mean episode reward: -46.86270164436693, num_cumulative_constraints: 161755, agent episode reward: [-46.86270164436693], time: 38.614
steps: 21795675, episodes: 522000, mean episode reward: -47.13004401590957, num_cumulative_constraints: 162104, agent episode reward: [-47.13004401590957], time: 39.123
steps: 21837447, episodes: 523000, mean episode reward: -47.009005204084424, num_cumulative_constraints: 162503, agent episode reward: [-47.009005204084424], time: 39.821
steps: 21879201, episodes: 524000, mean episode reward: -46.981433903636706, num_cumulative_constraints: 162797, agent episode reward: [-46.981433903636706], time: 38.414
steps: 21920824, episodes: 525000, mean episode reward: -46.88586751418137, num_cumulative_constraints: 163123, agent episode reward: [-46.88586751418137], time: 39.899
steps: 21962769, episodes: 526000, mean episode reward: -46.56519098423635, num_cumulative_constraints: 163338, agent episode reward: [-46.56519098423635], time: 41.061
steps: 22004368, episodes: 527000, mean episode reward: -46.65323811153877, num_cumulative_constraints: 163597, agent episode reward: [-46.65323811153877], time: 39.011
steps: 22046182, episodes: 528000, mean episode reward: -47.034129339464556, num_cumulative_constraints: 163884, agent episode reward: [-47.034129339464556], time: 39.333
steps: 22087966, episodes: 529000, mean episode reward: -47.02258173641378, num_cumulative_constraints: 164168, agent episode reward: [-47.02258173641378], time: 39.594
steps: 22130181, episodes: 530000, mean episode reward: -46.76227511161698, num_cumulative_constraints: 164369, agent episode reward: [-46.76227511161698], time: 40.019
steps: 22171961, episodes: 531000, mean episode reward: -46.75357826633297, num_cumulative_constraints: 164639, agent episode reward: [-46.75357826633297], time: 37.893
steps: 22213846, episodes: 532000, mean episode reward: -46.7767007686278, num_cumulative_constraints: 164941, agent episode reward: [-46.7767007686278], time: 39.295
steps: 22255901, episodes: 533000, mean episode reward: -46.91962385579723, num_cumulative_constraints: 165220, agent episode reward: [-46.91962385579723], time: 41.406
steps: 22297794, episodes: 534000, mean episode reward: -47.097045401127524, num_cumulative_constraints: 165488, agent episode reward: [-47.097045401127524], time: 38.759
steps: 22339441, episodes: 535000, mean episode reward: -46.68812443459826, num_cumulative_constraints: 165763, agent episode reward: [-46.68812443459826], time: 39.06
steps: 22380949, episodes: 536000, mean episode reward: -46.54237342182499, num_cumulative_constraints: 165998, agent episode reward: [-46.54237342182499], time: 38.498
steps: 22422680, episodes: 537000, mean episode reward: -46.80346376107364, num_cumulative_constraints: 166251, agent episode reward: [-46.80346376107364], time: 39.524
steps: 22464065, episodes: 538000, mean episode reward: -46.46190718161817, num_cumulative_constraints: 166485, agent episode reward: [-46.46190718161817], time: 38.479
steps: 22505605, episodes: 539000, mean episode reward: -47.06395555235224, num_cumulative_constraints: 166752, agent episode reward: [-47.06395555235224], time: 39.7
steps: 22547355, episodes: 540000, mean episode reward: -46.85781342451635, num_cumulative_constraints: 167014, agent episode reward: [-46.85781342451635], time: 39.019
steps: 22589025, episodes: 541000, mean episode reward: -46.47525349657783, num_cumulative_constraints: 167251, agent episode reward: [-46.47525349657783], time: 38.454
steps: 22630685, episodes: 542000, mean episode reward: -46.51959872575491, num_cumulative_constraints: 167511, agent episode reward: [-46.51959872575491], time: 38.886
steps: 22672101, episodes: 543000, mean episode reward: -46.54519102253366, num_cumulative_constraints: 167800, agent episode reward: [-46.54519102253366], time: 38.372
steps: 22713731, episodes: 544000, mean episode reward: -46.818887073024094, num_cumulative_constraints: 168066, agent episode reward: [-46.818887073024094], time: 38.486
steps: 22755174, episodes: 545000, mean episode reward: -46.875019458733796, num_cumulative_constraints: 168381, agent episode reward: [-46.875019458733796], time: 39.277
steps: 22796948, episodes: 546000, mean episode reward: -46.91403033713571, num_cumulative_constraints: 168623, agent episode reward: [-46.91403033713571], time: 38.398
steps: 22838387, episodes: 547000, mean episode reward: -46.50350897422383, num_cumulative_constraints: 168848, agent episode reward: [-46.50350897422383], time: 39.085
steps: 22879574, episodes: 548000, mean episode reward: -46.432947560060846, num_cumulative_constraints: 169146, agent episode reward: [-46.432947560060846], time: 38.775
steps: 22921584, episodes: 549000, mean episode reward: -46.862070485761464, num_cumulative_constraints: 169425, agent episode reward: [-46.862070485761464], time: 39.189
steps: 22963105, episodes: 550000, mean episode reward: -46.51998249977804, num_cumulative_constraints: 169655, agent episode reward: [-46.51998249977804], time: 39.347
steps: 23004628, episodes: 551000, mean episode reward: -46.85802772002404, num_cumulative_constraints: 169915, agent episode reward: [-46.85802772002404], time: 39.582
steps: 23046314, episodes: 552000, mean episode reward: -46.84998684663895, num_cumulative_constraints: 170148, agent episode reward: [-46.84998684663895], time: 41.236
steps: 23087891, episodes: 553000, mean episode reward: -46.64675626822397, num_cumulative_constraints: 170419, agent episode reward: [-46.64675626822397], time: 38.997
steps: 23129377, episodes: 554000, mean episode reward: -46.65374181167046, num_cumulative_constraints: 170675, agent episode reward: [-46.65374181167046], time: 38.692
steps: 23170804, episodes: 555000, mean episode reward: -46.498104619933876, num_cumulative_constraints: 170944, agent episode reward: [-46.498104619933876], time: 39.715
steps: 23212408, episodes: 556000, mean episode reward: -46.68659080476281, num_cumulative_constraints: 171210, agent episode reward: [-46.68659080476281], time: 38.276
steps: 23253878, episodes: 557000, mean episode reward: -46.87215611147965, num_cumulative_constraints: 171480, agent episode reward: [-46.87215611147965], time: 39.339
steps: 23295207, episodes: 558000, mean episode reward: -46.3222392987155, num_cumulative_constraints: 171704, agent episode reward: [-46.3222392987155], time: 38.912
steps: 23336336, episodes: 559000, mean episode reward: -46.536643181658, num_cumulative_constraints: 171999, agent episode reward: [-46.536643181658], time: 39.685
steps: 23377619, episodes: 560000, mean episode reward: -46.74012930999835, num_cumulative_constraints: 172305, agent episode reward: [-46.74012930999835], time: 39.508
steps: 23418952, episodes: 561000, mean episode reward: -46.170925492718155, num_cumulative_constraints: 172548, agent episode reward: [-46.170925492718155], time: 37.615
steps: 23460511, episodes: 562000, mean episode reward: -46.70167226493683, num_cumulative_constraints: 172789, agent episode reward: [-46.70167226493683], time: 38.887
steps: 23502153, episodes: 563000, mean episode reward: -46.72338083165494, num_cumulative_constraints: 173113, agent episode reward: [-46.72338083165494], time: 39.212
steps: 23544150, episodes: 564000, mean episode reward: -46.70932549294223, num_cumulative_constraints: 173390, agent episode reward: [-46.70932549294223], time: 38.75
steps: 23585861, episodes: 565000, mean episode reward: -46.84568001872962, num_cumulative_constraints: 173709, agent episode reward: [-46.84568001872962], time: 39.187
steps: 23628009, episodes: 566000, mean episode reward: -46.719739715597875, num_cumulative_constraints: 173990, agent episode reward: [-46.719739715597875], time: 40.437
steps: 23669626, episodes: 567000, mean episode reward: -46.621209720075335, num_cumulative_constraints: 174281, agent episode reward: [-46.621209720075335], time: 39.251
steps: 23712177, episodes: 568000, mean episode reward: -47.06559183145821, num_cumulative_constraints: 174543, agent episode reward: [-47.06559183145821], time: 39.291
steps: 23754260, episodes: 569000, mean episode reward: -46.861552328446834, num_cumulative_constraints: 174821, agent episode reward: [-46.861552328446834], time: 38.987
steps: 23796277, episodes: 570000, mean episode reward: -47.16231670530741, num_cumulative_constraints: 175122, agent episode reward: [-47.16231670530741], time: 38.807
steps: 23838049, episodes: 571000, mean episode reward: -46.53460032922821, num_cumulative_constraints: 175373, agent episode reward: [-46.53460032922821], time: 38.838
steps: 23879578, episodes: 572000, mean episode reward: -46.275767871415766, num_cumulative_constraints: 175601, agent episode reward: [-46.275767871415766], time: 38.544
steps: 23920957, episodes: 573000, mean episode reward: -46.74781011482154, num_cumulative_constraints: 175876, agent episode reward: [-46.74781011482154], time: 38.388
steps: 23962312, episodes: 574000, mean episode reward: -46.63939292542695, num_cumulative_constraints: 176179, agent episode reward: [-46.63939292542695], time: 38.974
steps: 24003721, episodes: 575000, mean episode reward: -46.88629672489738, num_cumulative_constraints: 176467, agent episode reward: [-46.88629672489738], time: 39.07
steps: 24045120, episodes: 576000, mean episode reward: -46.63556370861665, num_cumulative_constraints: 176773, agent episode reward: [-46.63556370861665], time: 38.373
steps: 24086406, episodes: 577000, mean episode reward: -46.60527895490836, num_cumulative_constraints: 177066, agent episode reward: [-46.60527895490836], time: 40.078
steps: 24127784, episodes: 578000, mean episode reward: -46.24975889930802, num_cumulative_constraints: 177308, agent episode reward: [-46.24975889930802], time: 37.682
steps: 24169323, episodes: 579000, mean episode reward: -46.73778356490358, num_cumulative_constraints: 177596, agent episode reward: [-46.73778356490358], time: 39.439
steps: 24210626, episodes: 580000, mean episode reward: -46.46361489802622, num_cumulative_constraints: 177850, agent episode reward: [-46.46361489802622], time: 39.326
steps: 24252107, episodes: 581000, mean episode reward: -46.451923442141286, num_cumulative_constraints: 178113, agent episode reward: [-46.451923442141286], time: 40.14
steps: 24293889, episodes: 582000, mean episode reward: -46.94197181948587, num_cumulative_constraints: 178361, agent episode reward: [-46.94197181948587], time: 38.743
steps: 24336007, episodes: 583000, mean episode reward: -47.501000377949545, num_cumulative_constraints: 178668, agent episode reward: [-47.501000377949545], time: 38.232
steps: 24377332, episodes: 584000, mean episode reward: -46.71138350537736, num_cumulative_constraints: 178966, agent episode reward: [-46.71138350537736], time: 38.671
steps: 24418638, episodes: 585000, mean episode reward: -46.67257726261285, num_cumulative_constraints: 179268, agent episode reward: [-46.67257726261285], time: 38.815
steps: 24459868, episodes: 586000, mean episode reward: -46.90059517808749, num_cumulative_constraints: 179568, agent episode reward: [-46.90059517808749], time: 37.914
steps: 24501167, episodes: 587000, mean episode reward: -46.661351707733076, num_cumulative_constraints: 179835, agent episode reward: [-46.661351707733076], time: 39.081
steps: 24542589, episodes: 588000, mean episode reward: -46.46904435975, num_cumulative_constraints: 180063, agent episode reward: [-46.46904435975], time: 38.891
steps: 24584213, episodes: 589000, mean episode reward: -46.97052102670692, num_cumulative_constraints: 180330, agent episode reward: [-46.97052102670692], time: 39.973
steps: 24625760, episodes: 590000, mean episode reward: -46.490505822227206, num_cumulative_constraints: 180534, agent episode reward: [-46.490505822227206], time: 38.662
steps: 24667422, episodes: 591000, mean episode reward: -46.84220645343712, num_cumulative_constraints: 180792, agent episode reward: [-46.84220645343712], time: 39.022
steps: 24709325, episodes: 592000, mean episode reward: -46.99028334448744, num_cumulative_constraints: 181036, agent episode reward: [-46.99028334448744], time: 39.445
steps: 24751328, episodes: 593000, mean episode reward: -46.78904214848707, num_cumulative_constraints: 181247, agent episode reward: [-46.78904214848707], time: 38.494
steps: 24793095, episodes: 594000, mean episode reward: -46.725669257005734, num_cumulative_constraints: 181480, agent episode reward: [-46.725669257005734], time: 39.578
steps: 24835021, episodes: 595000, mean episode reward: -46.85646612204126, num_cumulative_constraints: 181665, agent episode reward: [-46.85646612204126], time: 39.552
steps: 24876748, episodes: 596000, mean episode reward: -46.7665541777661, num_cumulative_constraints: 181882, agent episode reward: [-46.7665541777661], time: 40.573
steps: 24918847, episodes: 597000, mean episode reward: -47.38823367097161, num_cumulative_constraints: 182134, agent episode reward: [-47.38823367097161], time: 39.798
steps: 24960809, episodes: 598000, mean episode reward: -46.54479005310692, num_cumulative_constraints: 182351, agent episode reward: [-46.54479005310692], time: 38.556
steps: 25003003, episodes: 599000, mean episode reward: -47.04857418117888, num_cumulative_constraints: 182628, agent episode reward: [-47.04857418117888], time: 39.823
steps: 25044942, episodes: 600000, mean episode reward: -46.85025017983863, num_cumulative_constraints: 182910, agent episode reward: [-46.85025017983863], time: 39.884



smooth diff without safety_layer
steps: 41820, episodes: 1000, mean episode reward: -47.50978814103029, num_cumulative_constraints: 190, agent episode reward: [-47.50978814103029], time: 27.203
steps: 83735, episodes: 2000, mean episode reward: -47.4581858476071, num_cumulative_constraints: 395, agent episode reward: [-47.4581858476071], time: 29.781
steps: 125348, episodes: 3000, mean episode reward: -47.2098890727013, num_cumulative_constraints: 673, agent episode reward: [-47.2098890727013], time: 34.067
steps: 167057, episodes: 4000, mean episode reward: -47.39407446707949, num_cumulative_constraints: 975, agent episode reward: [-47.39407446707949], time: 34.96
steps: 208741, episodes: 5000, mean episode reward: -47.609342166135654, num_cumulative_constraints: 1223, agent episode reward: [-47.609342166135654], time: 34.515
steps: 250359, episodes: 6000, mean episode reward: -47.14690483943063, num_cumulative_constraints: 1442, agent episode reward: [-47.14690483943063], time: 34.668
steps: 291837, episodes: 7000, mean episode reward: -47.09986423864381, num_cumulative_constraints: 1652, agent episode reward: [-47.09986423864381], time: 35.576
steps: 333209, episodes: 8000, mean episode reward: -46.955753804393844, num_cumulative_constraints: 1845, agent episode reward: [-46.955753804393844], time: 35.442
steps: 374886, episodes: 9000, mean episode reward: -47.375130384060554, num_cumulative_constraints: 2110, agent episode reward: [-47.375130384060554], time: 35.047
steps: 416525, episodes: 10000, mean episode reward: -47.08480237109748, num_cumulative_constraints: 2333, agent episode reward: [-47.08480237109748], time: 35.442
steps: 458102, episodes: 11000, mean episode reward: -47.331769604579165, num_cumulative_constraints: 2596, agent episode reward: [-47.331769604579165], time: 35.703
steps: 499481, episodes: 12000, mean episode reward: -46.98679626969247, num_cumulative_constraints: 2779, agent episode reward: [-46.98679626969247], time: 34.695
steps: 541179, episodes: 13000, mean episode reward: -46.9945010376037, num_cumulative_constraints: 2956, agent episode reward: [-46.9945010376037], time: 34.751
steps: 582528, episodes: 14000, mean episode reward: -46.880216748223354, num_cumulative_constraints: 3133, agent episode reward: [-46.880216748223354], time: 36.647
steps: 624044, episodes: 15000, mean episode reward: -46.94975373356412, num_cumulative_constraints: 3303, agent episode reward: [-46.94975373356412], time: 43.122
steps: 665598, episodes: 16000, mean episode reward: -47.086864522128444, num_cumulative_constraints: 3463, agent episode reward: [-47.086864522128444], time: 35.115
steps: 706971, episodes: 17000, mean episode reward: -46.7591844443249, num_cumulative_constraints: 3657, agent episode reward: [-46.7591844443249], time: 36.955
steps: 748321, episodes: 18000, mean episode reward: -46.839759991689625, num_cumulative_constraints: 3848, agent episode reward: [-46.839759991689625], time: 33.686
steps: 789783, episodes: 19000, mean episode reward: -46.78663144748568, num_cumulative_constraints: 4022, agent episode reward: [-46.78663144748568], time: 33.551
steps: 831315, episodes: 20000, mean episode reward: -47.05517562993836, num_cumulative_constraints: 4219, agent episode reward: [-47.05517562993836], time: 34.918
steps: 872982, episodes: 21000, mean episode reward: -47.17616034506332, num_cumulative_constraints: 4446, agent episode reward: [-47.17616034506332], time: 34.53
steps: 914663, episodes: 22000, mean episode reward: -47.18741566759459, num_cumulative_constraints: 4620, agent episode reward: [-47.18741566759459], time: 33.325
steps: 955976, episodes: 23000, mean episode reward: -46.94121152354359, num_cumulative_constraints: 4846, agent episode reward: [-46.94121152354359], time: 32.701
steps: 997602, episodes: 24000, mean episode reward: -47.2668688758628, num_cumulative_constraints: 5067, agent episode reward: [-47.2668688758628], time: 33.758
steps: 1039116, episodes: 25000, mean episode reward: -47.230607077492515, num_cumulative_constraints: 5283, agent episode reward: [-47.230607077492515], time: 33.665
steps: 1080737, episodes: 26000, mean episode reward: -46.92349787972085, num_cumulative_constraints: 5467, agent episode reward: [-46.92349787972085], time: 33.826
steps: 1121983, episodes: 27000, mean episode reward: -46.80072028547725, num_cumulative_constraints: 5646, agent episode reward: [-46.80072028547725], time: 33.988
steps: 1163389, episodes: 28000, mean episode reward: -46.88083173197742, num_cumulative_constraints: 5808, agent episode reward: [-46.88083173197742], time: 35.808
steps: 1204788, episodes: 29000, mean episode reward: -46.99906368009481, num_cumulative_constraints: 6028, agent episode reward: [-46.99906368009481], time: 34.608
steps: 1246135, episodes: 30000, mean episode reward: -46.99059835545633, num_cumulative_constraints: 6222, agent episode reward: [-46.99059835545633], time: 33.734
steps: 1287485, episodes: 31000, mean episode reward: -47.131142438025684, num_cumulative_constraints: 6478, agent episode reward: [-47.131142438025684], time: 35.848
steps: 1328916, episodes: 32000, mean episode reward: -46.697801294220774, num_cumulative_constraints: 6703, agent episode reward: [-46.697801294220774], time: 35.079
steps: 1370189, episodes: 33000, mean episode reward: -47.027992491058974, num_cumulative_constraints: 6892, agent episode reward: [-47.027992491058974], time: 44.864
steps: 1411533, episodes: 34000, mean episode reward: -46.865400164297405, num_cumulative_constraints: 7091, agent episode reward: [-46.865400164297405], time: 62.77
steps: 1452708, episodes: 35000, mean episode reward: -46.79945734071664, num_cumulative_constraints: 7282, agent episode reward: [-46.79945734071664], time: 55.398
steps: 1493685, episodes: 36000, mean episode reward: -46.258335389516816, num_cumulative_constraints: 7428, agent episode reward: [-46.258335389516816], time: 55.138
steps: 1534739, episodes: 37000, mean episode reward: -46.58431176793563, num_cumulative_constraints: 7626, agent episode reward: [-46.58431176793563], time: 56.995
steps: 1575865, episodes: 38000, mean episode reward: -46.58735995842175, num_cumulative_constraints: 7801, agent episode reward: [-46.58735995842175], time: 50.779
steps: 1616981, episodes: 39000, mean episode reward: -46.90272142950072, num_cumulative_constraints: 7971, agent episode reward: [-46.90272142950072], time: 55.66



without safety layer
0.12 d_omega
steps: 44417, episodes: 1000, mean episode reward: -56.51624081739307, num_cumulative_constraints: 1646, agent episode reward: [-56.51624081739307], time: 39.548
steps: 88808, episodes: 2000, mean episode reward: -55.98498473474833, num_cumulative_constraints: 3222, agent episode reward: [-55.98498473474833], time: 42.586
steps: 133380, episodes: 3000, mean episode reward: -56.42928681979548, num_cumulative_constraints: 4973, agent episode reward: [-56.42928681979548], time: 45.192
steps: 177901, episodes: 4000, mean episode reward: -55.97539482358688, num_cumulative_constraints: 6649, agent episode reward: [-55.97539482358688], time: 43.974
steps: 222165, episodes: 5000, mean episode reward: -56.036676160327524, num_cumulative_constraints: 8318, agent episode reward: [-56.036676160327524], time: 46.011
steps: 266604, episodes: 6000, mean episode reward: -55.82003712300773, num_cumulative_constraints: 9914, agent episode reward: [-55.82003712300773], time: 45.597
steps: 310729, episodes: 7000, mean episode reward: -55.81680420052766, num_cumulative_constraints: 11624, agent episode reward: [-55.81680420052766], time: 46.203
steps: 354896, episodes: 8000, mean episode reward: -55.72053755756192, num_cumulative_constraints: 13244, agent episode reward: [-55.72053755756192], time: 47.962
steps: 399076, episodes: 9000, mean episode reward: -56.22779852129669, num_cumulative_constraints: 15015, agent episode reward: [-56.22779852129669], time: 45.309
steps: 443489, episodes: 10000, mean episode reward: -55.59207180392459, num_cumulative_constraints: 16633, agent episode reward: [-55.59207180392459], time: 44.823
steps: 487735, episodes: 11000, mean episode reward: -55.49588858572357, num_cumulative_constraints: 18166, agent episode reward: [-55.49588858572357], time: 45.309
steps: 532376, episodes: 12000, mean episode reward: -55.68270631698942, num_cumulative_constraints: 19769, agent episode reward: [-55.68270631698942], time: 44.615
steps: 576924, episodes: 13000, mean episode reward: -55.95032159510975, num_cumulative_constraints: 21423, agent episode reward: [-55.95032159510975], time: 44.886
steps: 621518, episodes: 14000, mean episode reward: -56.0607264446749, num_cumulative_constraints: 23133, agent episode reward: [-56.0607264446749], time: 45.907
steps: 665666, episodes: 15000, mean episode reward: -55.09496317585968, num_cumulative_constraints: 24608, agent episode reward: [-55.09496317585968], time: 44.506
steps: 709982, episodes: 16000, mean episode reward: -55.36752481137236, num_cumulative_constraints: 26190, agent episode reward: [-55.36752481137236], time: 44.311
steps: 753977, episodes: 17000, mean episode reward: -54.75230503127414, num_cumulative_constraints: 27754, agent episode reward: [-54.75230503127414], time: 44.656
steps: 798113, episodes: 18000, mean episode reward: -55.55244476603846, num_cumulative_constraints: 29394, agent episode reward: [-55.55244476603846], time: 48.797
steps: 843194, episodes: 19000, mean episode reward: -56.374600171261804, num_cumulative_constraints: 31104, agent episode reward: [-56.374600171261804], time: 51.616
steps: 887952, episodes: 20000, mean episode reward: -55.46673781349413, num_cumulative_constraints: 32645, agent episode reward: [-55.46673781349413], time: 49.248
steps: 931980, episodes: 21000, mean episode reward: -54.72003413724955, num_cumulative_constraints: 34095, agent episode reward: [-54.72003413724955], time: 45.628

steps: 44415, episodes: 1000, mean episode reward: -55.76687138139704, num_cumulative_constraints: 1795, agent episode reward: [-55.76687138139704], time: 39.069
steps: 89061, episodes: 2000, mean episode reward: -56.35492816601367, num_cumulative_constraints: 3615, agent episode reward: [-56.35492816601367], time: 42.05
steps: 133369, episodes: 3000, mean episode reward: -56.0207479201874, num_cumulative_constraints: 5377, agent episode reward: [-56.0207479201874], time: 43.581
steps: 177540, episodes: 4000, mean episode reward: -55.80424432121149, num_cumulative_constraints: 7097, agent episode reward: [-55.80424432121149], time: 44.206
steps: 222079, episodes: 5000, mean episode reward: -56.3266490614756, num_cumulative_constraints: 8880, agent episode reward: [-56.3266490614756], time: 44.685
steps: 267061, episodes: 6000, mean episode reward: -56.63177561116354, num_cumulative_constraints: 10665, agent episode reward: [-56.63177561116354], time: 45.712
steps: 311610, episodes: 7000, mean episode reward: -56.6939758778684, num_cumulative_constraints: 12537, agent episode reward: [-56.6939758778684], time: 46.999

same
steps: 44692, episodes: 1000, mean episode reward: -55.28200956147553, num_cumulative_constraints: 1601, agent episode reward: [-55.28200956147553], time: 34.637
steps: 89363, episodes: 2000, mean episode reward: -54.723968081082056, num_cumulative_constraints: 3015, agent episode reward: [-54.723968081082056], time: 38.339
steps: 134063, episodes: 3000, mean episode reward: -54.72013781825054, num_cumulative_constraints: 4430, agent episode reward: [-54.72013781825054], time: 41.436
steps: 178727, episodes: 4000, mean episode reward: -54.64045459809946, num_cumulative_constraints: 5832, agent episode reward: [-54.64045459809946], time: 43.184
steps: 223586, episodes: 5000, mean episode reward: -54.997387949617455, num_cumulative_constraints: 7205, agent episode reward: [-54.997387949617455], time: 44.48
steps: 268382, episodes: 6000, mean episode reward: -54.52711566935369, num_cumulative_constraints: 8520, agent episode reward: [-54.52711566935369], time: 44.562
steps: 313355, episodes: 7000, mean episode reward: -55.22646681719404, num_cumulative_constraints: 9930, agent episode reward: [-55.22646681719404], time: 43.851
steps: 357914, episodes: 8000, mean episode reward: -54.441809745594846, num_cumulative_constraints: 11236, agent episode reward: [-54.441809745594846], time: 42.433
steps: 402169, episodes: 9000, mean episode reward: -54.8064980575617, num_cumulative_constraints: 12698, agent episode reward: [-54.8064980575617], time: 43.163


steps: 21499606, episodes: 441000, mean episode reward: -52.837751472258866, num_cumulative_constraints: 1010764, agent episode reward: [-52.837751472258866], time: 48.122
steps: 21546121, episodes: 442000, mean episode reward: -52.61123901513171, num_cumulative_constraints: 1011880, agent episode reward: [-52.61123901513171], time: 47.885
steps: 21593036, episodes: 443000, mean episode reward: -52.998717201856024, num_cumulative_constraints: 1012959, agent episode reward: [-52.998717201856024], time: 47.62
steps: 21639967, episodes: 444000, mean episode reward: -53.40075733776949, num_cumulative_constraints: 1014213, agent episode reward: [-53.40075733776949], time: 47.702
steps: 21686743, episodes: 445000, mean episode reward: -53.55312777972146, num_cumulative_constraints: 1015458, agent episode reward: [-53.55312777972146], time: 48.094
steps: 21733153, episodes: 446000, mean episode reward: -53.21902477905857, num_cumulative_constraints: 1016688, agent episode reward: [-53.21902477905857], time: 49.712
steps: 21780302, episodes: 447000, mean episode reward: -53.611663627815695, num_cumulative_constraints: 1017887, agent episode reward: [-53.611663627815695], time: 47.511
steps: 21826890, episodes: 448000, mean episode reward: -53.241794328635706, num_cumulative_constraints: 1019062, agent episode reward: [-53.241794328635706], time: 48.832
steps: 21873044, episodes: 449000, mean episode reward: -52.6163936282623, num_cumulative_constraints: 1020180, agent episode reward: [-52.6163936282623], time: 47.31
steps: 21919870, episodes: 450000, mean episode reward: -53.52372347305696, num_cumulative_constraints: 1021399, agent episode reward: [-53.52372347305696], time: 47.663
steps: 21965476, episodes: 451000, mean episode reward: -52.47999012988172, num_cumulative_constraints: 1022601, agent episode reward: [-52.47999012988172], time: 46.697
steps: 22011847, episodes: 452000, mean episode reward: -52.79178069796847, num_cumulative_constraints: 1023736, agent episode reward: [-52.79178069796847], time: 47.682
steps: 22057917, episodes: 453000, mean episode reward: -52.73423474617456, num_cumulative_constraints: 1024873, agent episode reward: [-52.73423474617456], time: 47.436
steps: 22104162, episodes: 454000, mean episode reward: -53.325124259278205, num_cumulative_constraints: 1026161, agent episode reward: [-53.325124259278205], time: 48.902
steps: 22150470, episodes: 455000, mean episode reward: -53.35451819457711, num_cumulative_constraints: 1027417, agent episode reward: [-53.35451819457711], time: 47.867
steps: 22196913, episodes: 456000, mean episode reward: -53.079815183587435, num_cumulative_constraints: 1028652, agent episode reward: [-53.079815183587435], time: 47.152
steps: 22242832, episodes: 457000, mean episode reward: -53.03297752703191, num_cumulative_constraints: 1029896, agent episode reward: [-53.03297752703191], time: 47.15
steps: 22288977, episodes: 458000, mean episode reward: -53.55347327325286, num_cumulative_constraints: 1031259, agent episode reward: [-53.55347327325286], time: 47.202
steps: 22335025, episodes: 459000, mean episode reward: -53.702691884847326, num_cumulative_constraints: 1032658, agent episode reward: [-53.702691884847326], time: 46.975
steps: 22380809, episodes: 460000, mean episode reward: -52.94214079205331, num_cumulative_constraints: 1033809, agent episode reward: [-52.94214079205331], time: 47.837
steps: 22426628, episodes: 461000, mean episode reward: -53.53173074530698, num_cumulative_constraints: 1035201, agent episode reward: [-53.53173074530698], time: 48.718
steps: 22472762, episodes: 462000, mean episode reward: -53.16624407332638, num_cumulative_constraints: 1036466, agent episode reward: [-53.16624407332638], time: 47.004
steps: 22518960, episodes: 463000, mean episode reward: -53.532348543920314, num_cumulative_constraints: 1037751, agent episode reward: [-53.532348543920314], time: 48.109
steps: 22565371, episodes: 464000, mean episode reward: -53.77736364395985, num_cumulative_constraints: 1039086, agent episode reward: [-53.77736364395985], time: 46.832
steps: 22611721, episodes: 465000, mean episode reward: -53.49075206135186, num_cumulative_constraints: 1040426, agent episode reward: [-53.49075206135186], time: 47.371
steps: 22658086, episodes: 466000, mean episode reward: -53.40170239377668, num_cumulative_constraints: 1041647, agent episode reward: [-53.40170239377668], time: 46.784
steps: 22704884, episodes: 467000, mean episode reward: -53.222128318197804, num_cumulative_constraints: 1042906, agent episode reward: [-53.222128318197804], time: 47.529
steps: 22751545, episodes: 468000, mean episode reward: -52.84731119163503, num_cumulative_constraints: 1044095, agent episode reward: [-52.84731119163503], time: 47.581
steps: 22798407, episodes: 469000, mean episode reward: -53.35005321537637, num_cumulative_constraints: 1045373, agent episode reward: [-53.35005321537637], time: 49.249
steps: 22845488, episodes: 470000, mean episode reward: -53.33869151413339, num_cumulative_constraints: 1046627, agent episode reward: [-53.33869151413339], time: 47.617
steps: 22892559, episodes: 471000, mean episode reward: -53.40991275953307, num_cumulative_constraints: 1047863, agent episode reward: [-53.40991275953307], time: 48.564
steps: 22939739, episodes: 472000, mean episode reward: -53.52918942191382, num_cumulative_constraints: 1049101, agent episode reward: [-53.52918942191382], time: 47.741
steps: 22987128, episodes: 473000, mean episode reward: -54.11817804933213, num_cumulative_constraints: 1050433, agent episode reward: [-54.11817804933213], time: 48.56
steps: 23034419, episodes: 474000, mean episode reward: -53.53983272879081, num_cumulative_constraints: 1051718, agent episode reward: [-53.53983272879081], time: 48.489
steps: 23082067, episodes: 475000, mean episode reward: -54.09127318177264, num_cumulative_constraints: 1053007, agent episode reward: [-54.09127318177264], time: 49.209
steps: 23129638, episodes: 476000, mean episode reward: -54.06207661300262, num_cumulative_constraints: 1054304, agent episode reward: [-54.06207661300262], time: 50.07
steps: 23176761, episodes: 477000, mean episode reward: -53.16464393352816, num_cumulative_constraints: 1055494, agent episode reward: [-53.16464393352816], time: 48.377
steps: 23224210, episodes: 478000, mean episode reward: -54.14268824355627, num_cumulative_constraints: 1056937, agent episode reward: [-54.14268824355627], time: 48.637
steps: 23271524, episodes: 479000, mean episode reward: -53.6427290014595, num_cumulative_constraints: 1058247, agent episode reward: [-53.6427290014595], time: 49.478
steps: 23318242, episodes: 480000, mean episode reward: -52.84031054319044, num_cumulative_constraints: 1059413, agent episode reward: [-52.84031054319044], time: 47.242
steps: 23365363, episodes: 481000, mean episode reward: -53.490054444969815, num_cumulative_constraints: 1060683, agent episode reward: [-53.490054444969815], time: 49.034
steps: 23412359, episodes: 482000, mean episode reward: -53.453845029462535, num_cumulative_constraints: 1061985, agent episode reward: [-53.453845029462535], time: 48.15
steps: 23458621, episodes: 483000, mean episode reward: -53.458269085629865, num_cumulative_constraints: 1063291, agent episode reward: [-53.458269085629865], time: 49.246
steps: 23504621, episodes: 484000, mean episode reward: -52.761472021880465, num_cumulative_constraints: 1064513, agent episode reward: [-52.761472021880465], time: 46.993
steps: 23551246, episodes: 485000, mean episode reward: -52.93052325220381, num_cumulative_constraints: 1065683, agent episode reward: [-52.93052325220381], time: 47.252
steps: 23598367, episodes: 486000, mean episode reward: -53.89093456865267, num_cumulative_constraints: 1066950, agent episode reward: [-53.89093456865267], time: 48.343
steps: 23644776, episodes: 487000, mean episode reward: -53.17770006061012, num_cumulative_constraints: 1068167, agent episode reward: [-53.17770006061012], time: 47.667
steps: 23691273, episodes: 488000, mean episode reward: -53.2340825513534, num_cumulative_constraints: 1069430, agent episode reward: [-53.2340825513534], time: 47.526
steps: 23738283, episodes: 489000, mean episode reward: -54.202393497232244, num_cumulative_constraints: 1070833, agent episode reward: [-54.202393497232244], time: 47.761
steps: 23785404, episodes: 490000, mean episode reward: -54.266157561206214, num_cumulative_constraints: 1072203, agent episode reward: [-54.266157561206214], time: 49.767
steps: 23831895, episodes: 491000, mean episode reward: -53.46624189691874, num_cumulative_constraints: 1073460, agent episode reward: [-53.46624189691874], time: 48.985
steps: 23878386, episodes: 492000, mean episode reward: -52.94154540766235, num_cumulative_constraints: 1074704, agent episode reward: [-52.94154540766235], time: 47.004
steps: 23924853, episodes: 493000, mean episode reward: -52.496829176212294, num_cumulative_constraints: 1075869, agent episode reward: [-52.496829176212294], time: 47.801
steps: 23971136, episodes: 494000, mean episode reward: -52.45734316132929, num_cumulative_constraints: 1077145, agent episode reward: [-52.45734316132929], time: 48.474
steps: 24017195, episodes: 495000, mean episode reward: -52.461659951816124, num_cumulative_constraints: 1078458, agent episode reward: [-52.461659951816124], time: 47.073
steps: 24063632, episodes: 496000, mean episode reward: -53.05712461018378, num_cumulative_constraints: 1079805, agent episode reward: [-53.05712461018378], time: 49.927
steps: 24109512, episodes: 497000, mean episode reward: -52.793210375975086, num_cumulative_constraints: 1081064, agent episode reward: [-52.793210375975086], time: 48.6
steps: 24155086, episodes: 498000, mean episode reward: -52.87491317646539, num_cumulative_constraints: 1082355, agent episode reward: [-52.87491317646539], time: 46.468
steps: 24201551, episodes: 499000, mean episode reward: -53.52579236246403, num_cumulative_constraints: 1083764, agent episode reward: [-53.52579236246403], time: 47.699
steps: 24248612, episodes: 500000, mean episode reward: -54.14442346351253, num_cumulative_constraints: 1085205, agent episode reward: [-54.14442346351253], time: 48.934
steps: 24296616, episodes: 501000, mean episode reward: -54.178636664882355, num_cumulative_constraints: 1086602, agent episode reward: [-54.178636664882355], time: 49.192
steps: 24345149, episodes: 502000, mean episode reward: -56.03706695085404, num_cumulative_constraints: 1088280, agent episode reward: [-56.03706695085404], time: 49.754
steps: 24394017, episodes: 503000, mean episode reward: -58.07438021080185, num_cumulative_constraints: 1090518, agent episode reward: [-58.07438021080185], time: 51.261
steps: 24444742, episodes: 504000, mean episode reward: -60.54056956518214, num_cumulative_constraints: 1093242, agent episode reward: [-60.54056956518214], time: 51.907
steps: 24497883, episodes: 505000, mean episode reward: -61.87388620812827, num_cumulative_constraints: 1096063, agent episode reward: [-61.87388620812827], time: 54.22
steps: 24553309, episodes: 506000, mean episode reward: -63.54540292733983, num_cumulative_constraints: 1098938, agent episode reward: [-63.54540292733983], time: 55.67
steps: 24610217, episodes: 507000, mean episode reward: -64.47143281080281, num_cumulative_constraints: 1101676, agent episode reward: [-64.47143281080281], time: 56.977
steps: 24667855, episodes: 508000, mean episode reward: -68.34121827923407, num_cumulative_constraints: 1104974, agent episode reward: [-68.34121827923407], time: 57.619
steps: 24725240, episodes: 509000, mean episode reward: -69.36867637722128, num_cumulative_constraints: 1108628, agent episode reward: [-69.36867637722128], time: 57.514
steps: 24783003, episodes: 510000, mean episode reward: -72.39525610732863, num_cumulative_constraints: 1112722, agent episode reward: [-72.39525610732863], time: 58.255
steps: 24841352, episodes: 511000, mean episode reward: -72.84000455081299, num_cumulative_constraints: 1116900, agent episode reward: [-72.84000455081299], time: 57.553
steps: 24899688, episodes: 512000, mean episode reward: -76.41753564255036, num_cumulative_constraints: 1121573, agent episode reward: [-76.41753564255036], time: 58.238
steps: 24958165, episodes: 513000, mean episode reward: -84.13941107467517, num_cumulative_constraints: 1127113, agent episode reward: [-84.13941107467517], time: 58.691
steps: 25016850, episodes: 514000, mean episode reward: -80.60708663848742, num_cumulative_constraints: 1131866, agent episode reward: [-80.60708663848742], time: 58.13
steps: 25075771, episodes: 515000, mean episode reward: -79.31953613920389, num_cumulative_constraints: 1136044, agent episode reward: [-79.31953613920389], time: 58.821
steps: 25134750, episodes: 516000, mean episode reward: -73.55190559172979, num_cumulative_constraints: 1138513, agent episode reward: [-73.55190559172979], time: 58.473
steps: 25193554, episodes: 517000, mean episode reward: -70.03666032283058, num_cumulative_constraints: 1140793, agent episode reward: [-70.03666032283058], time: 59.835
steps: 25252678, episodes: 518000, mean episode reward: -69.37914512751287, num_cumulative_constraints: 1142797, agent episode reward: [-69.37914512751287], time: 59.712
steps: 25311836, episodes: 519000, mean episode reward: -69.16599412785831, num_cumulative_constraints: 1144751, agent episode reward: [-69.16599412785831], time: 58.681
steps: 25370351, episodes: 520000, mean episode reward: -69.83539584595349, num_cumulative_constraints: 1146774, agent episode reward: [-69.83539584595349], time: 57.813
steps: 25424674, episodes: 521000, mean episode reward: -65.83974473032151, num_cumulative_constraints: 1148672, agent episode reward: [-65.83974473032151], time: 54.573
steps: 25479034, episodes: 522000, mean episode reward: -63.424676696227486, num_cumulative_constraints: 1150613, agent episode reward: [-63.424676696227486], time: 55.372
steps: 25536492, episodes: 523000, mean episode reward: -67.5778506567169, num_cumulative_constraints: 1152561, agent episode reward: [-67.5778506567169], time: 57.393
steps: 25594909, episodes: 524000, mean episode reward: -71.52113507516607, num_cumulative_constraints: 1154467, agent episode reward: [-71.52113507516607], time: 59.627
steps: 25652641, episodes: 525000, mean episode reward: -67.60584306036762, num_cumulative_constraints: 1156316, agent episode reward: [-67.60584306036762], time: 57.912
steps: 25709781, episodes: 526000, mean episode reward: -65.42378867512973, num_cumulative_constraints: 1158233, agent episode reward: [-65.42378867512973], time: 56.694
steps: 25765713, episodes: 527000, mean episode reward: -63.79532963419569, num_cumulative_constraints: 1160059, agent episode reward: [-63.79532963419569], time: 55.345
steps: 25821074, episodes: 528000, mean episode reward: -63.23663794395614, num_cumulative_constraints: 1161845, agent episode reward: [-63.23663794395614], time: 54.938
steps: 25876021, episodes: 529000, mean episode reward: -62.46801766921606, num_cumulative_constraints: 1163613, agent episode reward: [-62.46801766921606], time: 55.129
steps: 25930248, episodes: 530000, mean episode reward: -60.9245147862454, num_cumulative_constraints: 1165155, agent episode reward: [-60.9245147862454], time: 54.01
steps: 25983643, episodes: 531000, mean episode reward: -59.98176384347588, num_cumulative_constraints: 1166751, agent episode reward: [-59.98176384347588], time: 53.465
steps: 26036068, episodes: 532000, mean episode reward: -57.89762672487074, num_cumulative_constraints: 1168238, agent episode reward: [-57.89762672487074], time: 53.31
steps: 26086735, episodes: 533000, mean episode reward: -57.39767942262036, num_cumulative_constraints: 1169825, agent episode reward: [-57.39767942262036], time: 50.55
steps: 26138972, episodes: 534000, mean episode reward: -58.353958718060156, num_cumulative_constraints: 1171365, agent episode reward: [-58.353958718060156], time: 53.084
steps: 26192797, episodes: 535000, mean episode reward: -59.49678120621709, num_cumulative_constraints: 1172993, agent episode reward: [-59.49678120621709], time: 54.526
steps: 26247764, episodes: 536000, mean episode reward: -59.76151842598313, num_cumulative_constraints: 1174633, agent episode reward: [-59.76151842598313], time: 55.316
steps: 26301674, episodes: 537000, mean episode reward: -58.91065880004365, num_cumulative_constraints: 1176262, agent episode reward: [-58.91065880004365], time: 54.756
steps: 26354081, episodes: 538000, mean episode reward: -57.9552002460605, num_cumulative_constraints: 1177971, agent episode reward: [-57.9552002460605], time: 53.614
steps: 26404461, episodes: 539000, mean episode reward: -58.37590858839503, num_cumulative_constraints: 1180011, agent episode reward: [-58.37590858839503], time: 50.838
steps: 26453992, episodes: 540000, mean episode reward: -58.130925171026234, num_cumulative_constraints: 1181966, agent episode reward: [-58.130925171026234], time: 50.541
steps: 26505437, episodes: 541000, mean episode reward: -58.02246200685004, num_cumulative_constraints: 1183705, agent episode reward: [-58.02246200685004], time: 51.792
steps: 26557174, episodes: 542000, mean episode reward: -58.48334673419457, num_cumulative_constraints: 1185425, agent episode reward: [-58.48334673419457], time: 52.092
steps: 26609144, episodes: 543000, mean episode reward: -59.19043784517603, num_cumulative_constraints: 1187205, agent episode reward: [-59.19043784517603], time: 51.607
steps: 26661001, episodes: 544000, mean episode reward: -59.1774435990904, num_cumulative_constraints: 1189072, agent episode reward: [-59.1774435990904], time: 52.169
steps: 26712496, episodes: 545000, mean episode reward: -58.64637050287685, num_cumulative_constraints: 1191007, agent episode reward: [-58.64637050287685], time: 55.099
steps: 26763957, episodes: 546000, mean episode reward: -58.96274775376656, num_cumulative_constraints: 1192999, agent episode reward: [-58.96274775376656], time: 51.909
steps: 26814467, episodes: 547000, mean episode reward: -58.66924395422036, num_cumulative_constraints: 1195181, agent episode reward: [-58.66924395422036], time: 51.598
steps: 26864759, episodes: 548000, mean episode reward: -57.489682605899525, num_cumulative_constraints: 1197129, agent episode reward: [-57.489682605899525], time: 50.932
steps: 26915191, episodes: 549000, mean episode reward: -57.75515239105907, num_cumulative_constraints: 1199195, agent episode reward: [-57.75515239105907], time: 51.053
steps: 26966079, episodes: 550000, mean episode reward: -57.797942850075124, num_cumulative_constraints: 1201152, agent episode reward: [-57.797942850075124], time: 51.781
steps: 27016235, episodes: 551000, mean episode reward: -57.4058624871103, num_cumulative_constraints: 1203145, agent episode reward: [-57.4058624871103], time: 51.098
steps: 27067525, episodes: 552000, mean episode reward: -57.64715367364947, num_cumulative_constraints: 1205063, agent episode reward: [-57.64715367364947], time: 52.203
steps: 27119428, episodes: 553000, mean episode reward: -57.30425524530526, num_cumulative_constraints: 1206814, agent episode reward: [-57.30425524530526], time: 52.031
steps: 27171175, episodes: 554000, mean episode reward: -57.571843253971615, num_cumulative_constraints: 1208699, agent episode reward: [-57.571843253971615], time: 52.848
steps: 27223312, episodes: 555000, mean episode reward: -57.377216596104844, num_cumulative_constraints: 1210459, agent episode reward: [-57.377216596104844], time: 53.208
steps: 27276060, episodes: 556000, mean episode reward: -57.666187387398516, num_cumulative_constraints: 1212298, agent episode reward: [-57.666187387398516], time: 53.661
steps: 27327304, episodes: 557000, mean episode reward: -56.7095117331738, num_cumulative_constraints: 1214043, agent episode reward: [-56.7095117331738], time: 52.456
steps: 27379421, episodes: 558000, mean episode reward: -56.86142922126551, num_cumulative_constraints: 1215711, agent episode reward: [-56.86142922126551], time: 52.635
steps: 27431221, episodes: 559000, mean episode reward: -56.0506361631605, num_cumulative_constraints: 1217279, agent episode reward: [-56.0506361631605], time: 54.856
steps: 27483388, episodes: 560000, mean episode reward: -56.53386498461738, num_cumulative_constraints: 1218883, agent episode reward: [-56.53386498461738], time: 53.305
steps: 27535474, episodes: 561000, mean episode reward: -57.538462515501706, num_cumulative_constraints: 1220590, agent episode reward: [-57.538462515501706], time: 52.989
steps: 27586115, episodes: 562000, mean episode reward: -56.718184592506255, num_cumulative_constraints: 1222244, agent episode reward: [-56.718184592506255], time: 52.463
steps: 27636096, episodes: 563000, mean episode reward: -56.74552668034932, num_cumulative_constraints: 1223989, agent episode reward: [-56.74552668034932], time: 50.376
steps: 27685553, episodes: 564000, mean episode reward: -56.614740352843235, num_cumulative_constraints: 1225614, agent episode reward: [-56.614740352843235], time: 50.383
steps: 27734407, episodes: 565000, mean episode reward: -56.10905323252561, num_cumulative_constraints: 1227195, agent episode reward: [-56.10905323252561], time: 51.211
steps: 27783378, episodes: 566000, mean episode reward: -56.543685422701444, num_cumulative_constraints: 1228813, agent episode reward: [-56.543685422701444], time: 51.224
steps: 27831138, episodes: 567000, mean episode reward: -55.296915664318675, num_cumulative_constraints: 1230439, agent episode reward: [-55.296915664318675], time: 48.65
steps: 27878956, episodes: 568000, mean episode reward: -54.868802636817875, num_cumulative_constraints: 1232034, agent episode reward: [-54.868802636817875], time: 49.063
steps: 27925982, episodes: 569000, mean episode reward: -54.84091326125732, num_cumulative_constraints: 1233651, agent episode reward: [-54.84091326125732], time: 48.662
steps: 27973252, episodes: 570000, mean episode reward: -55.56490792267944, num_cumulative_constraints: 1235448, agent episode reward: [-55.56490792267944], time: 48.404
steps: 28020252, episodes: 571000, mean episode reward: -54.91177732625797, num_cumulative_constraints: 1237108, agent episode reward: [-54.91177732625797], time: 47.517
steps: 28068128, episodes: 572000, mean episode reward: -55.71945633359398, num_cumulative_constraints: 1238833, agent episode reward: [-55.71945633359398], time: 49.171
steps: 28115311, episodes: 573000, mean episode reward: -55.18946573217496, num_cumulative_constraints: 1240558, agent episode reward: [-55.18946573217496], time: 49.608
steps: 28161388, episodes: 574000, mean episode reward: -54.98674755366491, num_cumulative_constraints: 1242390, agent episode reward: [-54.98674755366491], time: 48.555
steps: 28207961, episodes: 575000, mean episode reward: -54.8623543232448, num_cumulative_constraints: 1244143, agent episode reward: [-54.8623543232448], time: 47.595
steps: 28254296, episodes: 576000, mean episode reward: -54.8418788132108, num_cumulative_constraints: 1246006, agent episode reward: [-54.8418788132108], time: 47.836
steps: 28300304, episodes: 577000, mean episode reward: -54.40025470571312, num_cumulative_constraints: 1247749, agent episode reward: [-54.40025470571312], time: 47.709
steps: 28346324, episodes: 578000, mean episode reward: -54.70467581758786, num_cumulative_constraints: 1249505, agent episode reward: [-54.70467581758786], time: 47.386
steps: 28392561, episodes: 579000, mean episode reward: -54.49202220447453, num_cumulative_constraints: 1251169, agent episode reward: [-54.49202220447453], time: 47.027
steps: 28438333, episodes: 580000, mean episode reward: -54.36153061176531, num_cumulative_constraints: 1252882, agent episode reward: [-54.36153061176531], time: 49.291
steps: 28484816, episodes: 581000, mean episode reward: -54.93908607813508, num_cumulative_constraints: 1254633, agent episode reward: [-54.93908607813508], time: 48.133
steps: 28531515, episodes: 582000, mean episode reward: -54.70182282980366, num_cumulative_constraints: 1256361, agent episode reward: [-54.70182282980366], time: 47.872
steps: 28577976, episodes: 583000, mean episode reward: -54.939224371495406, num_cumulative_constraints: 1258131, agent episode reward: [-54.939224371495406], time: 47.825
steps: 28624576, episodes: 584000, mean episode reward: -54.81096267675045, num_cumulative_constraints: 1259852, agent episode reward: [-54.81096267675045], time: 48.563
steps: 28671306, episodes: 585000, mean episode reward: -55.1935778015792, num_cumulative_constraints: 1261650, agent episode reward: [-55.1935778015792], time: 48.425
steps: 28718370, episodes: 586000, mean episode reward: -54.74969052123431, num_cumulative_constraints: 1263306, agent episode reward: [-54.74969052123431], time: 48.562
steps: 28765449, episodes: 587000, mean episode reward: -54.63359924898403, num_cumulative_constraints: 1264865, agent episode reward: [-54.63359924898403], time: 48.494
steps: 28812731, episodes: 588000, mean episode reward: -54.687110816907975, num_cumulative_constraints: 1266492, agent episode reward: [-54.687110816907975], time: 49.969
steps: 28860083, episodes: 589000, mean episode reward: -55.112602474780246, num_cumulative_constraints: 1268193, agent episode reward: [-55.112602474780246], time: 48.683
steps: 28906946, episodes: 590000, mean episode reward: -54.79764315191287, num_cumulative_constraints: 1269975, agent episode reward: [-54.79764315191287], time: 47.687
steps: 28953990, episodes: 591000, mean episode reward: -54.24998152117987, num_cumulative_constraints: 1271577, agent episode reward: [-54.24998152117987], time: 48.353
steps: 29001766, episodes: 592000, mean episode reward: -55.18021837903501, num_cumulative_constraints: 1273293, agent episode reward: [-55.18021837903501], time: 49.169
steps: 29049440, episodes: 593000, mean episode reward: -55.25016105875035, num_cumulative_constraints: 1275113, agent episode reward: [-55.25016105875035], time: 50.152
steps: 29096509, episodes: 594000, mean episode reward: -55.05696843206059, num_cumulative_constraints: 1276909, agent episode reward: [-55.05696843206059], time: 48.079
steps: 29143596, episodes: 595000, mean episode reward: -54.61198854110783, num_cumulative_constraints: 1278574, agent episode reward: [-54.61198854110783], time: 49.312
steps: 29190833, episodes: 596000, mean episode reward: -54.80351195041259, num_cumulative_constraints: 1280325, agent episode reward: [-54.80351195041259], time: 48.873
steps: 29237943, episodes: 597000, mean episode reward: -55.04184908625854, num_cumulative_constraints: 1282083, agent episode reward: [-55.04184908625854], time: 49.416
steps: 29284467, episodes: 598000, mean episode reward: -54.10864992294835, num_cumulative_constraints: 1283842, agent episode reward: [-54.10864992294835], time: 47.479
steps: 29330999, episodes: 599000, mean episode reward: -54.84649549800856, num_cumulative_constraints: 1285668, agent episode reward: [-54.84649549800856], time: 47.697
steps: 29377231, episodes: 600000, mean episode reward: -54.3206533210957, num_cumulative_constraints: 1287433, agent episode reward: [-54.3206533210957], time: 47.255


cancel the d_omega difference punishment
steps: 42530, episodes: 1000, mean episode reward: -45.2012094031399, num_cumulative_constraints: 1402, num_done: 981, time: 30.522
steps: 84888, episodes: 2000, mean episode reward: -44.06503361673264, num_cumulative_constraints: 1180, num_done: 978, time: 37.079
steps: 127659, episodes: 3000, mean episode reward: -45.05577462657394, num_cumulative_constraints: 1324, num_done: 968, time: 41.693
steps: 170403, episodes: 4000, mean episode reward: -44.595457560123656, num_cumulative_constraints: 1238, num_done: 968, time: 40.388
steps: 212771, episodes: 5000, mean episode reward: -44.36123631990875, num_cumulative_constraints: 1267, num_done: 978, time: 42.345

change the corrider to 0.09
steps: 43323, episodes: 1000, mean episode reward: -41.85816672628917, num_cumulative_constraints: 369, num_done: 909, time: 36.72
steps: 86947, episodes: 2000, mean episode reward: -42.01438601692255, num_cumulative_constraints: 494, num_done: 880, time: 39.276
steps: 131785, episodes: 3000, mean episode reward: -42.82761291388536, num_cumulative_constraints: 581, num_done: 824, time: 42.381
steps: 177056, episodes: 4000, mean episode reward: -42.878187876261705, num_cumulative_constraints: 565, num_done: 802, time: 43.485
steps: 223571, episodes: 5000, mean episode reward: -43.41842294550154, num_cumulative_constraints: 631, num_done: 734, time: 44.354
steps: 269245, episodes: 6000, mean episode reward: -43.60551139652025, num_cumulative_constraints: 777, num_done: 780, time: 47.3
steps: 313661, episodes: 7000, mean episode reward: -42.477126374660585, num_cumulative_constraints: 571, num_done: 862, time: 58.544
steps: 357781, episodes: 8000, mean episode reward: -42.08837165941263, num_cumulative_constraints: 575, num_done: 910, time: 49.121
steps: 401616, episodes: 9000, mean episode reward: -42.25043875125113, num_cumulative_constraints: 601, num_done: 910, time: 42.768
steps: 444356, episodes: 10000, mean episode reward: -42.222143007718984, num_cumulative_constraints: 681, num_done: 947, time: 41.28
steps: 486249, episodes: 11000, mean episode reward: -42.22469479457194, num_cumulative_constraints: 726, num_done: 984, time: 40.795
steps: 528408, episodes: 12000, mean episode reward: -42.2681712021994, num_cumulative_constraints: 714, num_done: 974, time: 40.868
steps: 570714, episodes: 13000, mean episode reward: -41.95949131046, num_cumulative_constraints: 607, num_done: 973, time: 40.712
steps: 612750, episodes: 14000, mean episode reward: -41.743758972588424, num_cumulative_constraints: 543, num_done: 987, time: 40.374
steps: 654920, episodes: 15000, mean episode reward: -42.02083641330917, num_cumulative_constraints: 614, num_done: 979, time: 41.745
steps: 697281, episodes: 16000, mean episode reward: -41.74505015569799, num_cumulative_constraints: 543, num_done: 968, time: 40.88
steps: 739651, episodes: 17000, mean episode reward: -42.24510029115057, num_cumulative_constraints: 665, num_done: 977, time: 41.385


with safety_layer
steps: 43208, episodes: 1000, mean episode reward: -41.814879141110346, num_cumulative_constraints: 312, num_done: 951, time: 44.704
steps: 86607, episodes: 2000, mean episode reward: -42.19174816640158, num_cumulative_constraints: 435, num_done: 944, time: 58.35

without_safety stable solution
steps: 42730, episodes: 1000, mean episode reward: -41.91395115802124, num_cumulative_constraints: 460, num_done: 975, time: 38.108
steps: 86025, episodes: 2000, mean episode reward: -42.32356632377603, num_cumulative_constraints: 522, num_done: 957, time: 38.65
steps: 128622, episodes: 3000, mean episode reward: -41.72412191241678, num_cumulative_constraints: 443, num_done: 979, time: 39.29
steps: 171524, episodes: 4000, mean episode reward: -41.68623504037769, num_cumulative_constraints: 408, num_done: 964, time: 39.251
steps: 214660, episodes: 5000, mean episode reward: -41.851350089321876, num_cumulative_constraints: 419, num_done: 962, time: 38.925
steps: 257765, episodes: 6000, mean episode reward: -42.09098577243937, num_cumulative_constraints: 462, num_done: 968, time: 38.675
steps: 300773, episodes: 7000, mean episode reward: -41.74708776805342, num_cumulative_constraints: 426, num_done: 966, time: 38.967
num_done = 6771 / 7 = 967 9681/10 = 968.1
num_constraints = 3140/7 = 448  4868/10 = 442.5

0.000 num_done 13459/14 = 961.35 14 407/15 = 960.4
num_constraints : 6409/14 = 457 6937/15 = 462.46



0.15
without safe
steps: 42377, episodes: 1000, mean episode reward: -40.32551327331782, num_cumulative_constraints: 81, num_done: 981, time: 127.511
steps: 84818, episodes: 2000, mean episode reward: -40.194736520407766, num_cumulative_constraints: 91, num_done: 979, time: 128.0
steps: 127165, episodes: 3000, mean episode reward: -40.39827268575142, num_cumulative_constraints: 100, num_done: 979, time: 132.396
steps: 169555, episodes: 4000, mean episode reward: -40.16990303140176, num_cumulative_constraints: 79, num_done: 980, time: 130.692
steps: 212117, episodes: 5000, mean episode reward: -40.29842961094945, num_cumulative_constraints: 73, num_done: 974, time: 123.732



without
Starting iterations...
steps: 42648, episodes: 1000, mean episode reward: -40.422398681480644, num_cumulative_constraints: 113, num_done: 965, time: 122.573
steps: 84617, episodes: 2000, mean episode reward: -40.32207661499582, num_cumulative_constraints: 212, num_done: 985, time: 134.073
steps: 126352, episodes: 3000, mean episode reward: -40.157844624351746, num_cumulative_constraints: 204, num_done: 981, time: 130.59
steps: 168032, episodes: 4000, mean episode reward: -40.214066964259985, num_cumulative_constraints: 211, num_done: 985, time: 129.911
steps: 209723, episodes: 5000, mean episode reward: -40.45682339442977, num_cumulative_constraints: 273, num_done: 987, time: 128.869
steps: 251383, episodes: 6000, mean episode reward: -40.26628033737827, num_cumulative_constraints: 260, num_done: 984, time: 124.366
steps: 292821, episodes: 7000, mean episode reward: -40.874254858972336, num_cumulative_constraints: 428, num_done: 995, time: 125.017
steps: 334673, episodes: 8000, mean episode reward: -41.22302051939471, num_cumulative_constraints: 484, num_done: 990, time: 130.43
steps: 376175, episodes: 9000, mean episode reward: -41.03762843313557, num_cumulative_constraints: 479, num_done: 993, time: 123.584
steps: 417732, episodes: 10000, mean episode reward: -41.21491853158387, num_cumulative_constraints: 516, num_done: 989, time: 124.0
steps: 459080, episodes: 11000, mean episode reward: -40.53154827890494, num_cumulative_constraints: 339, num_done: 992, time: 129.501
steps: 500607, episodes: 12000, mean episode reward: -40.32003608295954, num_cumulative_constraints: 199, num_done: 993, time: 122.348
steps: 542117, episodes: 13000, mean episode reward: -40.74724750381417, num_cumulative_constraints: 405, num_done: 990, time: 125.126
steps: 583688, episodes: 14000, mean episode reward: -40.582065927818576, num_cumulative_constraints: 274, num_done: 995, time: 125.895
steps: 624995, episodes: 15000, mean episode reward: -40.150930185882025, num_cumulative_constraints: 202, num_done: 997, time: 121.566
steps: 666327, episodes: 16000, mean episode reward: -39.888072593965454, num_cumulative_constraints: 145, num_done: 995, time: 123.377
steps: 707832, episodes: 17000, mean episode reward: -40.27971508370314, num_cumulative_constraints: 207, num_done: 994, time: 130.322
steps: 749261, episodes: 18000, mean episode reward: -40.33224487702755, num_cumulative_constraints: 252, num_done: 995, time: 121.556
steps: 790628, episodes: 19000, mean episode reward: -40.28927058063783, num_cumulative_constraints: 249, num_done: 993, time: 130.822
steps: 832111, episodes: 20000, mean episode reward: -40.91262383588093, num_cumulative_constraints: 441, num_done: 992, time: 123.17
steps: 873610, episodes: 21000, mean episode reward: -40.21601201367024, num_cumulative_constraints: 191, num_done: 996, time: 132.988
steps: 915083, episodes: 22000, mean episode reward: -40.65385070454358, num_cumulative_constraints: 384, num_done: 994, time: 122.965
steps: 957145, episodes: 23000, mean episode reward: -40.406408250501286, num_cumulative_constraints: 209, num_done: 971, time: 125.042
steps: 999712, episodes: 24000, mean episode reward: -40.63344418440666, num_cumulative_constraints: 235, num_done: 953, time: 124.23
steps: 1042469, episodes: 25000, mean episode reward: -40.62748434786033, num_cumulative_constraints: 218, num_done: 921, time: 125.888
steps: 1085900, episodes: 26000, mean episode reward: -41.84601421695568, num_cumulative_constraints: 488, num_done: 899, time: 129.672
steps: 1130339, episodes: 27000, mean episode reward: -41.255987304492834, num_cumulative_constraints: 186, num_done: 853, time: 132.64
steps: 1174373, episodes: 28000, mean episode reward: -41.4876235837306, num_cumulative_constraints: 324, num_done: 879, time: 129.117
steps: 1216789, episodes: 29000, mean episode reward: -40.95596734929274, num_cumulative_constraints: 339, num_done: 960, time: 128.267
steps: 1259016, episodes: 30000, mean episode reward: -40.44650446849482, num_cumulative_constraints: 169, num_done: 976, time: 130.838
steps: 1301605, episodes: 31000, mean episode reward: -41.10540941938723, num_cumulative_constraints: 343, num_done: 957, time: 129.101
steps: 1344001, episodes: 32000, mean episode reward: -41.840387418838716, num_cumulative_constraints: 651, num_done: 961, time: 125.391
steps: 1385763, episodes: 33000, mean episode reward: -40.1934793493459, num_cumulative_constraints: 161, num_done: 990, time: 124.723
steps: 1427636, episodes: 34000, mean episode reward: -40.43137963469126, num_cumulative_constraints: 214, num_done: 983, time: 127.57
steps: 1469451, episodes: 35000, mean episode reward: -40.30128622066068, num_cumulative_constraints: 204, num_done: 977, time: 128.578
steps: 1511796, episodes: 36000, mean episode reward: -40.84934424673153, num_cumulative_constraints: 362, num_done: 962, time: 128.625
steps: 1554142, episodes: 37000, mean episode reward: -40.86570855378024, num_cumulative_constraints: 290, num_done: 979, time: 125.214
steps: 1596231, episodes: 38000, mean episode reward: -40.76340864558083, num_cumulative_constraints: 338, num_done: 974, time: 131.49
steps: 1638152, episodes: 39000, mean episode reward: -40.64030659747909, num_cumulative_constraints: 283, num_done: 980, time: 136.658
steps: 1679911, episodes: 40000, mean episode reward: -40.61616166422748, num_cumulative_constraints: 316, num_done: 984, time: 127.164
steps: 1721401, episodes: 41000, mean episode reward: -40.59708689621525, num_cumulative_constraints: 308, num_done: 992, time: 127.383
steps: 1762851, episodes: 42000, mean episode reward: -40.76966305860172, num_cumulative_constraints: 364, num_done: 997, time: 124.714
steps: 1804347, episodes: 43000, mean episode reward: -40.890373860285955, num_cumulative_constraints: 379, num_done: 996, time: 123.877
steps: 1846018, episodes: 44000, mean episode reward: -40.61566321616031, num_cumulative_constraints: 273, num_done: 991, time: 122.725
steps: 1888010, episodes: 45000, mean episode reward: -40.60317704315162, num_cumulative_constraints: 264, num_done: 977, time: 127.459
steps: 1930367, episodes: 46000, mean episode reward: -40.60316454508265, num_cumulative_constraints: 246, num_done: 956, time: 128.214
steps: 1972995, episodes: 47000, mean episode reward: -40.773865110676425, num_cumulative_constraints: 247, num_done: 959, time: 129.163
steps: 2016566, episodes: 48000, mean episode reward: -40.99273269169846, num_cumulative_constraints: 313, num_done: 907, time: 125.334
steps: 2060743, episodes: 49000, mean episode reward: -41.22441225242929, num_cumulative_constraints: 278, num_done: 877, time: 126.848
steps: 2107233, episodes: 50000, mean episode reward: -41.685060289879885, num_cumulative_constraints: 142, num_done: 742, time: 131.378
steps: 2156277, episodes: 51000, mean episode reward: -42.48760476481434, num_cumulative_constraints: 187, num_done: 614, time: 133.924
steps: 2206383, episodes: 52000, mean episode reward: -43.17907436746423, num_cumulative_constraints: 403, num_done: 591, time: 132.949
steps: 2255387, episodes: 53000, mean episode reward: -42.89313875558101, num_cumulative_constraints: 322, num_done: 655, time: 140.229
steps: 2303612, episodes: 54000, mean episode reward: -42.03591623860859, num_cumulative_constraints: 220, num_done: 688, time: 141.51
steps: 2351280, episodes: 55000, mean episode reward: -41.93382439400397, num_cumulative_constraints: 206, num_done: 749, time: 131.11
steps: 2396467, episodes: 56000, mean episode reward: -40.93610288683149, num_cumulative_constraints: 126, num_done: 887, time: 127.906
steps: 2439976, episodes: 57000, mean episode reward: -40.39110434343455, num_cumulative_constraints: 172, num_done: 924, time: 134.939
steps: 2483134, episodes: 58000, mean episode reward: -41.094977768599755, num_cumulative_constraints: 362, num_done: 942, time: 125.119
steps: 2525271, episodes: 59000, mean episode reward: -40.410800325392415, num_cumulative_constraints: 241, num_done: 973, time: 122.997
steps: 2567285, episodes: 60000, mean episode reward: -40.59769875860181, num_cumulative_constraints: 326, num_done: 979, time: 131.736
steps: 2609222, episodes: 61000, mean episode reward: -40.45742248411103, num_cumulative_constraints: 275, num_done: 978, time: 123.962
steps: 2650913, episodes: 62000, mean episode reward: -40.17151212294641, num_cumulative_constraints: 203, num_done: 984, time: 121.915
steps: 2692755, episodes: 63000, mean episode reward: -40.711386834203786, num_cumulative_constraints: 340, num_done: 975, time: 125.539
steps: 2734703, episodes: 64000, mean episode reward: -41.04333831778052, num_cumulative_constraints: 418, num_done: 977, time: 127.008
steps: 2776282, episodes: 65000, mean episode reward: -40.19558097709641, num_cumulative_constraints: 167, num_done: 983, time: 125.598
steps: 2817833, episodes: 66000, mean episode reward: -40.527278012208996, num_cumulative_constraints: 287, num_done: 987, time: 127.786
steps: 2859532, episodes: 67000, mean episode reward: -40.39673271879835, num_cumulative_constraints: 215, num_done: 977, time: 126.373
steps: 2901314, episodes: 68000, mean episode reward: -40.439730118531514, num_cumulative_constraints: 242, num_done: 967, time: 130.219
steps: 2943392, episodes: 69000, mean episode reward: -40.16485364530576, num_cumulative_constraints: 172, num_done: 942, time: 133.822
steps: 2986265, episodes: 70000, mean episode reward: -40.78266266341646, num_cumulative_constraints: 256, num_done: 910, time: 128.561
steps: 3029499, episodes: 71000, mean episode reward: -40.94467080696547, num_cumulative_constraints: 234, num_done: 891, time: 126.666
steps: 3072507, episodes: 72000, mean episode reward: -40.888616187407926, num_cumulative_constraints: 232, num_done: 904, time: 126.234
steps: 3115127, episodes: 73000, mean episode reward: -40.49950239043642, num_cumulative_constraints: 166, num_done: 919, time: 120.536
steps: 3157540, episodes: 74000, mean episode reward: -40.47901432341718, num_cumulative_constraints: 157, num_done: 935, time: 124.743
steps: 3200161, episodes: 75000, mean episode reward: -40.718309994921874, num_cumulative_constraints: 257, num_done: 919, time: 124.802
steps: 3242715, episodes: 76000, mean episode reward: -40.36390564872079, num_cumulative_constraints: 156, num_done: 924, time: 129.709
steps: 3285291, episodes: 77000, mean episode reward: -40.62217094210435, num_cumulative_constraints: 181, num_done: 934, time: 121.512
steps: 3327298, episodes: 78000, mean episode reward: -40.44345084923522, num_cumulative_constraints: 217, num_done: 959, time: 125.432
steps: 3369579, episodes: 79000, mean episode reward: -40.36203814860623, num_cumulative_constraints: 182, num_done: 950, time: 133.241
steps: 3411657, episodes: 80000, mean episode reward: -40.495877679907245, num_cumulative_constraints: 225, num_done: 956, time: 122.819
steps: 3453623, episodes: 81000, mean episode reward: -39.99446082743912, num_cumulative_constraints: 118, num_done: 955, time: 129.201
steps: 3495663, episodes: 82000, mean episode reward: -40.02546980613526, num_cumulative_constraints: 105, num_done: 948, time: 130.203
steps: 3538241, episodes: 83000, mean episode reward: -40.47437800889692, num_cumulative_constraints: 225, num_done: 918, time: 127.093
steps: 3580678, episodes: 84000, mean episode reward: -40.849001864005665, num_cumulative_constraints: 323, num_done: 926, time: 129.223
steps: 3623274, episodes: 85000, mean episode reward: -40.505774954663984, num_cumulative_constraints: 202, num_done: 917, time: 125.485
steps: 3666053, episodes: 86000, mean episode reward: -40.35086031511576, num_cumulative_constraints: 158, num_done: 901, time: 125.693
steps: 3708995, episodes: 87000, mean episode reward: -40.609280595272516, num_cumulative_constraints: 192, num_done: 893, time: 147.907
steps: 3751570, episodes: 88000, mean episode reward: -40.67735762354994, num_cumulative_constraints: 262, num_done: 918, time: 147.79
steps: 3793568, episodes: 89000, mean episode reward: -40.44295531174075, num_cumulative_constraints: 256, num_done: 945, time: 143.973
steps: 3835328, episodes: 90000, mean episode reward: -39.991758399764535, num_cumulative_constraints: 135, num_done: 960, time: 142.958
steps: 3877034, episodes: 91000, mean episode reward: -40.18151227014214, num_cumulative_constraints: 220, num_done: 962, time: 145.736
steps: 3919322, episodes: 92000, mean episode reward: -40.13879885216289, num_cumulative_constraints: 136, num_done: 940, time: 141.886
steps: 3961203, episodes: 93000, mean episode reward: -40.54929516583757, num_cumulative_constraints: 279, num_done: 960, time: 148.623
steps: 4002724, episodes: 94000, mean episode reward: -40.30082497994625, num_cumulative_constraints: 286, num_done: 972, time: 139.777
steps: 4044154, episodes: 95000, mean episode reward: -40.22050731214082, num_cumulative_constraints: 224, num_done: 985, time: 149.044
steps: 4085629, episodes: 96000, mean episode reward: -40.06710326124224, num_cumulative_constraints: 201, num_done: 975, time: 145.283
steps: 4127142, episodes: 97000, mean episode reward: -40.3645672538237, num_cumulative_constraints: 277, num_done: 978, time: 140.674
steps: 4168304, episodes: 98000, mean episode reward: -39.90159110899831, num_cumulative_constraints: 153, num_done: 989, time: 142.177
steps: 4209315, episodes: 99000, mean episode reward: -39.72085665657597, num_cumulative_constraints: 147, num_done: 992, time: 144.794
steps: 4250315, episodes: 100000, mean episode reward: -40.09627341313193, num_cumulative_constraints: 276, num_done: 994, time: 144.081
steps: 4291402, episodes: 101000, mean episode reward: -39.81458088533438, num_cumulative_constraints: 143, num_done: 991, time: 119.113
steps: 4332501, episodes: 102000, mean episode reward: -39.997239898018925, num_cumulative_constraints: 194, num_done: 993, time: 129.204
steps: 4373652, episodes: 103000, mean episode reward: -39.882707246664154, num_cumulative_constraints: 171, num_done: 989, time: 132.202
steps: 4414698, episodes: 104000, mean episode reward: -40.332240449778155, num_cumulative_constraints: 342, num_done: 993, time: 127.05
steps: 4455939, episodes: 105000, mean episode reward: -40.08752457347587, num_cumulative_constraints: 225, num_done: 988, time: 127.465
steps: 4496852, episodes: 106000, mean episode reward: -39.70270450049779, num_cumulative_constraints: 160, num_done: 997, time: 125.839
steps: 4537808, episodes: 107000, mean episode reward: -39.773379242466405, num_cumulative_constraints: 194, num_done: 994, time: 129.147
steps: 4578961, episodes: 108000, mean episode reward: -39.803051667470875, num_cumulative_constraints: 123, num_done: 990, time: 129.986
steps: 4620017, episodes: 109000, mean episode reward: -39.84779914012859, num_cumulative_constraints: 162, num_done: 994, time: 125.075
steps: 4661200, episodes: 110000, mean episode reward: -39.96785059425715, num_cumulative_constraints: 175, num_done: 993, time: 125.269
steps: 4702510, episodes: 111000, mean episode reward: -40.01592564140208, num_cumulative_constraints: 170, num_done: 987, time: 122.415
steps: 4743370, episodes: 112000, mean episode reward: -39.769007819741695, num_cumulative_constraints: 197, num_done: 998, time: 126.618
steps: 4784427, episodes: 113000, mean episode reward: -40.11779842540623, num_cumulative_constraints: 245, num_done: 994, time: 127.731
steps: 4825385, episodes: 114000, mean episode reward: -39.81576551358869, num_cumulative_constraints: 210, num_done: 993, time: 126.089
steps: 4866437, episodes: 115000, mean episode reward: -40.267285881557534, num_cumulative_constraints: 330, num_done: 988, time: 122.774
steps: 4907589, episodes: 116000, mean episode reward: -40.282193097462105, num_cumulative_constraints: 301, num_done: 988, time: 128.563
steps: 4948795, episodes: 117000, mean episode reward: -39.934108403841236, num_cumulative_constraints: 203, num_done: 982, time: 122.335
steps: 4989912, episodes: 118000, mean episode reward: -39.71024275000739, num_cumulative_constraints: 178, num_done: 983, time: 127.378
steps: 5031023, episodes: 119000, mean episode reward: -40.133385808360615, num_cumulative_constraints: 268, num_done: 985, time: 125.983
steps: 5072273, episodes: 120000, mean episode reward: -39.966971972980225, num_cumulative_constraints: 257, num_done: 976, time: 125.841
steps: 5113360, episodes: 121000, mean episode reward: -39.82014880908993, num_cumulative_constraints: 174, num_done: 982, time: 124.952
steps: 5154325, episodes: 122000, mean episode reward: -40.237400009431475, num_cumulative_constraints: 315, num_done: 989, time: 129.896
steps: 5195739, episodes: 123000, mean episode reward: -40.06558399470081, num_cumulative_constraints: 195, num_done: 972, time: 131.799
steps: 5237615, episodes: 124000, mean episode reward: -40.258123129683995, num_cumulative_constraints: 199, num_done: 954, time: 127.127
steps: 5279371, episodes: 125000, mean episode reward: -40.32744057665894, num_cumulative_constraints: 257, num_done: 965, time: 130.787
steps: 5320880, episodes: 126000, mean episode reward: -39.82526884514303, num_cumulative_constraints: 143, num_done: 964, time: 125.835
steps: 5362357, episodes: 127000, mean episode reward: -39.95381032118627, num_cumulative_constraints: 205, num_done: 968, time: 124.553
steps: 5403911, episodes: 128000, mean episode reward: -40.09576250472618, num_cumulative_constraints: 253, num_done: 967, time: 133.037
steps: 5445296, episodes: 129000, mean episode reward: -39.992878925766576, num_cumulative_constraints: 256, num_done: 968, time: 140.158
steps: 5486463, episodes: 130000, mean episode reward: -40.06137022638864, num_cumulative_constraints: 248, num_done: 984, time: 133.738
steps: 5527689, episodes: 131000, mean episode reward: -39.80608483324058, num_cumulative_constraints: 181, num_done: 979, time: 124.022
steps: 5569075, episodes: 132000, mean episode reward: -40.11908246633833, num_cumulative_constraints: 243, num_done: 974, time: 125.428
steps: 5610301, episodes: 133000, mean episode reward: -39.870948290580294, num_cumulative_constraints: 169, num_done: 981, time: 130.315
steps: 5651692, episodes: 134000, mean episode reward: -40.079849217230475, num_cumulative_constraints: 253, num_done: 973, time: 131.356
steps: 5693197, episodes: 135000, mean episode reward: -39.83579274043964, num_cumulative_constraints: 165, num_done: 966, time: 129.21
steps: 5734754, episodes: 136000, mean episode reward: -39.90880814964843, num_cumulative_constraints: 224, num_done: 965, time: 128.059
steps: 5776741, episodes: 137000, mean episode reward: -40.11047160245397, num_cumulative_constraints: 227, num_done: 944, time: 125.949
steps: 5818579, episodes: 138000, mean episode reward: -40.06222759190225, num_cumulative_constraints: 194, num_done: 951, time: 127.368
steps: 5860174, episodes: 139000, mean episode reward: -39.729771950897074, num_cumulative_constraints: 153, num_done: 956, time: 120.246
steps: 5901910, episodes: 140000, mean episode reward: -39.97142962085045, num_cumulative_constraints: 179, num_done: 952, time: 118.841
steps: 5943842, episodes: 141000, mean episode reward: -39.97612507896399, num_cumulative_constraints: 141, num_done: 949, time: 129.367
steps: 5985541, episodes: 142000, mean episode reward: -40.02337482051503, num_cumulative_constraints: 166, num_done: 954, time: 128.876
steps: 6027616, episodes: 143000, mean episode reward: -40.00822730325069, num_cumulative_constraints: 148, num_done: 943, time: 133.417
steps: 6069563, episodes: 144000, mean episode reward: -40.14101907982975, num_cumulative_constraints: 209, num_done: 947, time: 127.756
steps: 6111295, episodes: 145000, mean episode reward: -40.087977514946616, num_cumulative_constraints: 222, num_done: 949, time: 127.241
steps: 6152934, episodes: 146000, mean episode reward: -39.915006037359, num_cumulative_constraints: 160, num_done: 961, time: 128.226
steps: 6194506, episodes: 147000, mean episode reward: -39.91126027925914, num_cumulative_constraints: 146, num_done: 963, time: 131.887
steps: 6236341, episodes: 148000, mean episode reward: -39.94313953581016, num_cumulative_constraints: 117, num_done: 949, time: 129.243
steps: 6277730, episodes: 149000, mean episode reward: -39.96951099383553, num_cumulative_constraints: 185, num_done: 971, time: 132.088
steps: 6319182, episodes: 150000, mean episode reward: -39.5686949794904, num_cumulative_constraints: 123, num_done: 962, time: 137.757
steps: 6360676, episodes: 151000, mean episode reward: -40.126546563848095, num_cumulative_constraints: 237, num_done: 970, time: 125.71
steps: 6402491, episodes: 152000, mean episode reward: -40.10494718022911, num_cumulative_constraints: 198, num_done: 956, time: 130.415
steps: 6444084, episodes: 153000, mean episode reward: -39.68105700232984, num_cumulative_constraints: 166, num_done: 965, time: 132.287
steps: 6485772, episodes: 154000, mean episode reward: -40.45812235714998, num_cumulative_constraints: 313, num_done: 963, time: 126.916
steps: 6527036, episodes: 155000, mean episode reward: -39.778958301576864, num_cumulative_constraints: 163, num_done: 979, time: 129.515
steps: 6568208, episodes: 156000, mean episode reward: -39.78480038680882, num_cumulative_constraints: 155, num_done: 982, time: 127.572
steps: 6609365, episodes: 157000, mean episode reward: -39.83545564769138, num_cumulative_constraints: 169, num_done: 979, time: 127.049
steps: 6650485, episodes: 158000, mean episode reward: -39.709083368008145, num_cumulative_constraints: 130, num_done: 982, time: 122.104
steps: 6691548, episodes: 159000, mean episode reward: -40.137761318757434, num_cumulative_constraints: 243, num_done: 986, time: 130.457
steps: 6732635, episodes: 160000, mean episode reward: -39.62645345805121, num_cumulative_constraints: 95, num_done: 983, time: 124.938
steps: 6773785, episodes: 161000, mean episode reward: -40.07273741741523, num_cumulative_constraints: 233, num_done: 980, time: 117.097
steps: 6814840, episodes: 162000, mean episode reward: -39.94131718999681, num_cumulative_constraints: 270, num_done: 978, time: 126.869
steps: 6856194, episodes: 163000, mean episode reward: -40.09516024774574, num_cumulative_constraints: 224, num_done: 976, time: 129.403
steps: 6897690, episodes: 164000, mean episode reward: -40.17573216766755, num_cumulative_constraints: 241, num_done: 977, time: 128.358
steps: 6938842, episodes: 165000, mean episode reward: -39.78364195162989, num_cumulative_constraints: 228, num_done: 975, time: 131.733
steps: 6980014, episodes: 166000, mean episode reward: -39.89535170937355, num_cumulative_constraints: 222, num_done: 982, time: 125.104
steps: 7021346, episodes: 167000, mean episode reward: -39.92227424963569, num_cumulative_constraints: 209, num_done: 982, time: 117.5
steps: 7062607, episodes: 168000, mean episode reward: -39.97938785764512, num_cumulative_constraints: 223, num_done: 983, time: 119.46
steps: 7103500, episodes: 169000, mean episode reward: -39.97687226029901, num_cumulative_constraints: 299, num_done: 991, time: 121.029
steps: 7144989, episodes: 170000, mean episode reward: -39.883658272549944, num_cumulative_constraints: 154, num_done: 974, time: 126.713
steps: 7186345, episodes: 171000, mean episode reward: -39.81558977346536, num_cumulative_constraints: 172, num_done: 974, time: 129.188
steps: 7227357, episodes: 172000, mean episode reward: -40.10412008837266, num_cumulative_constraints: 299, num_done: 991, time: 126.056
steps: 7268139, episodes: 173000, mean episode reward: -39.61747101937655, num_cumulative_constraints: 185, num_done: 994, time: 121.892
steps: 7309088, episodes: 174000, mean episode reward: -39.56700705890785, num_cumulative_constraints: 156, num_done: 988, time: 120.58
steps: 7350582, episodes: 175000, mean episode reward: -39.99941031326181, num_cumulative_constraints: 178, num_done: 978, time: 124.392
steps: 7391623, episodes: 176000, mean episode reward: -39.608383996387325, num_cumulative_constraints: 138, num_done: 989, time: 123.912
steps: 7432879, episodes: 177000, mean episode reward: -39.50026601084344, num_cumulative_constraints: 103, num_done: 978, time: 128.039
steps: 7473914, episodes: 178000, mean episode reward: -39.63401549339475, num_cumulative_constraints: 110, num_done: 990, time: 126.406
steps: 7515094, episodes: 179000, mean episode reward: -39.72002368938295, num_cumulative_constraints: 164, num_done: 983, time: 124.338
steps: 7556505, episodes: 180000, mean episode reward: -40.03455062140683, num_cumulative_constraints: 204, num_done: 980, time: 125.24
steps: 7597634, episodes: 181000, mean episode reward: -39.61358729095101, num_cumulative_constraints: 134, num_done: 979, time: 123.697
steps: 7638774, episodes: 182000, mean episode reward: -39.53622851543996, num_cumulative_constraints: 111, num_done: 980, time: 130.573
steps: 7679988, episodes: 183000, mean episode reward: -39.79520452071397, num_cumulative_constraints: 154, num_done: 981, time: 122.217
steps: 7721189, episodes: 184000, mean episode reward: -39.61917781352573, num_cumulative_constraints: 157, num_done: 982, time: 130.598
steps: 7762287, episodes: 185000, mean episode reward: -39.66369674264986, num_cumulative_constraints: 151, num_done: 981, time: 128.468
steps: 7803432, episodes: 186000, mean episode reward: -39.47308465677321, num_cumulative_constraints: 112, num_done: 980, time: 127.502
steps: 7844863, episodes: 187000, mean episode reward: -39.76528663322041, num_cumulative_constraints: 134, num_done: 974, time: 133.286
steps: 7886050, episodes: 188000, mean episode reward: -39.60611845547628, num_cumulative_constraints: 137, num_done: 982, time: 129.606
steps: 7927447, episodes: 189000, mean episode reward: -39.8910513008749, num_cumulative_constraints: 181, num_done: 978, time: 120.391
steps: 7968695, episodes: 190000, mean episode reward: -39.68536487689651, num_cumulative_constraints: 128, num_done: 982, time: 123.976
steps: 8010216, episodes: 191000, mean episode reward: -39.839323569067126, num_cumulative_constraints: 134, num_done: 975, time: 129.959
steps: 8051600, episodes: 192000, mean episode reward: -39.67028487234306, num_cumulative_constraints: 126, num_done: 976, time: 125.885
steps: 8093192, episodes: 193000, mean episode reward: -39.82558820354034, num_cumulative_constraints: 158, num_done: 967, time: 129.097
steps: 8134559, episodes: 194000, mean episode reward: -39.816159306591075, num_cumulative_constraints: 177, num_done: 975, time: 118.053
steps: 8175925, episodes: 195000, mean episode reward: -39.81976941901429, num_cumulative_constraints: 228, num_done: 972, time: 124.236
steps: 8217906, episodes: 196000, mean episode reward: -39.95060966240756, num_cumulative_constraints: 129, num_done: 944, time: 132.932
steps: 8259555, episodes: 197000, mean episode reward: -39.6928571976503, num_cumulative_constraints: 161, num_done: 954, time: 122.073
steps: 8301517, episodes: 198000, mean episode reward: -39.85722724547158, num_cumulative_constraints: 212, num_done: 947, time: 122.398
steps: 8342892, episodes: 199000, mean episode reward: -39.86937063240125, num_cumulative_constraints: 234, num_done: 963, time: 122.3
steps: 8384355, episodes: 200000, mean episode reward: -39.70039228429, num_cumulative_constraints: 165, num_done: 962, time: 124.46
steps: 8425863, episodes: 201000, mean episode reward: -39.77622687155019, num_cumulative_constraints: 136, num_done: 973, time: 126.263
steps: 8467159, episodes: 202000, mean episode reward: -39.750875253463875, num_cumulative_constraints: 164, num_done: 971, time: 126.309
steps: 8508027, episodes: 203000, mean episode reward: -39.74529266860835, num_cumulative_constraints: 199, num_done: 990, time: 120.798
steps: 8548718, episodes: 204000, mean episode reward: -39.546747829710206, num_cumulative_constraints: 167, num_done: 996, time: 124.862
steps: 8589405, episodes: 205000, mean episode reward: -39.606215446274916, num_cumulative_constraints: 173, num_done: 998, time: 125.634
steps: 8630177, episodes: 206000, mean episode reward: -39.67404602861432, num_cumulative_constraints: 173, num_done: 994, time: 121.778
steps: 8670925, episodes: 207000, mean episode reward: -39.60577661370621, num_cumulative_constraints: 149, num_done: 996, time: 127.797
steps: 8711773, episodes: 208000, mean episode reward: -39.63507330416988, num_cumulative_constraints: 181, num_done: 995, time: 126.786
steps: 8752417, episodes: 209000, mean episode reward: -39.550882505899686, num_cumulative_constraints: 204, num_done: 995, time: 119.921
steps: 8793159, episodes: 210000, mean episode reward: -39.559055322655624, num_cumulative_constraints: 149, num_done: 997, time: 131.748
steps: 8833917, episodes: 211000, mean episode reward: -39.66425137145713, num_cumulative_constraints: 189, num_done: 995, time: 123.307
steps: 8874635, episodes: 212000, mean episode reward: -39.790089062967716, num_cumulative_constraints: 218, num_done: 998, time: 121.136
steps: 8915316, episodes: 213000, mean episode reward: -39.50194149820009, num_cumulative_constraints: 150, num_done: 994, time: 131.839
steps: 8956023, episodes: 214000, mean episode reward: -39.43555710716549, num_cumulative_constraints: 116, num_done: 997, time: 127.493
steps: 8996860, episodes: 215000, mean episode reward: -39.64956441501419, num_cumulative_constraints: 121, num_done: 996, time: 128.653
steps: 9037595, episodes: 216000, mean episode reward: -39.473977544729564, num_cumulative_constraints: 157, num_done: 993, time: 124.861
steps: 9078430, episodes: 217000, mean episode reward: -39.483154370987194, num_cumulative_constraints: 112, num_done: 997, time: 122.487
steps: 9119360, episodes: 218000, mean episode reward: -39.71866008043862, num_cumulative_constraints: 211, num_done: 991, time: 122.009
steps: 9160107, episodes: 219000, mean episode reward: -39.54655378165976, num_cumulative_constraints: 146, num_done: 994, time: 124.958
steps: 9200971, episodes: 220000, mean episode reward: -39.6970070326085, num_cumulative_constraints: 148, num_done: 992, time: 129.477
steps: 9241737, episodes: 221000, mean episode reward: -39.35086916089868, num_cumulative_constraints: 105, num_done: 993, time: 122.807
steps: 9283050, episodes: 222000, mean episode reward: -39.80606637794721, num_cumulative_constraints: 173, num_done: 975, time: 125.904
steps: 9324060, episodes: 223000, mean episode reward: -39.41848570782221, num_cumulative_constraints: 100, num_done: 982, time: 119.164
steps: 9365295, episodes: 224000, mean episode reward: -39.706978128855944, num_cumulative_constraints: 111, num_done: 983, time: 128.576
steps: 9406357, episodes: 225000, mean episode reward: -39.65515388390114, num_cumulative_constraints: 162, num_done: 984, time: 131.369
steps: 9447436, episodes: 226000, mean episode reward: -39.577041534002745, num_cumulative_constraints: 118, num_done: 982, time: 128.335
steps: 9488540, episodes: 227000, mean episode reward: -39.71163061352061, num_cumulative_constraints: 164, num_done: 983, time: 127.187
steps: 9529949, episodes: 228000, mean episode reward: -39.692382844720136, num_cumulative_constraints: 132, num_done: 962, time: 127.784
steps: 9571632, episodes: 229000, mean episode reward: -39.83267987298816, num_cumulative_constraints: 151, num_done: 951, time: 125.915
steps: 9613268, episodes: 230000, mean episode reward: -39.6560070545181, num_cumulative_constraints: 102, num_done: 958, time: 126.598
steps: 9654934, episodes: 231000, mean episode reward: -39.7437996652529, num_cumulative_constraints: 132, num_done: 960, time: 125.414
steps: 9696803, episodes: 232000, mean episode reward: -39.90100312056183, num_cumulative_constraints: 202, num_done: 947, time: 131.088
steps: 9738301, episodes: 233000, mean episode reward: -39.81430862333236, num_cumulative_constraints: 213, num_done: 963, time: 125.696
steps: 9779410, episodes: 234000, mean episode reward: -39.43459350971169, num_cumulative_constraints: 119, num_done: 977, time: 125.015
steps: 9820243, episodes: 235000, mean episode reward: -39.56418779386211, num_cumulative_constraints: 165, num_done: 992, time: 121.932
steps: 9861427, episodes: 236000, mean episode reward: -39.7775469562571, num_cumulative_constraints: 210, num_done: 986, time: 126.245
steps: 9902319, episodes: 237000, mean episode reward: -39.68021389002971, num_cumulative_constraints: 166, num_done: 992, time: 131.317
steps: 9943175, episodes: 238000, mean episode reward: -39.79582413950571, num_cumulative_constraints: 231, num_done: 990, time: 126.932
steps: 9984091, episodes: 239000, mean episode reward: -39.710954733414404, num_cumulative_constraints: 209, num_done: 995, time: 118.2
steps: 10025089, episodes: 240000, mean episode reward: -39.712262344765115, num_cumulative_constraints: 146, num_done: 991, time: 120.942
steps: 10065891, episodes: 241000, mean episode reward: -39.539538888580516, num_cumulative_constraints: 174, num_done: 991, time: 121.822
steps: 10106685, episodes: 242000, mean episode reward: -39.41314332738993, num_cumulative_constraints: 112, num_done: 994, time: 125.141
steps: 10147554, episodes: 243000, mean episode reward: -39.8122669373543, num_cumulative_constraints: 208, num_done: 993, time: 125.516
steps: 10188462, episodes: 244000, mean episode reward: -39.56760999134579, num_cumulative_constraints: 125, num_done: 992, time: 123.334
steps: 10229243, episodes: 245000, mean episode reward: -39.62409711350307, num_cumulative_constraints: 164, num_done: 992, time: 124.212
steps: 10270169, episodes: 246000, mean episode reward: -39.429875811008486, num_cumulative_constraints: 111, num_done: 985, time: 126.053
steps: 10311046, episodes: 247000, mean episode reward: -39.49913207757954, num_cumulative_constraints: 113, num_done: 992, time: 125.429
steps: 10351805, episodes: 248000, mean episode reward: -39.723218268517726, num_cumulative_constraints: 176, num_done: 996, time: 129.402
steps: 10392526, episodes: 249000, mean episode reward: -39.51319664911387, num_cumulative_constraints: 127, num_done: 997, time: 128.556
steps: 10433292, episodes: 250000, mean episode reward: -39.73438666341455, num_cumulative_constraints: 198, num_done: 991, time: 133.596
steps: 10474130, episodes: 251000, mean episode reward: -39.56379472129712, num_cumulative_constraints: 168, num_done: 988, time: 126.507
steps: 10515197, episodes: 252000, mean episode reward: -39.5934560258715, num_cumulative_constraints: 112, num_done: 979, time: 133.853
steps: 10556312, episodes: 253000, mean episode reward: -39.789852946576055, num_cumulative_constraints: 135, num_done: 982, time: 125.031
steps: 10597160, episodes: 254000, mean episode reward: -40.00896409004995, num_cumulative_constraints: 211, num_done: 996, time: 127.65
steps: 10638003, episodes: 255000, mean episode reward: -39.81389742220284, num_cumulative_constraints: 185, num_done: 994, time: 126.634
steps: 10678751, episodes: 256000, mean episode reward: -39.48382329106516, num_cumulative_constraints: 118, num_done: 995, time: 119.208
steps: 10719787, episodes: 257000, mean episode reward: -39.87330547389819, num_cumulative_constraints: 192, num_done: 982, time: 133.43
steps: 10761947, episodes: 258000, mean episode reward: -40.279959636043436, num_cumulative_constraints: 177, num_done: 929, time: 129.295
steps: 10804975, episodes: 259000, mean episode reward: -40.69570712035847, num_cumulative_constraints: 203, num_done: 881, time: 125.545
steps: 10847599, episodes: 260000, mean episode reward: -40.789209861538815, num_cumulative_constraints: 244, num_done: 909, time: 127.582
steps: 10888678, episodes: 261000, mean episode reward: -39.94891981185129, num_cumulative_constraints: 222, num_done: 990, time: 129.561
steps: 10929585, episodes: 262000, mean episode reward: -39.707129211358044, num_cumulative_constraints: 165, num_done: 997, time: 130.673
steps: 10970858, episodes: 263000, mean episode reward: -39.713774862262696, num_cumulative_constraints: 93, num_done: 982, time: 124.311
steps: 11012434, episodes: 264000, mean episode reward: -40.02027932859853, num_cumulative_constraints: 166, num_done: 965, time: 124.273
steps: 11053857, episodes: 265000, mean episode reward: -39.95559350278555, num_cumulative_constraints: 208, num_done: 963, time: 128.536
steps: 11095303, episodes: 266000, mean episode reward: -40.72084500722299, num_cumulative_constraints: 427, num_done: 966, time: 128.657
steps: 11137370, episodes: 267000, mean episode reward: -40.23868692194595, num_cumulative_constraints: 236, num_done: 939, time: 129.756
steps: 11179857, episodes: 268000, mean episode reward: -40.45816245045896, num_cumulative_constraints: 219, num_done: 918, time: 122.393
steps: 11222042, episodes: 269000, mean episode reward: -40.17669001721745, num_cumulative_constraints: 131, num_done: 934, time: 130.103
steps: 11264012, episodes: 270000, mean episode reward: -40.19099067745469, num_cumulative_constraints: 207, num_done: 939, time: 127.022
steps: 11306087, episodes: 271000, mean episode reward: -40.01774833559628, num_cumulative_constraints: 105, num_done: 938, time: 122.139
steps: 11347474, episodes: 272000, mean episode reward: -39.808119379122886, num_cumulative_constraints: 148, num_done: 970, time: 129.379
steps: 11388533, episodes: 273000, mean episode reward: -39.447270329965, num_cumulative_constraints: 90, num_done: 983, time: 125.121
steps: 11429828, episodes: 274000, mean episode reward: -39.56235749336532, num_cumulative_constraints: 109, num_done: 970, time: 124.928
steps: 11471083, episodes: 275000, mean episode reward: -39.63559816109108, num_cumulative_constraints: 126, num_done: 977, time: 122.145
steps: 11512334, episodes: 276000, mean episode reward: -39.81310768019412, num_cumulative_constraints: 134, num_done: 981, time: 128.928
steps: 11553360, episodes: 277000, mean episode reward: -39.63722432402034, num_cumulative_constraints: 84, num_done: 991, time: 126.63
steps: 11594258, episodes: 278000, mean episode reward: -39.77204400259016, num_cumulative_constraints: 151, num_done: 997, time: 123.896
steps: 11635289, episodes: 279000, mean episode reward: -39.834600305529605, num_cumulative_constraints: 131, num_done: 994, time: 124.062
steps: 11676229, episodes: 280000, mean episode reward: -39.62426033885581, num_cumulative_constraints: 135, num_done: 992, time: 132.562
steps: 11717057, episodes: 281000, mean episode reward: -39.474767918556566, num_cumulative_constraints: 110, num_done: 997, time: 127.95
steps: 11757865, episodes: 282000, mean episode reward: -39.42841064123902, num_cumulative_constraints: 93, num_done: 997, time: 122.953
steps: 11798601, episodes: 283000, mean episode reward: -39.42392383601724, num_cumulative_constraints: 106, num_done: 998, time: 128.408
steps: 11839536, episodes: 284000, mean episode reward: -39.585666820638025, num_cumulative_constraints: 77, num_done: 997, time: 123.251
steps: 11880454, episodes: 285000, mean episode reward: -39.585981114775, num_cumulative_constraints: 126, num_done: 998, time: 125.469
steps: 11921398, episodes: 286000, mean episode reward: -39.546909851708364, num_cumulative_constraints: 105, num_done: 996, time: 129.246
steps: 11962402, episodes: 287000, mean episode reward: -39.552571222885426, num_cumulative_constraints: 119, num_done: 991, time: 127.125
steps: 12003358, episodes: 288000, mean episode reward: -39.565510332448845, num_cumulative_constraints: 100, num_done: 995, time: 127.688
steps: 12044332, episodes: 289000, mean episode reward: -39.678944158924715, num_cumulative_constraints: 147, num_done: 992, time: 127.662
steps: 12085149, episodes: 290000, mean episode reward: -39.3765500907216, num_cumulative_constraints: 98, num_done: 995, time: 129.833
steps: 12125920, episodes: 291000, mean episode reward: -39.379757502743495, num_cumulative_constraints: 104, num_done: 998, time: 123.266
steps: 12166751, episodes: 292000, mean episode reward: -39.54898819382144, num_cumulative_constraints: 166, num_done: 997, time: 125.251
steps: 12207965, episodes: 293000, mean episode reward: -40.050146654607964, num_cumulative_constraints: 225, num_done: 989, time: 128.439
steps: 12249044, episodes: 294000, mean episode reward: -39.68664724833727, num_cumulative_constraints: 167, num_done: 990, time: 123.084
steps: 12290290, episodes: 295000, mean episode reward: -39.782629147098035, num_cumulative_constraints: 170, num_done: 986, time: 127.259
steps: 12331399, episodes: 296000, mean episode reward: -39.698817513497936, num_cumulative_constraints: 173, num_done: 987, time: 123.561
steps: 12372588, episodes: 297000, mean episode reward: -39.689564054179115, num_cumulative_constraints: 134, num_done: 987, time: 127.766
steps: 12413516, episodes: 298000, mean episode reward: -39.64339217136148, num_cumulative_constraints: 171, num_done: 995, time: 130.845
steps: 12454654, episodes: 299000, mean episode reward: -39.51558057534064, num_cumulative_constraints: 132, num_done: 986, time: 132.696
steps: 12496012, episodes: 300000, mean episode reward: -39.6926458075235, num_cumulative_constraints: 168, num_done: 977, time: 126.87
steps: 12537205, episodes: 301000, mean episode reward: -39.61218723157855, num_cumulative_constraints: 143, num_done: 980, time: 128.529
steps: 12578506, episodes: 302000, mean episode reward: -39.87110168065921, num_cumulative_constraints: 212, num_done: 973, time: 126.689
steps: 12619770, episodes: 303000, mean episode reward: -39.35833685264829, num_cumulative_constraints: 78, num_done: 977, time: 123.318
steps: 12661219, episodes: 304000, mean episode reward: -39.62579489174333, num_cumulative_constraints: 137, num_done: 970, time: 128.726
steps: 12702724, episodes: 305000, mean episode reward: -39.704002671296024, num_cumulative_constraints: 177, num_done: 972, time: 129.063
steps: 12744235, episodes: 306000, mean episode reward: -39.57390672462422, num_cumulative_constraints: 122, num_done: 977, time: 125.295
steps: 12785410, episodes: 307000, mean episode reward: -39.40237258711965, num_cumulative_constraints: 105, num_done: 986, time: 127.12
steps: 12827009, episodes: 308000, mean episode reward: -39.840840593921776, num_cumulative_constraints: 195, num_done: 986, time: 124.126
steps: 12868856, episodes: 309000, mean episode reward: -39.59830926850308, num_cumulative_constraints: 115, num_done: 982, time: 128.284
steps: 12910791, episodes: 310000, mean episode reward: -39.73326059001094, num_cumulative_constraints: 127, num_done: 974, time: 132.341
steps: 12952754, episodes: 311000, mean episode reward: -39.837966397828, num_cumulative_constraints: 188, num_done: 971, time: 131.75
steps: 12994690, episodes: 312000, mean episode reward: -39.75166497977004, num_cumulative_constraints: 197, num_done: 981, time: 123.131
steps: 13036812, episodes: 313000, mean episode reward: -39.824408436957036, num_cumulative_constraints: 162, num_done: 972, time: 124.312
steps: 13079456, episodes: 314000, mean episode reward: -39.79092653022706, num_cumulative_constraints: 129, num_done: 955, time: 125.186
steps: 13122513, episodes: 315000, mean episode reward: -39.98787329787137, num_cumulative_constraints: 177, num_done: 950, time: 124.387
steps: 13165399, episodes: 316000, mean episode reward: -39.677827868158126, num_cumulative_constraints: 102, num_done: 957, time: 127.686
steps: 13208779, episodes: 317000, mean episode reward: -39.75669376735093, num_cumulative_constraints: 108, num_done: 938, time: 125.444
steps: 13252212, episodes: 318000, mean episode reward: -39.99454002642767, num_cumulative_constraints: 147, num_done: 950, time: 122.358
steps: 13295496, episodes: 319000, mean episode reward: -39.95379651398977, num_cumulative_constraints: 140, num_done: 955, time: 131.033
steps: 13338706, episodes: 320000, mean episode reward: -40.0727364619138, num_cumulative_constraints: 198, num_done: 952, time: 127.079
steps: 13382275, episodes: 321000, mean episode reward: -40.05852553142498, num_cumulative_constraints: 181, num_done: 942, time: 128.48
steps: 13425923, episodes: 322000, mean episode reward: -40.09491227082186, num_cumulative_constraints: 155, num_done: 926, time: 130.674
steps: 13468992, episodes: 323000, mean episode reward: -39.94567054723787, num_cumulative_constraints: 108, num_done: 950, time: 127.258
steps: 13511761, episodes: 324000, mean episode reward: -40.17017876437548, num_cumulative_constraints: 224, num_done: 945, time: 124.153
steps: 13554203, episodes: 325000, mean episode reward: -40.00082765882761, num_cumulative_constraints: 178, num_done: 973, time: 129.718
steps: 13596904, episodes: 326000, mean episode reward: -40.08847125070441, num_cumulative_constraints: 244, num_done: 959, time: 131.219
steps: 13639020, episodes: 327000, mean episode reward: -39.80725705739839, num_cumulative_constraints: 149, num_done: 973, time: 130.218
steps: 13680879, episodes: 328000, mean episode reward: -40.01224535478873, num_cumulative_constraints: 242, num_done: 973, time: 126.081
steps: 13722751, episodes: 329000, mean episode reward: -39.757974910467425, num_cumulative_constraints: 163, num_done: 975, time: 127.83
steps: 13764508, episodes: 330000, mean episode reward: -39.47117248769864, num_cumulative_constraints: 153, num_done: 975, time: 124.438
steps: 13806364, episodes: 331000, mean episode reward: -39.72877152740985, num_cumulative_constraints: 169, num_done: 972, time: 127.833
steps: 13848314, episodes: 332000, mean episode reward: -39.94293166843979, num_cumulative_constraints: 217, num_done: 968, time: 123.189
steps: 13890322, episodes: 333000, mean episode reward: -39.95307332219581, num_cumulative_constraints: 241, num_done: 967, time: 131.695
steps: 13932150, episodes: 334000, mean episode reward: -39.8696570041552, num_cumulative_constraints: 182, num_done: 974, time: 127.707
steps: 13974339, episodes: 335000, mean episode reward: -39.88406675368296, num_cumulative_constraints: 180, num_done: 958, time: 122.931
steps: 14016252, episodes: 336000, mean episode reward: -39.89561178298865, num_cumulative_constraints: 193, num_done: 965, time: 123.916
steps: 14058289, episodes: 337000, mean episode reward: -39.94630117124289, num_cumulative_constraints: 205, num_done: 964, time: 130.907
steps: 14100629, episodes: 338000, mean episode reward: -40.19708198457927, num_cumulative_constraints: 187, num_done: 967, time: 128.44
steps: 14142487, episodes: 339000, mean episode reward: -39.98944523638002, num_cumulative_constraints: 166, num_done: 971, time: 129.01
steps: 14184393, episodes: 340000, mean episode reward: -39.99561847397391, num_cumulative_constraints: 187, num_done: 970, time: 122.332
steps: 14226161, episodes: 341000, mean episode reward: -39.71460108288959, num_cumulative_constraints: 133, num_done: 972, time: 129.87
steps: 14267688, episodes: 342000, mean episode reward: -39.57809889841411, num_cumulative_constraints: 125, num_done: 979, time: 124.419
steps: 14309447, episodes: 343000, mean episode reward: -39.70681652335559, num_cumulative_constraints: 112, num_done: 976, time: 126.858
steps: 14351461, episodes: 344000, mean episode reward: -39.79677771028049, num_cumulative_constraints: 110, num_done: 965, time: 123.536
steps: 14393362, episodes: 345000, mean episode reward: -39.69425155526997, num_cumulative_constraints: 135, num_done: 972, time: 125.517
steps: 14435041, episodes: 346000, mean episode reward: -39.7463673360017, num_cumulative_constraints: 132, num_done: 976, time: 132.859
steps: 14476733, episodes: 347000, mean episode reward: -40.011893103408894, num_cumulative_constraints: 200, num_done: 984, time: 127.953
steps: 14518305, episodes: 348000, mean episode reward: -40.00230023642488, num_cumulative_constraints: 231, num_done: 979, time: 128.598
steps: 14559898, episodes: 349000, mean episode reward: -39.84150671902532, num_cumulative_constraints: 142, num_done: 977, time: 133.37
steps: 14601363, episodes: 350000, mean episode reward: -40.28600491515264, num_cumulative_constraints: 323, num_done: 981, time: 127.054
steps: 14642990, episodes: 351000, mean episode reward: -40.04024168409809, num_cumulative_constraints: 231, num_done: 974, time: 127.642
steps: 14684468, episodes: 352000, mean episode reward: -39.78497088169762, num_cumulative_constraints: 189, num_done: 982, time: 130.598
steps: 14726040, episodes: 353000, mean episode reward: -40.1131589549, num_cumulative_constraints: 220, num_done: 982, time: 127.143
steps: 14767752, episodes: 354000, mean episode reward: -39.82854125085146, num_cumulative_constraints: 136, num_done: 977, time: 120.115
steps: 14809262, episodes: 355000, mean episode reward: -39.785500630105304, num_cumulative_constraints: 139, num_done: 982, time: 128.073
steps: 14850483, episodes: 356000, mean episode reward: -39.72171056554055, num_cumulative_constraints: 167, num_done: 991, time: 127.752
steps: 14891962, episodes: 357000, mean episode reward: -40.31083045085854, num_cumulative_constraints: 294, num_done: 987, time: 118.021
steps: 14933421, episodes: 358000, mean episode reward: -39.87131628715305, num_cumulative_constraints: 155, num_done: 988, time: 128.221
steps: 14974725, episodes: 359000, mean episode reward: -40.04455502511509, num_cumulative_constraints: 235, num_done: 993, time: 125.83
steps: 15016231, episodes: 360000, mean episode reward: -40.014781170595484, num_cumulative_constraints: 175, num_done: 992, time: 132.682
steps: 15057601, episodes: 361000, mean episode reward: -40.067578808144695, num_cumulative_constraints: 236, num_done: 993, time: 127.128
steps: 15098942, episodes: 362000, mean episode reward: -39.99645039517765, num_cumulative_constraints: 198, num_done: 997, time: 122.955
steps: 15140206, episodes: 363000, mean episode reward: -39.944661387625224, num_cumulative_constraints: 189, num_done: 993, time: 134.294
steps: 15181338, episodes: 364000, mean episode reward: -39.92104746307171, num_cumulative_constraints: 232, num_done: 993, time: 126.339
steps: 15222567, episodes: 365000, mean episode reward: -40.016825135020625, num_cumulative_constraints: 194, num_done: 998, time: 123.577
steps: 15263707, episodes: 366000, mean episode reward: -39.665016207287245, num_cumulative_constraints: 145, num_done: 996, time: 122.528
steps: 15304901, episodes: 367000, mean episode reward: -39.79835323347526, num_cumulative_constraints: 151, num_done: 996, time: 119.933
steps: 15346146, episodes: 368000, mean episode reward: -39.87861470725424, num_cumulative_constraints: 172, num_done: 994, time: 129.576
steps: 15387242, episodes: 369000, mean episode reward: -39.730981192336856, num_cumulative_constraints: 123, num_done: 999, time: 131.258
steps: 15428416, episodes: 370000, mean episode reward: -39.66969308605034, num_cumulative_constraints: 148, num_done: 993, time: 132.162
steps: 15469631, episodes: 371000, mean episode reward: -39.811007682625295, num_cumulative_constraints: 135, num_done: 993, time: 119.509
steps: 15510817, episodes: 372000, mean episode reward: -39.89703517839934, num_cumulative_constraints: 168, num_done: 988, time: 128.253
steps: 15552237, episodes: 373000, mean episode reward: -40.021615464373845, num_cumulative_constraints: 195, num_done: 971, time: 122.148
steps: 15593790, episodes: 374000, mean episode reward: -39.83190626332571, num_cumulative_constraints: 121, num_done: 963, time: 127.952
steps: 15634825, episodes: 375000, mean episode reward: -39.880123449193896, num_cumulative_constraints: 231, num_done: 988, time: 130.774
steps: 15675933, episodes: 376000, mean episode reward: -39.606256188340005, num_cumulative_constraints: 139, num_done: 980, time: 128.628
steps: 15717098, episodes: 377000, mean episode reward: -39.93574497343974, num_cumulative_constraints: 185, num_done: 985, time: 123.776
steps: 15758261, episodes: 378000, mean episode reward: -39.89172149673627, num_cumulative_constraints: 187, num_done: 981, time: 123.845
steps: 15799547, episodes: 379000, mean episode reward: -40.06088035449781, num_cumulative_constraints: 226, num_done: 975, time: 126.492
steps: 15840992, episodes: 380000, mean episode reward: -39.94480147394622, num_cumulative_constraints: 163, num_done: 974, time: 118.729
steps: 15882424, episodes: 381000, mean episode reward: -40.22518683211589, num_cumulative_constraints: 241, num_done: 971, time: 125.677
steps: 15924066, episodes: 382000, mean episode reward: -40.250306080530954, num_cumulative_constraints: 232, num_done: 963, time: 133.808
steps: 15967619, episodes: 383000, mean episode reward: -40.879445664645964, num_cumulative_constraints: 338, num_done: 897, time: 124.784
steps: 16009249, episodes: 384000, mean episode reward: -39.97180299957757, num_cumulative_constraints: 189, num_done: 959, time: 126.716
steps: 16050391, episodes: 385000, mean episode reward: -40.05652329651548, num_cumulative_constraints: 251, num_done: 985, time: 128.838
steps: 16091813, episodes: 386000, mean episode reward: -40.22207185694217, num_cumulative_constraints: 251, num_done: 977, time: 119.784
steps: 16133881, episodes: 387000, mean episode reward: -40.40587931159525, num_cumulative_constraints: 299, num_done: 946, time: 122.417
steps: 16175511, episodes: 388000, mean episode reward: -40.2080320554198, num_cumulative_constraints: 260, num_done: 970, time: 125.14
steps: 16217214, episodes: 389000, mean episode reward: -39.92778157671497, num_cumulative_constraints: 222, num_done: 969, time: 130.006
steps: 16258971, episodes: 390000, mean episode reward: -39.89148124667848, num_cumulative_constraints: 193, num_done: 966, time: 124.621
steps: 16300807, episodes: 391000, mean episode reward: -40.01464633933507, num_cumulative_constraints: 191, num_done: 969, time: 131.154
steps: 16342562, episodes: 392000, mean episode reward: -39.83178389227141, num_cumulative_constraints: 168, num_done: 979, time: 123.699
steps: 16384122, episodes: 393000, mean episode reward: -39.90124020064226, num_cumulative_constraints: 188, num_done: 988, time: 130.261
steps: 16425281, episodes: 394000, mean episode reward: -39.897225950602454, num_cumulative_constraints: 233, num_done: 991, time: 125.565
steps: 16467232, episodes: 395000, mean episode reward: -40.37736947528789, num_cumulative_constraints: 285, num_done: 961, time: 125.558
steps: 16510350, episodes: 396000, mean episode reward: -40.50520384440766, num_cumulative_constraints: 215, num_done: 894, time: 129.783
steps: 16553592, episodes: 397000, mean episode reward: -40.48598464975488, num_cumulative_constraints: 155, num_done: 879, time: 139.405
steps: 16597401, episodes: 398000, mean episode reward: -40.393836878477344, num_cumulative_constraints: 133, num_done: 870, time: 125.236
steps: 16642820, episodes: 399000, mean episode reward: -41.06633382240592, num_cumulative_constraints: 254, num_done: 811, time: 124.366
steps: 16688362, episodes: 400000, mean episode reward: -41.06140224248843, num_cumulative_constraints: 205, num_done: 827, time: 128.818
steps: 16735207, episodes: 401000, mean episode reward: -41.28565335964445, num_cumulative_constraints: 258, num_done: 783, time: 133.069
steps: 16780222, episodes: 402000, mean episode reward: -40.70181438543924, num_cumulative_constraints: 253, num_done: 859, time: 128.148
steps: 16823240, episodes: 403000, mean episode reward: -40.77537619430322, num_cumulative_constraints: 361, num_done: 919, time: 129.917
steps: 16866259, episodes: 404000, mean episode reward: -40.82544150765987, num_cumulative_constraints: 347, num_done: 913, time: 133.084
steps: 16908635, episodes: 405000, mean episode reward: -40.33704602778487, num_cumulative_constraints: 242, num_done: 944, time: 129.792
steps: 16950021, episodes: 406000, mean episode reward: -39.952413205771016, num_cumulative_constraints: 189, num_done: 973, time: 127.709
steps: 16991017, episodes: 407000, mean episode reward: -39.79288486947221, num_cumulative_constraints: 155, num_done: 996, time: 127.401
steps: 17032018, episodes: 408000, mean episode reward: -39.7509243619067, num_cumulative_constraints: 152, num_done: 992, time: 129.315
steps: 17073043, episodes: 409000, mean episode reward: -39.913754183775566, num_cumulative_constraints: 232, num_done: 984, time: 126.014
steps: 17114371, episodes: 410000, mean episode reward: -40.16605175362917, num_cumulative_constraints: 232, num_done: 983, time: 127.179
steps: 17156064, episodes: 411000, mean episode reward: -40.52206897918182, num_cumulative_constraints: 360, num_done: 969, time: 121.324
steps: 17198150, episodes: 412000, mean episode reward: -41.023407956993324, num_cumulative_constraints: 444, num_done: 955, time: 128.132
steps: 17240875, episodes: 413000, mean episode reward: -40.73447995783742, num_cumulative_constraints: 257, num_done: 921, time: 131.951
steps: 17284193, episodes: 414000, mean episode reward: -41.24980822099799, num_cumulative_constraints: 317, num_done: 893, time: 127.346
steps: 17328211, episodes: 415000, mean episode reward: -41.58484818063635, num_cumulative_constraints: 342, num_done: 863, time: 127.445
steps: 17372269, episodes: 416000, mean episode reward: -41.076142159571596, num_cumulative_constraints: 229, num_done: 858, time: 128.081
steps: 17414996, episodes: 417000, mean episode reward: -40.47000642672123, num_cumulative_constraints: 212, num_done: 923, time: 122.814
steps: 17456586, episodes: 418000, mean episode reward: -40.305284919666704, num_cumulative_constraints: 223, num_done: 987, time: 133.489
steps: 17498027, episodes: 419000, mean episode reward: -40.29905732442771, num_cumulative_constraints: 266, num_done: 986, time: 126.853
steps: 17539242, episodes: 420000, mean episode reward: -40.39462401893465, num_cumulative_constraints: 331, num_done: 991, time: 128.412
steps: 17580498, episodes: 421000, mean episode reward: -40.749029551013955, num_cumulative_constraints: 396, num_done: 996, time: 135.789
steps: 17621723, episodes: 422000, mean episode reward: -40.591422876556926, num_cumulative_constraints: 339, num_done: 995, time: 129.426
steps: 17662724, episodes: 423000, mean episode reward: -39.87997895223119, num_cumulative_constraints: 201, num_done: 996, time: 131.79
steps: 17704097, episodes: 424000, mean episode reward: -40.12033560916099, num_cumulative_constraints: 169, num_done: 988, time: 127.269
steps: 17745903, episodes: 425000, mean episode reward: -40.29268293270555, num_cumulative_constraints: 179, num_done: 965, time: 119.978
steps: 17787692, episodes: 426000, mean episode reward: -40.17652340598201, num_cumulative_constraints: 169, num_done: 965, time: 122.328
steps: 17829204, episodes: 427000, mean episode reward: -40.30403061551825, num_cumulative_constraints: 217, num_done: 984, time: 134.396
steps: 17870580, episodes: 428000, mean episode reward: -40.18513204396077, num_cumulative_constraints: 205, num_done: 987, time: 133.386
steps: 17911831, episodes: 429000, mean episode reward: -40.163715437447955, num_cumulative_constraints: 224, num_done: 993, time: 133.272
steps: 17952859, episodes: 430000, mean episode reward: -40.26077504431358, num_cumulative_constraints: 314, num_done: 994, time: 128.997
steps: 17994167, episodes: 431000, mean episode reward: -40.01585905781988, num_cumulative_constraints: 149, num_done: 990, time: 128.971
steps: 18035579, episodes: 432000, mean episode reward: -40.064871540170614, num_cumulative_constraints: 107, num_done: 989, time: 133.477
steps: 18077255, episodes: 433000, mean episode reward: -40.26826615635689, num_cumulative_constraints: 128, num_done: 969, time: 123.979
steps: 18119388, episodes: 434000, mean episode reward: -40.19314968475797, num_cumulative_constraints: 145, num_done: 948, time: 128.334
steps: 18162425, episodes: 435000, mean episode reward: -40.39561262439825, num_cumulative_constraints: 131, num_done: 917, time: 128.683
steps: 18206632, episodes: 436000, mean episode reward: -40.37907739797831, num_cumulative_constraints: 84, num_done: 892, time: 131.704
steps: 18253039, episodes: 437000, mean episode reward: -41.26361912712305, num_cumulative_constraints: 231, num_done: 815, time: 142.292
steps: 18299148, episodes: 438000, mean episode reward: -41.07104075537067, num_cumulative_constraints: 213, num_done: 867, time: 131.436
steps: 18345423, episodes: 439000, mean episode reward: -41.00792518455538, num_cumulative_constraints: 205, num_done: 853, time: 126.9
steps: 18390724, episodes: 440000, mean episode reward: -40.808342821111566, num_cumulative_constraints: 211, num_done: 855, time: 133.435
steps: 18435020, episodes: 441000, mean episode reward: -40.590681887594904, num_cumulative_constraints: 167, num_done: 876, time: 120.522
steps: 18477267, episodes: 442000, mean episode reward: -40.038011374226436, num_cumulative_constraints: 136, num_done: 957, time: 128.23
steps: 18518659, episodes: 443000, mean episode reward: -40.14131784315375, num_cumulative_constraints: 176, num_done: 989, time: 134.975
steps: 18559879, episodes: 444000, mean episode reward: -40.20491439192985, num_cumulative_constraints: 218, num_done: 996, time: 120.252
steps: 18601062, episodes: 445000, mean episode reward: -39.9209647383913, num_cumulative_constraints: 166, num_done: 990, time: 126.555
steps: 18642144, episodes: 446000, mean episode reward: -39.65754211473063, num_cumulative_constraints: 112, num_done: 995, time: 123.437
steps: 18683233, episodes: 447000, mean episode reward: -39.853919194713356, num_cumulative_constraints: 155, num_done: 996, time: 132.881
steps: 18724938, episodes: 448000, mean episode reward: -40.23611184391277, num_cumulative_constraints: 240, num_done: 969, time: 125.397
steps: 18767178, episodes: 449000, mean episode reward: -40.245518420787434, num_cumulative_constraints: 189, num_done: 939, time: 123.309
steps: 18809650, episodes: 450000, mean episode reward: -40.57512179754413, num_cumulative_constraints: 185, num_done: 930, time: 127.377
steps: 18851953, episodes: 451000, mean episode reward: -40.422470628086344, num_cumulative_constraints: 170, num_done: 920, time: 123.794
steps: 18894624, episodes: 452000, mean episode reward: -40.369879182506416, num_cumulative_constraints: 149, num_done: 900, time: 127.776
steps: 18938485, episodes: 453000, mean episode reward: -40.936382640570926, num_cumulative_constraints: 195, num_done: 840, time: 126.235
steps: 18983006, episodes: 454000, mean episode reward: -41.46559164336354, num_cumulative_constraints: 306, num_done: 815, time: 134.399
steps: 19026941, episodes: 455000, mean episode reward: -40.55142991119881, num_cumulative_constraints: 130, num_done: 860, time: 126.833
steps: 19070503, episodes: 456000, mean episode reward: -40.50382261131282, num_cumulative_constraints: 194, num_done: 885, time: 129.002
steps: 19112790, episodes: 457000, mean episode reward: -40.07098187523502, num_cumulative_constraints: 198, num_done: 937, time: 127.033
steps: 19154246, episodes: 458000, mean episode reward: -39.604435333330116, num_cumulative_constraints: 130, num_done: 964, time: 128.634
steps: 19195717, episodes: 459000, mean episode reward: -39.90530550535872, num_cumulative_constraints: 205, num_done: 965, time: 127.619
steps: 19236842, episodes: 460000, mean episode reward: -39.811785292241346, num_cumulative_constraints: 237, num_done: 975, time: 129.095
steps: 19277852, episodes: 461000, mean episode reward: -39.80631102710158, num_cumulative_constraints: 215, num_done: 984, time: 128.255
steps: 19318580, episodes: 462000, mean episode reward: -39.54844150909212, num_cumulative_constraints: 185, num_done: 995, time: 130.801
steps: 19359373, episodes: 463000, mean episode reward: -39.59499991300507, num_cumulative_constraints: 193, num_done: 992, time: 128.759
steps: 19400328, episodes: 464000, mean episode reward: -39.834379158746735, num_cumulative_constraints: 219, num_done: 985, time: 128.774
steps: 19441254, episodes: 465000, mean episode reward: -39.70063618457917, num_cumulative_constraints: 182, num_done: 984, time: 127.533
steps: 19482093, episodes: 466000, mean episode reward: -39.89473624105782, num_cumulative_constraints: 239, num_done: 991, time: 124.381
steps: 19522813, episodes: 467000, mean episode reward: -39.584842815543304, num_cumulative_constraints: 171, num_done: 991, time: 137.584
steps: 19563504, episodes: 468000, mean episode reward: -39.86922770598902, num_cumulative_constraints: 258, num_done: 997, time: 130.849
steps: 19604171, episodes: 469000, mean episode reward: -39.921736933595035, num_cumulative_constraints: 283, num_done: 995, time: 120.592
steps: 19645072, episodes: 470000, mean episode reward: -40.93890696813855, num_cumulative_constraints: 580, num_done: 987, time: 132.563
steps: 19685937, episodes: 471000, mean episode reward: -39.9263062583781, num_cumulative_constraints: 311, num_done: 982, time: 129.004
steps: 19726964, episodes: 472000, mean episode reward: -40.00322418361886, num_cumulative_constraints: 260, num_done: 980, time: 124.935
steps: 19767991, episodes: 473000, mean episode reward: -39.79831172250269, num_cumulative_constraints: 215, num_done: 978, time: 129.865
steps: 19809044, episodes: 474000, mean episode reward: -40.16447477259389, num_cumulative_constraints: 304, num_done: 978, time: 126.927
steps: 19850354, episodes: 475000, mean episode reward: -40.10927301841717, num_cumulative_constraints: 265, num_done: 966, time: 134.213
steps: 19891758, episodes: 476000, mean episode reward: -40.01177914975099, num_cumulative_constraints: 186, num_done: 967, time: 130.43
steps: 19932845, episodes: 477000, mean episode reward: -39.72962876848456, num_cumulative_constraints: 180, num_done: 975, time: 129.597
steps: 19974153, episodes: 478000, mean episode reward: -39.82262839391727, num_cumulative_constraints: 140, num_done: 969, time: 126.475
steps: 20015429, episodes: 479000, mean episode reward: -39.58234585143866, num_cumulative_constraints: 94, num_done: 967, time: 134.548
steps: 20056449, episodes: 480000, mean episode reward: -39.67626832119298, num_cumulative_constraints: 148, num_done: 984, time: 133.774
steps: 20097756, episodes: 481000, mean episode reward: -39.52093080537417, num_cumulative_constraints: 95, num_done: 971, time: 123.913
steps: 20138996, episodes: 482000, mean episode reward: -39.663093527242495, num_cumulative_constraints: 113, num_done: 976, time: 126.415
steps: 20180128, episodes: 483000, mean episode reward: -39.62362660223637, num_cumulative_constraints: 83, num_done: 982, time: 126.864
steps: 20221549, episodes: 484000, mean episode reward: -39.6969080407788, num_cumulative_constraints: 94, num_done: 971, time: 116.831
steps: 20262981, episodes: 485000, mean episode reward: -39.689296969844925, num_cumulative_constraints: 108, num_done: 964, time: 133.297
steps: 20304651, episodes: 486000, mean episode reward: -39.7268230316431, num_cumulative_constraints: 111, num_done: 959, time: 123.652
steps: 20347012, episodes: 487000, mean episode reward: -40.09972804004325, num_cumulative_constraints: 108, num_done: 937, time: 129.713
steps: 20390894, episodes: 488000, mean episode reward: -40.56646205841802, num_cumulative_constraints: 127, num_done: 857, time: 137.194
steps: 20434672, episodes: 489000, mean episode reward: -40.3870799377604, num_cumulative_constraints: 149, num_done: 889, time: 126.868
steps: 20478369, episodes: 490000, mean episode reward: -40.2422539131594, num_cumulative_constraints: 100, num_done: 887, time: 134.422
steps: 20521875, episodes: 491000, mean episode reward: -40.33161184985493, num_cumulative_constraints: 144, num_done: 925, time: 137.393
steps: 20564871, episodes: 492000, mean episode reward: -39.8827312841615, num_cumulative_constraints: 99, num_done: 931, time: 133.479
steps: 20607318, episodes: 493000, mean episode reward: -39.7727877857925, num_cumulative_constraints: 126, num_done: 953, time: 131.648
steps: 20649514, episodes: 494000, mean episode reward: -39.705297584627345, num_cumulative_constraints: 131, num_done: 957, time: 128.196
steps: 20691596, episodes: 495000, mean episode reward: -39.82613464213145, num_cumulative_constraints: 132, num_done: 956, time: 127.119
steps: 20733096, episodes: 496000, mean episode reward: -39.6322610494691, num_cumulative_constraints: 129, num_done: 974, time: 129.148
steps: 20774185, episodes: 497000, mean episode reward: -39.536912860593745, num_cumulative_constraints: 168, num_done: 982, time: 123.599
steps: 20815511, episodes: 498000, mean episode reward: -40.044956289249136, num_cumulative_constraints: 244, num_done: 976, time: 149.383
steps: 20856918, episodes: 499000, mean episode reward: -40.122665702009776, num_cumulative_constraints: 246, num_done: 980, time: 139.89
steps: 20898306, episodes: 500000, mean episode reward: -39.90752995926616, num_cumulative_constraints: 198, num_done: 977, time: 129.47
steps: 20939640, episodes: 501000, mean episode reward: -39.90571098332779, num_cumulative_constraints: 201, num_done: 971, time: 124.355
steps: 20981037, episodes: 502000, mean episode reward: -39.93480578521273, num_cumulative_constraints: 204, num_done: 981, time: 130.624
steps: 21022216, episodes: 503000, mean episode reward: -39.572045174989455, num_cumulative_constraints: 128, num_done: 983, time: 125.113
steps: 21063561, episodes: 504000, mean episode reward: -39.61346388940468, num_cumulative_constraints: 137, num_done: 975, time: 125.856
steps: 21104887, episodes: 505000, mean episode reward: -39.80107258523023, num_cumulative_constraints: 158, num_done: 981, time: 124.389
steps: 21146224, episodes: 506000, mean episode reward: -39.66885755991783, num_cumulative_constraints: 162, num_done: 974, time: 124.487
steps: 21187315, episodes: 507000, mean episode reward: -39.38015986202321, num_cumulative_constraints: 75, num_done: 984, time: 125.729
steps: 21228500, episodes: 508000, mean episode reward: -39.67560269867838, num_cumulative_constraints: 184, num_done: 979, time: 122.351
steps: 21269760, episodes: 509000, mean episode reward: -39.57905245516554, num_cumulative_constraints: 121, num_done: 976, time: 124.548
steps: 21311264, episodes: 510000, mean episode reward: -39.81231068099329, num_cumulative_constraints: 154, num_done: 965, time: 128.812
steps: 21352817, episodes: 511000, mean episode reward: -39.720543278179335, num_cumulative_constraints: 116, num_done: 967, time: 121.235
steps: 21394636, episodes: 512000, mean episode reward: -39.74543377910167, num_cumulative_constraints: 85, num_done: 954, time: 132.039
steps: 21436439, episodes: 513000, mean episode reward: -39.835900980028, num_cumulative_constraints: 126, num_done: 950, time: 129.262
steps: 21478321, episodes: 514000, mean episode reward: -39.68492370331268, num_cumulative_constraints: 105, num_done: 946, time: 128.718
steps: 21519844, episodes: 515000, mean episode reward: -39.806112052657596, num_cumulative_constraints: 126, num_done: 960, time: 127.652
steps: 21561329, episodes: 516000, mean episode reward: -39.7878788142699, num_cumulative_constraints: 164, num_done: 960, time: 134.462
steps: 21603131, episodes: 517000, mean episode reward: -39.68233098759986, num_cumulative_constraints: 118, num_done: 943, time: 127.012
steps: 21644885, episodes: 518000, mean episode reward: -39.78014971305212, num_cumulative_constraints: 126, num_done: 945, time: 126.826
steps: 21686857, episodes: 519000, mean episode reward: -39.78133971069888, num_cumulative_constraints: 69, num_done: 934, time: 125.632
steps: 21728739, episodes: 520000, mean episode reward: -39.768511720672805, num_cumulative_constraints: 128, num_done: 940, time: 124.802
steps: 21770778, episodes: 521000, mean episode reward: -39.683541754206935, num_cumulative_constraints: 76, num_done: 927, time: 129.875
steps: 21812972, episodes: 522000, mean episode reward: -39.848435474370085, num_cumulative_constraints: 96, num_done: 924, time: 127.214
steps: 21855066, episodes: 523000, mean episode reward: -40.052932115266145, num_cumulative_constraints: 176, num_done: 923, time: 124.217
steps: 21897705, episodes: 524000, mean episode reward: -40.37918267860592, num_cumulative_constraints: 199, num_done: 900, time: 126.863
steps: 21940136, episodes: 525000, mean episode reward: -40.04855956517661, num_cumulative_constraints: 99, num_done: 911, time: 123.352
steps: 21982827, episodes: 526000, mean episode reward: -40.21114458197278, num_cumulative_constraints: 129, num_done: 897, time: 126.69
steps: 22025763, episodes: 527000, mean episode reward: -40.32962051986723, num_cumulative_constraints: 105, num_done: 897, time: 119.467
steps: 22068794, episodes: 528000, mean episode reward: -39.96539708025186, num_cumulative_constraints: 91, num_done: 908, time: 124.754
steps: 22112287, episodes: 529000, mean episode reward: -40.51026879602661, num_cumulative_constraints: 187, num_done: 910, time: 127.284
steps: 22155544, episodes: 530000, mean episode reward: -40.24014605519308, num_cumulative_constraints: 128, num_done: 919, time: 133.768
steps: 22200593, episodes: 531000, mean episode reward: -40.729011468130324, num_cumulative_constraints: 163, num_done: 860, time: 138.864
steps: 22244486, episodes: 532000, mean episode reward: -40.07340893180545, num_cumulative_constraints: 124, num_done: 919, time: 130.333
steps: 22289028, episodes: 533000, mean episode reward: -40.33048342687139, num_cumulative_constraints: 89, num_done: 880, time: 130.398
steps: 22331864, episodes: 534000, mean episode reward: -40.02578010881576, num_cumulative_constraints: 122, num_done: 931, time: 128.318
steps: 22374864, episodes: 535000, mean episode reward: -39.993971915040575, num_cumulative_constraints: 108, num_done: 937, time: 123.823
steps: 22417946, episodes: 536000, mean episode reward: -40.18621258863368, num_cumulative_constraints: 147, num_done: 938, time: 121.48
steps: 22460966, episodes: 537000, mean episode reward: -40.51253236627617, num_cumulative_constraints: 228, num_done: 931, time: 131.133
steps: 22504252, episodes: 538000, mean episode reward: -40.16318419707685, num_cumulative_constraints: 109, num_done: 919, time: 126.594
steps: 22547467, episodes: 539000, mean episode reward: -40.159040437310274, num_cumulative_constraints: 153, num_done: 915, time: 132.855
steps: 22591098, episodes: 540000, mean episode reward: -40.40706949665792, num_cumulative_constraints: 160, num_done: 901, time: 128.037
steps: 22633970, episodes: 541000, mean episode reward: -40.04394909520479, num_cumulative_constraints: 154, num_done: 924, time: 125.892
steps: 22676790, episodes: 542000, mean episode reward: -39.85961995024674, num_cumulative_constraints: 81, num_done: 928, time: 130.535
steps: 22719415, episodes: 543000, mean episode reward: -39.96975271911949, num_cumulative_constraints: 108, num_done: 931, time: 130.234
steps: 22761868, episodes: 544000, mean episode reward: -40.235412136647994, num_cumulative_constraints: 209, num_done: 941, time: 128.968
steps: 22803638, episodes: 545000, mean episode reward: -39.70477476987755, num_cumulative_constraints: 136, num_done: 963, time: 120.315
steps: 22845204, episodes: 546000, mean episode reward: -39.7973702425825, num_cumulative_constraints: 144, num_done: 970, time: 129.301
steps: 22886417, episodes: 547000, mean episode reward: -39.72674991331125, num_cumulative_constraints: 162, num_done: 986, time: 118.469
steps: 22927560, episodes: 548000, mean episode reward: -39.64564014855531, num_cumulative_constraints: 125, num_done: 988, time: 131.539
steps: 22968897, episodes: 549000, mean episode reward: -39.86314237317168, num_cumulative_constraints: 183, num_done: 979, time: 121.396
steps: 23010052, episodes: 550000, mean episode reward: -39.816510942849334, num_cumulative_constraints: 141, num_done: 990, time: 128.356
steps: 23051116, episodes: 551000, mean episode reward: -39.54278910444384, num_cumulative_constraints: 116, num_done: 989, time: 130.7
steps: 23092243, episodes: 552000, mean episode reward: -39.748885699819304, num_cumulative_constraints: 119, num_done: 995, time: 128.015
steps: 23133251, episodes: 553000, mean episode reward: -39.55679480112209, num_cumulative_constraints: 101, num_done: 994, time: 127.983
steps: 23174492, episodes: 554000, mean episode reward: -39.95148360288029, num_cumulative_constraints: 163, num_done: 989, time: 133.477
steps: 23215652, episodes: 555000, mean episode reward: -39.6847717739644, num_cumulative_constraints: 116, num_done: 989, time: 126.383
steps: 23257166, episodes: 556000, mean episode reward: -39.694613364261926, num_cumulative_constraints: 89, num_done: 976, time: 131.56
steps: 23298405, episodes: 557000, mean episode reward: -39.59319115620432, num_cumulative_constraints: 89, num_done: 984, time: 128.236
steps: 23339547, episodes: 558000, mean episode reward: -39.59978939049497, num_cumulative_constraints: 107, num_done: 989, time: 126.554
steps: 23380835, episodes: 559000, mean episode reward: -39.64064961218966, num_cumulative_constraints: 126, num_done: 985, time: 124.219
steps: 23421985, episodes: 560000, mean episode reward: -39.649640132110655, num_cumulative_constraints: 164, num_done: 985, time: 125.234
steps: 23463199, episodes: 561000, mean episode reward: -39.62099925105731, num_cumulative_constraints: 129, num_done: 980, time: 130.224
steps: 23504289, episodes: 562000, mean episode reward: -39.5900960774187, num_cumulative_constraints: 122, num_done: 988, time: 127.677
steps: 23545204, episodes: 563000, mean episode reward: -39.44441814864469, num_cumulative_constraints: 84, num_done: 994, time: 128.184
steps: 23585995, episodes: 564000, mean episode reward: -39.35575970637558, num_cumulative_constraints: 117, num_done: 997, time: 122.414
steps: 23626916, episodes: 565000, mean episode reward: -39.569861276940536, num_cumulative_constraints: 140, num_done: 995, time: 132.72
steps: 23667935, episodes: 566000, mean episode reward: -39.758634569277284, num_cumulative_constraints: 152, num_done: 995, time: 127.52
steps: 23708946, episodes: 567000, mean episode reward: -39.54806405524804, num_cumulative_constraints: 103, num_done: 998, time: 129.555
steps: 23749897, episodes: 568000, mean episode reward: -39.37761722546864, num_cumulative_constraints: 58, num_done: 996, time: 125.365
steps: 23790861, episodes: 569000, mean episode reward: -39.62835739857523, num_cumulative_constraints: 96, num_done: 997, time: 128.223
steps: 23831612, episodes: 570000, mean episode reward: -39.52416836169837, num_cumulative_constraints: 142, num_done: 1000, time: 124.486
steps: 23873094, episodes: 571000, mean episode reward: -39.98495849519937, num_cumulative_constraints: 215, num_done: 986, time: 124.92
steps: 23916265, episodes: 572000, mean episode reward: -40.18395880284832, num_cumulative_constraints: 160, num_done: 955, time: 125.613
steps: 23961059, episodes: 573000, mean episode reward: -40.472559025884976, num_cumulative_constraints: 192, num_done: 907, time: 127.969
steps: 24008585, episodes: 574000, mean episode reward: -41.356673053464284, num_cumulative_constraints: 236, num_done: 774, time: 128.886
steps: 24055609, episodes: 575000, mean episode reward: -41.36914644904256, num_cumulative_constraints: 256, num_done: 812, time: 134.464
steps: 24097624, episodes: 576000, mean episode reward: -40.10795817653353, num_cumulative_constraints: 192, num_done: 946, time: 122.7
steps: 24139000, episodes: 577000, mean episode reward: -39.91678283787405, num_cumulative_constraints: 173, num_done: 971, time: 128.276
steps: 24180461, episodes: 578000, mean episode reward: -40.04224921231091, num_cumulative_constraints: 213, num_done: 969, time: 131.158
steps: 24222684, episodes: 579000, mean episode reward: -40.37500054805832, num_cumulative_constraints: 254, num_done: 936, time: 127.958
steps: 24266311, episodes: 580000, mean episode reward: -40.58882905585636, num_cumulative_constraints: 215, num_done: 860, time: 133.725
steps: 24309839, episodes: 581000, mean episode reward: -40.71001010018278, num_cumulative_constraints: 274, num_done: 865, time: 126.816
steps: 24352991, episodes: 582000, mean episode reward: -40.52038415899318, num_cumulative_constraints: 257, num_done: 903, time: 131.1
steps: 24395430, episodes: 583000, mean episode reward: -40.423617014323824, num_cumulative_constraints: 253, num_done: 938, time: 137.828
steps: 24437448, episodes: 584000, mean episode reward: -40.0185200587766, num_cumulative_constraints: 179, num_done: 962, time: 128.449
steps: 24479649, episodes: 585000, mean episode reward: -39.979313996576096, num_cumulative_constraints: 147, num_done: 958, time: 129.021
steps: 24525770, episodes: 586000, mean episode reward: -40.94062356399786, num_cumulative_constraints: 218, num_done: 855, time: 129.48
steps: 24573976, episodes: 587000, mean episode reward: -41.91943705246923, num_cumulative_constraints: 291, num_done: 771, time: 134.781
steps: 24617851, episodes: 588000, mean episode reward: -40.355045082647976, num_cumulative_constraints: 130, num_done: 914, time: 126.055
steps: 24661406, episodes: 589000, mean episode reward: -40.681551163252315, num_cumulative_constraints: 226, num_done: 896, time: 130.769
steps: 24705426, episodes: 590000, mean episode reward: -40.54980485346144, num_cumulative_constraints: 230, num_done: 911, time: 132.923
steps: 24748693, episodes: 591000, mean episode reward: -39.91284863111442, num_cumulative_constraints: 161, num_done: 955, time: 123.15
steps: 24791574, episodes: 592000, mean episode reward: -40.156995392669245, num_cumulative_constraints: 209, num_done: 959, time: 126.451
steps: 24834477, episodes: 593000, mean episode reward: -40.32136737918606, num_cumulative_constraints: 274, num_done: 972, time: 123.319
steps: 24877364, episodes: 594000, mean episode reward: -40.01553738390561, num_cumulative_constraints: 189, num_done: 945, time: 127.691
steps: 24920198, episodes: 595000, mean episode reward: -40.0145346603038, num_cumulative_constraints: 187, num_done: 941, time: 116.643
steps: 24962711, episodes: 596000, mean episode reward: -39.92204757846818, num_cumulative_constraints: 155, num_done: 932, time: 128.008
steps: 25004863, episodes: 597000, mean episode reward: -39.98210346267132, num_cumulative_constraints: 225, num_done: 954, time: 134.555
steps: 25047037, episodes: 598000, mean episode reward: -40.01304572225317, num_cumulative_constraints: 179, num_done: 954, time: 131.092
steps: 25089082, episodes: 599000, mean episode reward: -39.790949064042806, num_cumulative_constraints: 106, num_done: 953, time: 135.261
steps: 25130592, episodes: 600000, mean episode reward: -39.87189105584186, num_cumulative_constraints: 177, num_done: 971, time: 126.836
Traceback (most recent call last):



an acceptable result
steps: 41469, episodes: 1000, mean episode reward: -39.83992819672483, num_cumulative_constraints: 120, num_done: 994, time: 123.663
steps: 83034, episodes: 2000, mean episode reward: -40.034078983784056, num_cumulative_constraints: 147, num_done: 997, time: 134.821
steps: 124759, episodes: 3000, mean episode reward: -40.04824604058396, num_cumulative_constraints: 151, num_done: 992, time: 122.275
steps: 41515, episodes: 1000, mean episode reward: -39.75131346087127, num_cumulative_constraints: 118, num_done: 991, time: 127.824
steps: 83129, episodes: 2000, mean episode reward: -39.95366077647824, num_cumulative_constraints: 126, num_done: 989, time: 130.875
steps: 124521, episodes: 3000, mean episode reward: -39.63181597593854, num_cumulative_constraints: 82, num_done: 998, time: 126.004
steps: 165935, episodes: 4000, mean episode reward: -39.77201762668582, num_cumulative_constraints: 123, num_done: 995, time: 128.293
steps: 207608, episodes: 5000, mean episode reward: -39.82561552212753, num_cumulative_constraints: 92, num_done: 990, time: 126.681



using safety layer 1.0
steps: 42605, episodes: 1000, mean episode reward: -41.319606863964474, num_cumulative_constraints: 239, num_done: 971, time: 161.816
steps: 85235, episodes: 2000, mean episode reward: -41.5092998554965, num_cumulative_constraints: 216, num_done: 974, time: 171.057
steps: 127774, episodes: 3000, mean episode reward: -41.18617473013557, num_cumulative_constraints: 138, num_done: 976, time: 167.155
steps: 170266, episodes: 4000, mean episode reward: -41.06148652123632, num_cumulative_constraints: 196, num_done: 982, time: 178.891
steps: 212669, episodes: 5000, mean episode reward: -41.01778539567716, num_cumulative_constraints: 200, num_done: 972, time: 173.34
steps: 255167, episodes: 6000, mean episode reward: -41.131256076109246, num_cumulative_constraints: 182, num_done: 982, time: 170.115
steps: 297792, episodes: 7000, mean episode reward: -41.038783092513846, num_cumulative_constraints: 168, num_done: 971, time: 168.574


using safety_layer y3 x3 temp = 0.05
steps: 41493, episodes: 1000, mean episode reward: -40.02923936433127, num_cumulative_constraints: 166, num_done: 991, time: 114.512
71
steps: 83132, episodes: 2000, mean episode reward: -40.00084959849031, num_cumulative_constraints: 139, num_done: 992, time: 119.236
135
steps: 124704, episodes: 3000, mean episode reward: -39.8685330472544, num_cumulative_constraints: 125, num_done: 991, time: 121.566
197
steps: 166116, episodes: 4000, mean episode reward: -39.814159057460685, num_cumulative_constraints: 172, num_done: 991, time: 119.519
281


X1.1
steps: 41551, episodes: 1000, mean episode reward: -39.99305123436322, num_cumulative_constraints: 143, num_done: 987, time: 119.689
175
steps: 83006, episodes: 2000, mean episode reward: -39.73187525220397, num_cumulative_constraints: 112, num_done: 998, time: 123.973
302
steps: 124615, episodes: 3000, mean episode reward: -39.83026720550817, num_cumulative_constraints: 109, num_done: 994, time: 134.565
438
steps: 166070, episodes: 4000, mean episode reward: -39.67281694898806, num_cumulative_constraints: 110, num_done: 993, time: 133.032
567
steps: 207555, episodes: 5000, mean episode reward: -39.907986174363465, num_cumulative_constraints: 128, num_done: 998, time: 131.997
741
steps: 249056, episodes: 6000, mean episode reward: -39.975617668267255, num_cumulative_constraints: 171, num_done: 996, time: 125.381
919
steps: 290559, episodes: 7000, mean episode reward: -39.74281104367608, num_cumulative_constraints: 134, num_done: 989, time: 135.75
1088
steps: 331989, episodes: 8000, mean episode reward: -39.770058077148015, num_cumulative_constraints: 130, num_done: 996, time: 132.915
1244
steps: 373473, episodes: 9000, mean episode reward: -39.82116129035104, num_cumulative_constraints: 138, num_done: 992, time: 132.319
1379
steps: 414999, episodes: 10000, mean episode reward: -39.97150219772702, num_cumulative_constraints: 153, num_done: 991, time: 133.327
1541
steps: 456454, episodes: 11000, mean episode reward: -39.909307152591836, num_cumulative_constraints: 150, num_done: 994, time: 127.39
1702
steps: 497998, episodes: 12000, mean episode reward: -39.94235472209215, num_cumulative_constraints: 160, num_done: 986, time: 133.047
1876
steps: 539419, episodes: 13000, mean episode reward: -39.89744697179729, num_cumulative_constraints: 134, num_done: 994, time: 128.312
2039
steps: 41553, episodes: 1000, mean episode reward: -39.88857602758861, num_cumulative_constraints: 126, num_done: 995, time: 127.139
144
steps: 83214, episodes: 2000, mean episode reward: -39.959381777441756, num_cumulative_constraints: 139, num_done: 991, time: 135.707
270
steps: 124715, episodes: 3000, mean episode reward: -39.73763032424228, num_cumulative_constraints: 108, num_done: 995, time: 137.131
403
steps: 166299, episodes: 4000, mean episode reward: -40.05057860287625, num_cumulative_constraints: 165, num_done: 989, time: 278.436
570
steps: 207881, episodes: 5000, mean episode reward: -40.13707775345632, num_cumulative_constraints: 163, num_done: 991, time: 298.118
715
steps: 249417, episodes: 6000, mean episode reward: -39.85875164531, num_cumulative_constraints: 118, num_done: 997, time: 292.917
874
steps: 290855, episodes: 7000, mean episode reward: -39.83031113515669, num_cumulative_constraints: 151, num_done: 994, time: 302.515
1010
steps: 332375, episodes: 8000, mean episode reward: -39.840847108751575, num_cumulative_constraints: 132, num_done: 993, time: 300.001
1159
steps: 373963, episodes: 9000, mean episode reward: -39.9974028836603, num_cumulative_constraints: 155, num_done: 992, time: 306.963
1312
steps: 415533, episodes: 10000, mean episode reward: -39.858307506800905, num_cumulative_constraints: 112, num_done: 991, time: 334.708
1434
steps: 456987, episodes: 11000, mean episode reward: -39.808328576391276, num_cumulative_constraints: 123, num_done: 996, time: 340.3
1569
steps: 498468, episodes: 12000, mean episode reward: -39.78445778657459, num_cumulative_constraints: 120, num_done: 995, time: 319.963
1700
steps: 539985, episodes: 13000, mean episode reward: -39.898216547935824, num_cumulative_constraints: 137, num_done: 996, time: 335.952
1856
steps: 581594, episodes: 14000, mean episode reward: -40.06442082384769, num_cumulative_constraints: 155, num_done: 992, time: 348.991
2013
steps: 623171, episodes: 15000, mean episode reward: -40.02785626815886, num_cumulative_constraints: 145, num_done: 995, time: 329.078
2162
steps: 664672, episodes: 16000, mean episode reward: -39.887081732060544, num_cumulative_constraints: 134, num_done: 993, time: 342.284
2306
steps: 706304, episodes: 17000, mean episode reward: -39.93315274734695, num_cumulative_constraints: 137, num_done: 988, time: 351.792
2457
steps: 747757, episodes: 18000, mean episode reward: -39.789036967605995, num_cumulative_constraints: 133, num_done: 992, time: 333.941
2578
steps: 789304, episodes: 19000, mean episode reward: -39.742552119807584, num_cumulative_constraints: 86, num_done: 992, time: 329.164
2697
steps: 830706, episodes: 20000, mean episode reward: -39.81092640346106, num_cumulative_constraints: 130, num_done: 993, time: 316.782
2840
steps: 872048, episodes: 21000, mean episode reward: -39.62526254902734, num_cumulative_constraints: 94, num_done: 999, time: 323.808
2961
steps: 913590, episodes: 22000, mean episode reward: -39.70541912264592, num_cumulative_constraints: 111, num_done: 992, time: 349.921
3094
steps: 955195, episodes: 23000, mean episode reward: -40.03273942236347, num_cumulative_constraints: 153, num_done: 988, time: 330.008
3248
steps: 996822, episodes: 24000, mean episode reward: -39.834391184721326, num_cumulative_constraints: 116, num_done: 988, time: 336.235
3395
steps: 1038267, episodes: 25000, mean episode reward: -39.7633997700333, num_cumulative_constraints: 118, num_done: 994, time: 315.152
3524
steps: 1079769, episodes: 26000, mean episode reward: -39.8035106173446, num_cumulative_constraints: 115, num_done: 995, time: 333.467
3672
steps: 1121288, episodes: 27000, mean episode reward: -39.7366452885396, num_cumulative_constraints: 117, num_done: 992, time: 309.0
3805
steps: 1162718, episodes: 28000, mean episode reward: -39.67981365818451, num_cumulative_constraints: 138, num_done: 992, time: 334.633
3949
steps: 1204313, episodes: 29000, mean episode reward: -39.8620408201118, num_cumulative_constraints: 134, num_done: 982, time: 332.747
4096
steps: 1246058, episodes: 30000, mean episode reward: -39.9943966761848, num_cumulative_constraints: 127, num_done: 991, time: 322.517
4223
steps: 1287587, episodes: 31000, mean episode reward: -39.69469493489465, num_cumulative_constraints: 107, num_done: 995, time: 312.391
4348
steps: 1329268, episodes: 32000, mean episode reward: -39.93099934638863, num_cumulative_constraints: 119, num_done: 991, time: 328.709
4475
steps: 1370784, episodes: 33000, mean episode reward: -39.78268805096348, num_cumulative_constraints: 120, num_done: 994, time: 328.02
4615
steps: 1412304, episodes: 34000, mean episode reward: -39.877249357211944, num_cumulative_constraints: 134, num_done: 994, time: 348.462
4753
steps: 1453930, episodes: 35000, mean episode reward: -39.97355772918436, num_cumulative_constraints: 168, num_done: 990, time: 339.524
4905
steps: 1495272, episodes: 36000, mean episode reward: -39.644052416170965, num_cumulative_constraints: 112, num_done: 993, time: 354.116
5035
steps: 1536627, episodes: 37000, mean episode reward: -39.626434051745335, num_cumulative_constraints: 105, num_done: 997, time: 323.255
5168
steps: 1578275, episodes: 38000, mean episode reward: -39.92704671287913, num_cumulative_constraints: 126, num_done: 984, time: 328.408
5327
steps: 1619940, episodes: 39000, mean episode reward: -39.8339985986503, num_cumulative_constraints: 118, num_done: 991, time: 337.221
5462
steps: 1661493, episodes: 40000, mean episode reward: -40.00886232515558, num_cumulative_constraints: 135, num_done: 994, time: 330.966
5607
steps: 1703116, episodes: 41000, mean episode reward: -39.94674859550904, num_cumulative_constraints: 138, num_done: 989, time: 339.569
5749
steps: 1744892, episodes: 42000, mean episode reward: -40.019524432684015, num_cumulative_constraints: 137, num_done: 990, time: 327.296
5886
steps: 1786528, episodes: 43000, mean episode reward: -40.16195902366128, num_cumulative_constraints: 168, num_done: 993, time: 325.926
6048
steps: 1827794, episodes: 44000, mean episode reward: -39.60162590648306, num_cumulative_constraints: 116, num_done: 997, time: 311.017
6188
steps: 1869165, episodes: 45000, mean episode reward: -39.609983651860546, num_cumulative_constraints: 88, num_done: 995, time: 311.947
6291
steps: 1910595, episodes: 46000, mean episode reward: -39.75247357108835, num_cumulative_constraints: 129, num_done: 995, time: 325.98
6444
steps: 1952124, episodes: 47000, mean episode reward: -39.929896312244864, num_cumulative_constraints: 155, num_done: 990, time: 343.134
6596
steps: 1993649, episodes: 48000, mean episode reward: -39.85421180442692, num_cumulative_constraints: 136, num_done: 990, time: 324.354
6750
steps: 2035110, episodes: 49000, mean episode reward: -39.7642059593575, num_cumulative_constraints: 160, num_done: 994, time: 327.432
6913
steps: 2076627, episodes: 50000, mean episode reward: -39.87451872610123, num_cumulative_constraints: 128, num_done: 992, time: 332.73
7072
steps: 2118065, episodes: 51000, mean episode reward: -39.75383168840947, num_cumulative_constraints: 114, num_done: 994, time: 321.457
7212
steps: 2159740, episodes: 52000, mean episode reward: -39.987425886230746, num_cumulative_constraints: 120, num_done: 990, time: 327.259
7350
steps: 2201336, episodes: 53000, mean episode reward: -39.81693108341406, num_cumulative_constraints: 141, num_done: 985, time: 337.303
7505
steps: 2242875, episodes: 54000, mean episode reward: -39.76967278901118, num_cumulative_constraints: 120, num_done: 990, time: 356.691
7632
steps: 2284368, episodes: 55000, mean episode reward: -39.689730726801784, num_cumulative_constraints: 110, num_done: 992, time: 340.541
7762
steps: 2326033, episodes: 56000, mean episode reward: -40.09730787918405, num_cumulative_constraints: 151, num_done: 990, time: 323.032
7909
steps: 2367573, episodes: 57000, mean episode reward: -39.916267953280716, num_cumulative_constraints: 138, num_done: 995, time: 326.055
8047
steps: 2409068, episodes: 58000, mean episode reward: -39.82691175171256, num_cumulative_constraints: 125, num_done: 989, time: 329.078
8185
steps: 2450542, episodes: 59000, mean episode reward: -39.783129883743136, num_cumulative_constraints: 105, num_done: 994, time: 341.575
8309
steps: 2492119, episodes: 60000, mean episode reward: -39.8219956150001, num_cumulative_constraints: 100, num_done: 988, time: 336.002
8432
steps: 2533715, episodes: 61000, mean episode reward: -39.83587842805362, num_cumulative_constraints: 136, num_done: 992, time: 336.327
8589
steps: 2575180, episodes: 62000, mean episode reward: -39.888078309708206, num_cumulative_constraints: 119, num_done: 992, time: 328.36
8731
steps: 2616625, episodes: 63000, mean episode reward: -39.84113226173929, num_cumulative_constraints: 112, num_done: 994, time: 336.822
8867
steps: 2658209, episodes: 64000, mean episode reward: -39.780050638412156, num_cumulative_constraints: 115, num_done: 991, time: 328.61
9005
steps: 2699589, episodes: 65000, mean episode reward: -39.49376909348989, num_cumulative_constraints: 90, num_done: 994, time: 325.133
9133
steps: 2741194, episodes: 66000, mean episode reward: -39.75592114755289, num_cumulative_constraints: 96, num_done: 988, time: 319.262
9261
steps: 2782710, episodes: 67000, mean episode reward: -39.808385271915526, num_cumulative_constraints: 134, num_done: 993, time: 338.209
9395
steps: 2824260, episodes: 68000, mean episode reward: -39.77742143009155, num_cumulative_constraints: 88, num_done: 994, time: 358.504
9529
steps: 2865930, episodes: 69000, mean episode reward: -39.7786124977593, num_cumulative_constraints: 105, num_done: 993, time: 342.218
9672
steps: 2907327, episodes: 70000, mean episode reward: -39.86410829114219, num_cumulative_constraints: 138, num_done: 994, time: 322.448
9834
steps: 2948875, episodes: 71000, mean episode reward: -39.96357526834043, num_cumulative_constraints: 141, num_done: 991, time: 338.917
9999
steps: 2990475, episodes: 72000, mean episode reward: -39.96320296592238, num_cumulative_constraints: 164, num_done: 992, time: 329.468
10170
steps: 3032120, episodes: 73000, mean episode reward: -39.83539102689526, num_cumulative_constraints: 113, num_done: 992, time: 312.484
10306
steps: 3073641, episodes: 74000, mean episode reward: -39.84364797125565, num_cumulative_constraints: 137, num_done: 992, time: 346.065
10456
steps: 3115116, episodes: 75000, mean episode reward: -39.76400778259057, num_cumulative_constraints: 139, num_done: 990, time: 328.392
10591
steps: 3156805, episodes: 76000, mean episode reward: -39.99335469456206, num_cumulative_constraints: 123, num_done: 991, time: 336.759
10743
steps: 3198174, episodes: 77000, mean episode reward: -39.67328837274597, num_cumulative_constraints: 102, num_done: 996, time: 332.899
10867
steps: 3239932, episodes: 78000, mean episode reward: -39.87031206059072, num_cumulative_constraints: 114, num_done: 986, time: 334.019
10998
steps: 3281488, episodes: 79000, mean episode reward: -39.8397794501432, num_cumulative_constraints: 129, num_done: 992, time: 342.711
11144
steps: 3322917, episodes: 80000, mean episode reward: -39.82366748608748, num_cumulative_constraints: 112, num_done: 996, time: 324.955
11288
steps: 3364473, episodes: 81000, mean episode reward: -39.95387859259053, num_cumulative_constraints: 128, num_done: 987, time: 331.986
11431
steps: 3405983, episodes: 82000, mean episode reward: -39.76011718722731, num_cumulative_constraints: 116, num_done: 994, time: 336.81
11568
steps: 3447501, episodes: 83000, mean episode reward: -39.837981356844196, num_cumulative_constraints: 130, num_done: 994, time: 328.674
11695
steps: 3489089, episodes: 84000, mean episode reward: -39.97144313189931, num_cumulative_constraints: 141, num_done: 992, time: 334.527
11875
steps: 3530809, episodes: 85000, mean episode reward: -39.97087232861433, num_cumulative_constraints: 139, num_done: 990, time: 322.15
12015
steps: 3572356, episodes: 86000, mean episode reward: -39.757223574742675, num_cumulative_constraints: 126, num_done: 993, time: 330.778
12150
steps: 3613930, episodes: 87000, mean episode reward: -40.04609105678483, num_cumulative_constraints: 158, num_done: 991, time: 304.843
12299
steps: 3655473, episodes: 88000, mean episode reward: -39.86115792031066, num_cumulative_constraints: 131, num_done: 991, time: 310.69
12447
steps: 3697013, episodes: 89000, mean episode reward: -39.853883878842396, num_cumulative_constraints: 122, num_done: 989, time: 334.34
12577
steps: 3738594, episodes: 90000, mean episode reward: -39.83762122841913, num_cumulative_constraints: 100, num_done: 993, time: 332.849
12716
steps: 3780216, episodes: 91000, mean episode reward: -39.870187409113846, num_cumulative_constraints: 135, num_done: 991, time: 305.014
12834
steps: 3821800, episodes: 92000, mean episode reward: -40.087556287781, num_cumulative_constraints: 175, num_done: 992, time: 323.961
12982
steps: 3863306, episodes: 93000, mean episode reward: -39.889201538711724, num_cumulative_constraints: 125, num_done: 990, time: 324.958
13110
steps: 3904925, episodes: 94000, mean episode reward: -39.983442708317014, num_cumulative_constraints: 109, num_done: 994, time: 322.368
13246
steps: 3946450, episodes: 95000, mean episode reward: -39.89102392881733, num_cumulative_constraints: 145, num_done: 993, time: 335.887
13395
steps: 3987953, episodes: 96000, mean episode reward: -39.843048360242186, num_cumulative_constraints: 94, num_done: 996, time: 297.688
13511
steps: 4029455, episodes: 97000, mean episode reward: -39.86652432428686, num_cumulative_constraints: 137, num_done: 995, time: 322.989
13655
steps: 4070881, episodes: 98000, mean episode reward: -39.744994624495476, num_cumulative_constraints: 137, num_done: 994, time: 319.55
13826

X1.2
steps: 41598, episodes: 1000, mean episode reward: -40.121175702913625, num_cumulative_constraints: 192, num_done: 985, time: 121.793
402
steps: 83311, episodes: 2000, mean episode reward: -40.22942043119062, num_cumulative_constraints: 184, num_done: 987, time: 141.034
804
steps: 124974, episodes: 3000, mean episode reward: -40.117996249539935, num_cumulative_constraints: 146, num_done: 987, time: 132.127
1169
steps: 166663, episodes: 4000, mean episode reward: -40.11817951984788, num_cumulative_constraints: 176, num_done: 988, time: 137.337
1572
steps: 208184, episodes: 5000, mean episode reward: -39.915412812265274, num_cumulative_constraints: 144, num_done: 994, time: 141.772
1986
steps: 249869, episodes: 6000, mean episode reward: -40.11667480453638, num_cumulative_constraints: 153, num_done: 991, time: 138.104
2401




new without safety

new with safety
steps: 41753, episodes: 1000, mean episode reward: -39.86247824299734, num_cumulative_constraints: 96, num_done: 992, time: 128.018
333
steps: 83367, episodes: 2000, mean episode reward: -39.89842840501026, num_cumulative_constraints: 112, num_done: 991, time: 127.632
710
steps: 124965, episodes: 3000, mean episode reward: -39.67525040308788, num_cumulative_constraints: 68, num_done: 991, time: 134.836
1040
steps: 166734, episodes: 4000, mean episode reward: -40.08255258167074, num_cumulative_constraints: 110, num_done: 985, time: 263.051
1410
steps: 208183, episodes: 5000, mean episode reward: -39.578053617189624, num_cumulative_constraints: 69, num_done: 994, time: 269.271
1771
steps: 249802, episodes: 6000, mean episode reward: -39.91403662806685, num_cumulative_constraints: 93, num_done: 993, time: 280.243
2126
steps: 291435, episodes: 7000, mean episode reward: -39.841103215069715, num_cumulative_constraints: 88, num_done: 992, time: 276.981
2472
steps: 332963, episodes: 8000, mean episode reward: -39.67304957461945, num_cumulative_constraints: 61, num_done: 993, time: 281.48
2784
steps: 374591, episodes: 9000, mean episode reward: -39.93733123803091, num_cumulative_constraints: 92, num_done: 990, time: 320.314
3129
steps: 416145, episodes: 10000, mean episode reward: -39.73061771761711, num_cumulative_constraints: 82, num_done: 991, time: 317.015
3467
steps: 457750, episodes: 11000, mean episode reward: -40.003497613120466, num_cumulative_constraints: 118, num_done: 989, time: 314.132
3815
steps: 499324, episodes: 12000, mean episode reward: -39.78589610580675, num_cumulative_constraints: 87, num_done: 992, time: 326.256
4192
steps: 540909, episodes: 13000, mean episode reward: -39.89333683986034, num_cumulative_constraints: 92, num_done: 990, time: 325.663
4548
steps: 582521, episodes: 14000, mean episode reward: -39.68037783997762, num_cumulative_constraints: 60, num_done: 990, time: 320.531
4904

average num_constraints = 1228/14 = 87.714
average num_done = 13872/14 = 990.92857

steps: 41568, episodes: 1000, mean episode reward: -39.78693610734858, num_cumulative_constraints: 107, num_done: 984, time: 127.141
374
steps: 82955, episodes: 2000, mean episode reward: -39.516385771968935, num_cumulative_constraints: 63, num_done: 993, time: 120.656
718
steps: 124479, episodes: 3000, mean episode reward: -39.73912033692116, num_cumulative_constraints: 98, num_done: 988, time: 130.541
1128
steps: 166148, episodes: 4000, mean episode reward: -39.840122199682256, num_cumulative_constraints: 94, num_done: 991, time: 123.598
1463
steps: 207824, episodes: 5000, mean episode reward: -39.79383732336739, num_cumulative_constraints: 80, num_done: 989, time: 120.164
1795
steps: 249567, episodes: 6000, mean episode reward: -39.86510417786926, num_cumulative_constraints: 78, num_done: 992, time: 129.398
2123
steps: 291189, episodes: 7000, mean episode reward: -39.99466842543759, num_cumulative_constraints: 124, num_done: 988, time: 129.493
2522
steps: 332797, episodes: 8000, mean episode reward: -39.91196031848835, num_cumulative_constraints: 77, num_done: 990, time: 119.154
2866
steps: 374493, episodes: 9000, mean episode reward: -40.0167565713303, num_cumulative_constraints: 107, num_done: 990, time: 138.83
3275
steps: 416047, episodes: 10000, mean episode reward: -39.843069290472194, num_cumulative_constraints: 104, num_done: 991, time: 131.319
3640
steps: 457620, episodes: 11000, mean episode reward: -39.79481297049641, num_cumulative_constraints: 80, num_done: 996, time: 124.89
3980
steps: 499208, episodes: 12000, mean episode reward: -39.87690010364549, num_cumulative_constraints: 103, num_done: 989, time: 127.443
4343
steps: 540949, episodes: 13000, mean episode reward: -39.862020287981224, num_cumulative_constraints: 76, num_done: 985, time: 134.572
4673
steps: 582661, episodes: 14000, mean episode reward: -39.96558346794127, num_cumulative_constraints: 94, num_done: 989, time: 130.87
5071
steps: 624346, episodes: 15000, mean episode reward: -39.85799970716064, num_cumulative_constraints: 66, num_done: 993, time: 139.648
5423
steps: 666149, episodes: 16000, mean episode reward: -40.153016731628696, num_cumulative_constraints: 133, num_done: 982, time: 125.57
5788

1285/14 = 91.78
13855/14 = 989.64
1484/16 = 92.75
15830/16 = 989.375


temp from 0.03 to 0.044
steps: 41695, episodes: 1000, mean episode reward: -39.944784867160216, num_cumulative_constraints: 82, num_done: 989, time: 144.019
605
steps: 83311, episodes: 2000, mean episode reward: -39.77554775834849, num_cumulative_constraints: 83, num_done: 989, time: 138.422
1150
steps: 124951, episodes: 3000, mean episode reward: -39.898973958109636, num_cumulative_constraints: 97, num_done: 993, time: 139.7
1711
steps: 166550, episodes: 4000, mean episode reward: -39.77354045536057, num_cumulative_constraints: 92, num_done: 990, time: 137.035
2280
steps: 208275, episodes: 5000, mean episode reward: -39.8208915090268, num_cumulative_constraints: 82, num_done: 985, time: 137.636
2880
steps: 249899, episodes: 6000, mean episode reward: -39.88051063804059, num_cumulative_constraints: 103, num_done: 991, time: 137.892
3466
steps: 291445, episodes: 7000, mean episode reward: -40.01205199422626, num_cumulative_constraints: 120, num_done: 992, time: 136.888
3995
steps: 333121, episodes: 8000, mean episode reward: -39.99546261364623, num_cumulative_constraints: 103, num_done: 988, time: 136.834
4611
steps: 374779, episodes: 9000, mean episode reward: -39.81396752425996, num_cumulative_constraints: 78, num_done: 988, time: 135.391
5194
steps: 416336, episodes: 10000, mean episode reward: -39.51454601906436, num_cumulative_constraints: 53, num_done: 990, time: 145.315
5746
steps: 457937, episodes: 11000, mean episode reward: -39.79331180724034, num_cumulative_constraints: 92, num_done: 994, time: 132.82
6314
steps: 499438, episodes: 12000, mean episode reward: -39.774699771380504, num_cumulative_constraints: 82, num_done: 992, time: 142.747
6867
steps: 541063, episodes: 13000, mean episode reward: -39.928814145801574, num_cumulative_constraints: 100, num_done: 991, time: 139.368
7477
steps: 582854, episodes: 14000, mean episode reward: -40.14775290734989, num_cumulative_constraints: 117, num_done: 985, time: 137.748
8067
steps: 624500, episodes: 15000, mean episode reward: -39.87763375856069, num_cumulative_constraints: 61, num_done: 990, time: 142.453
8577
steps: 666179, episodes: 16000, mean episode reward: -39.686950447273, num_cumulative_constraints: 57, num_done: 989, time: 136.794
9093
steps: 707858, episodes: 17000, mean episode reward: -39.724469355300585, num_cumulative_constraints: 57, num_done: 989, time: 136.573
9653
steps: 749366, episodes: 18000, mean episode reward: -39.74921649941672, num_cumulative_constraints: 80, num_done: 991, time: 139.977
10257
steps: 790865, episodes: 19000, mean episode reward: -39.77469897613758, num_cumulative_constraints: 90, num_done: 992, time: 127.952
10816

1629 / 19 = 85.73684210526316  add the terminal safety_layer_call cancellation 476/6 = 79.333333
18808/989.89  add the terminal safety_layer_call cancellation 5943/6 = 990.5
(1629 + 58 + 81 + 69)/22 = 83.5
(18808 + 991 +988 + 990)/22 = 21777/22 = 989.8636


